{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n",
    "We know that (in the “classic” view), the test error of a model tends to decrease and then increase as we increase model complexity. For low complexity, the bias dominates; for high complexity, the variance dominates.\n",
    "\n",
    "The training error, however, only decreases with increasing model complexity. If we use training error to select a model, we’ll select a model that overfits. And during training, when we select a model, only the training error is available to us.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://i.stack.imgur.com/alkeM.png\" alt=\"Image from “Elements of Statistical Learning”\" />\n",
    "<figcaption aria-hidden=\"true\">Image from “Elements of Statistical Learning”</figcaption>\n",
    "</figure>\n",
    "\n",
    "*Image source: Elements of Statistical Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is cross validation. Until now, we have been dividing our data into two parts:\n",
    "\n",
    "-   Training data: used to train the model\n",
    "-   Test data: used to evaluate the performance of our model on new, unseen data\n",
    "\n",
    "Now, we will make one more split:\n",
    "\n",
    "-   Training data: used to train the model\n",
    "-   Validation data: used to select the model complexity (usually by tuning some *hyperparameters*)\n",
    "-   Test data: used to evaluate the performance of our model on new, unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we will refine this idea in order to reduce the dependence on the particular samples we choose for the training, and to increase the number of samples available for training. In K-fold cross validation, we split the data into $K$ parts, each part being approximately equal in size. For each split, we fit the data on $K-1$ parts and test the data on the remaining part. Then, we average the score over the $K$ parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for $K=5$, it might look like this:\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, TimeSeriesSplit\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from tqdm import tqdm\n",
    "from ipywidgets import interact, fixed, widgets\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection using best K-Fold CV score\n",
    "\n",
    "First, we will try to use K-fold CV to select a polynomial model to fit the data in our first example.\n",
    "\n",
    "We will use the `scikit-learn` module for K-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polynomial_regression_data(n=100, xrange=[-1,1], coefs=[1,0.5,0,2], sigma=0.5):\n",
    "  x = np.random.uniform(xrange[0], xrange[1], n)\n",
    "  y = np.polynomial.polynomial.polyval(x,coefs) + sigma * np.random.randn(n)\n",
    "\n",
    "  return x.reshape(-1,1), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs=[1, 0.5, 0, 2.5]\n",
    "n_samples = 100\n",
    "sigma = 0.4\n",
    "\n",
    "# generate polynimal data\n",
    "x, y = generate_polynomial_regression_data(n=n_samples, coefs=coefs, sigma=sigma)\n",
    "\n",
    "# divide into training and test set\n",
    "# (we will later divide training data again, into training and validation set)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\n",
    "\n",
    "\n",
    "@interact(d = widgets.IntSlider(min=1, max=25, value=1), show_train = True, show_test = False)\n",
    "def plot_poly_fit(d, show_train, show_test,\n",
    "                  xtr = fixed(x_train), ytr = fixed(y_train), \n",
    "                  xts = fixed(x_test), yts = fixed(y_test)):\n",
    "  \n",
    "  xtr_trans = np.power(xtr, np.arange(0, d))\n",
    "\n",
    "  if show_train:\n",
    "    sns.scatterplot(x = xtr.squeeze(), y = ytr);\n",
    "  if show_test:\n",
    "    sns.scatterplot(x = xts.squeeze(), y = yts);\n",
    "  reg = LinearRegression().fit(xtr_trans, ytr)\n",
    "  ytr_hat = reg.predict(xtr_trans)\n",
    "\n",
    "  mse_tr = metrics.mean_squared_error(ytr, ytr_hat)\n",
    "  mse_ts = metrics.mean_squared_error(yts, reg.predict(np.power(xts, np.arange(0, d))))\n",
    "\n",
    "  sns.lineplot(x = xtr.squeeze(), y = ytr_hat, color='red')\n",
    "  plt.xlabel('x');\n",
    "  plt.ylabel('y');\n",
    "  plt.title(\"Training MSE: %f\\nTest MSE: %f\" % (mse_tr, mse_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will explore the use of K-fold cross validation for model selection\n",
    "\n",
    "K-fold cross validation is sometimes used for model evaluation, and sometimes used for model selection. There are a few important differences in how K-fold CV is used in each case:\n",
    "\n",
    "-   **Model evaluation**: When the total number of samples available is very small, we may not want to split off a single held-out test set for evaluation, since the results will vary dramatically depending on the draw of training vs. test samples. Under these circumstances, we may prefer to use K-fold CV to evaluate the model. In this case, we pass the entire dataset to the K-fold CV.\n",
    "-   **Model selection**: When we want to select the best model out of a set of possible candidate models (or equivalently, select model hyperparamaters, such as degree of a polynomial model or number of knots in a spline model), we need a validation set to help us evaluate each candidate model on data not used to fit the model parameters. In this case, we *only* pass the training subset of the data to the K-fold CV. We won’t use the test set at all inside the K-fold cross validation, because using the test set for model selection is a form of data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting a data set for K-fold cross validation is conceptually very simple. The basic idea is:\n",
    "\n",
    "-   We get a list of indices of training data, and decide how many “folds” we will use. The number of validation samples in each fold $N_{val}$ wil be the total number of training samples, divided by the number of folds.\n",
    "-   Then, we iterate over the number of folds. In the first fold, we put the first $N_{val}$ samples in the validation set and exclude them from the training set. In the second fold, we put the second batch of $N_{val}$ samples in the validation set, and exclude them from the training set. Continue until $K$ folds.\n",
    "\n",
    "In most circumstances, we will shuffle the list of training data indices first.\n",
    "\n",
    "The `scikit-learn` library provides a `KFold` that does this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfold = 5\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "\n",
    "for isplit, idx in enumerate(kf.split(x_train)):     \n",
    "    idx_tr, idx_val = idx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "although it’s also easy to do this ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfold = 5                                   # number of folds (you choose!)\n",
    "nval = x_train.shape[0]//nfold              # number of validation samples per fold\n",
    "idx_split = [i*nval for i in range(nfold)]  \n",
    "idx_list = np.arange(x_train.shape[0])      # list of training data indices\n",
    "np.random.shuffle(idx_list)                 # shuffle list of indices\n",
    "\n",
    "for i, idx in enumerate(idx_split):\n",
    "  idx_val = idx_list[idx:idx+nval]\n",
    "  idx_tr = np.delete(idx_list, idx_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outer loop can be used to divide the data into training and validation, but then we’ll also need an inner loop to train and validate each model for this particular fold.\n",
    "\n",
    "In this case, suppose we want to evaluate polynomial models with different model orders from\n",
    "\n",
    "$$d=1, \\quad \\hat{y} = w_0 + w_1 x$$\n",
    "\n",
    "to\n",
    "\n",
    "$$d=10, \\quad \\hat{y} = w_0 + w_1 x + w_2 x^2 + \\ldots + w_{10} x^{10}$$\n",
    "\n",
    "We could do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a k-fold object\n",
    "nfold = 5\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "\n",
    "# model orders to be tested\n",
    "dmax = 10\n",
    "dtest_list = np.arange(1,dmax+1)\n",
    "nd = len(dtest_list)\n",
    "\n",
    "for isplit, idx in enumerate(kf.split(x_train)):     \n",
    "  idx_tr, idx_val = idx \n",
    "\n",
    "  for dtest in dtest_list:\n",
    "    # get \"transformed\" training and validation data\n",
    "    x_train_dtest =  x_train[idx_tr]**np.arange(1,dtest+1)\n",
    "    y_train_kfold =  y_train[idx_tr]\n",
    "    x_val_dtest   =  x_train[idx_val]**np.arange(1,dtest+1)\n",
    "    y_val_kfold   =  y_train[idx_val]\n",
    "\n",
    "    # fit model on training data\n",
    "    reg_dtest = LinearRegression().fit(x_train_dtest, y_train_kfold)\n",
    "    \n",
    "    # measure MSE on validation data\n",
    "    y_hat   = reg_dtest.predict(x_val_dtest)\n",
    "    mse_val = metrics.mean_squared_error(y_val_kfold, y_hat)\n",
    "    r2_val  = metrics.r2_score(y_val_kfold, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, however, that there was a lot of wasted computation there. We computed the same polynomial features multiple times in different folds. Instead, we should compute the entire set of transformed features in advance, then just select the ones we need in each iteration over model order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a k-fold object\n",
    "nfold = 5\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "\n",
    "# model orders to be tested\n",
    "dmax = 10\n",
    "dtest_list = np.arange(1,dmax+1)\n",
    "nd = len(dtest_list)\n",
    "\n",
    "# create transformed features up to d_max\n",
    "x_train_trans = x_train**np.arange(1,dmax+1)\n",
    "\n",
    "for isplit, idx in enumerate(kf.split(x_train)):     \n",
    "  idx_tr, idx_val = idx \n",
    "\n",
    "  for dtest in dtest_list:\n",
    "    # get \"transformed\" training and validation data for this model order\n",
    "    x_train_dtest =  x_train_trans[idx_tr,  :dtest]\n",
    "    y_train_kfold = y_train[idx_tr]\n",
    "    x_val_dtest   =  x_train_trans[idx_val, :dtest]\n",
    "    y_val_kfold   = y_train[idx_val]\n",
    "\n",
    "    # fit model on training data\n",
    "    reg_dtest = LinearRegression().fit(x_train_dtest, y_train_kfold)\n",
    "    \n",
    "    # measure MSE on validation data\n",
    "    y_hat   = reg_dtest.predict(x_val_dtest)\n",
    "    mse_val = metrics.mean_squared_error(y_val_kfold, y_hat)\n",
    "    r2_val  = metrics.r2_score(y_val_kfold, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s much better! Let’s look at what this is doing - we’ll run it again with some extra visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a k-fold object\n",
    "nfold = 5\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "\n",
    "# model orders to be tested\n",
    "dmax = 10\n",
    "dtest_list = np.arange(1,dmax+1)\n",
    "nd = len(dtest_list)\n",
    "\n",
    "# create transformed features up to d_max\n",
    "x_train_trans = x_train**np.arange(1,dmax+1)\n",
    "\n",
    "# create a big figure\n",
    "fig, axs = plt.subplots(nfold, nd, sharex=True, sharey=True)\n",
    "fig.set_figheight(nfold+1);\n",
    "fig.set_figwidth(nd+1);\n",
    "\n",
    "for isplit, idx in enumerate(kf.split(x_train)):     \n",
    "  idx_tr, idx_val = idx \n",
    "\n",
    "  for didx, dtest in enumerate(dtest_list):\n",
    "    # get \"transformed\" training and validation data for this model order\n",
    "    x_train_dtest =  x_train_trans[idx_tr,  :dtest]\n",
    "    y_train_kfold = y_train[idx_tr]\n",
    "    x_val_dtest   =  x_train_trans[idx_val, :dtest]\n",
    "    y_val_kfold   = y_train[idx_val]\n",
    "\n",
    "    # fit model on training data\n",
    "    reg_dtest = LinearRegression().fit(x_train_dtest, y_train_kfold)\n",
    "    \n",
    "    # measure MSE on validation data\n",
    "    y_hat   = reg_dtest.predict(x_val_dtest)\n",
    "    mse_val = metrics.mean_squared_error(y_val_kfold, y_hat)\n",
    "    r2_val  = metrics.r2_score(y_val_kfold, y_hat)\n",
    "\n",
    "    # this is just for visualization/understanding - in a \"real\" problem you would not include this\n",
    "    p = sns.lineplot(x = x_train_dtest[:,0].squeeze(), y = reg_dtest.predict(x_train_dtest), color='red', ax=axs[isplit, didx]);\n",
    "    p = sns.scatterplot(x = x_val_dtest[:, 0].squeeze(), y = y_val_kfold,  ax=axs[isplit, didx]);\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we’ll add some arrays in which to save the validation performance from each fold, so that we can average them afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a k-fold object\n",
    "nfold = 5\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "\n",
    "# model orders to be tested\n",
    "dmax = 10\n",
    "dtest_list = np.arange(1,dmax+1)\n",
    "nd = len(dtest_list)\n",
    "\n",
    "mse_val = np.zeros((nd,nfold))\n",
    "r2_val  = np.zeros((nd,nfold))\n",
    "\n",
    "# create transformed features up to d_max\n",
    "x_train_trans = x_train**np.arange(1,dmax+1)\n",
    "\n",
    "# loop over the folds\n",
    "# the first loop variable tells us how many out of nfold folds we have gone through\n",
    "# the second loop variable tells us how to split the data\n",
    "for isplit, idx in enumerate(kf.split(x_train)):\n",
    "        \n",
    "  # these are the indices for the training and validation indices\n",
    "  # for this iteration of the k folds\n",
    "  idx_tr, idx_val = idx \n",
    "\n",
    "  x_train_kfold = x_train_trans[idx_tr]\n",
    "  y_train_kfold = y_train[idx_tr]\n",
    "  x_val_kfold = x_train_trans[idx_val]\n",
    "  y_val_kfold = y_train[idx_val]\n",
    "\n",
    "  for didx, dtest in enumerate(dtest_list):\n",
    "\n",
    "    # get transformed features\n",
    "    x_train_dtest =  x_train_kfold[:, :dtest]\n",
    "    x_val_dtest   =  x_val_kfold[:, :dtest]\n",
    "\n",
    "    # fit data\n",
    "    reg_dtest = LinearRegression().fit(x_train_dtest, y_train_kfold)\n",
    "    \n",
    "    # measure MSE on validation data\n",
    "    y_hat = reg_dtest.predict(x_val_dtest)\n",
    "    mse_val[didx, isplit] = metrics.mean_squared_error(y_val_kfold, y_hat)\n",
    "    r2_val[didx, isplit] = metrics.r2_score(y_val_kfold, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the mean (across K folds) validation error for each model order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=dtest_list, y=mse_val.mean(axis=1), marker='o');\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold MSE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see which model order gave us the lowest MSE on the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_min = np.argmin(mse_val.mean(axis=1))\n",
    "d_min_mse = dtest_list[idx_min]\n",
    "d_min_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_val.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=dtest_list, y=mse_val.mean(axis=1));\n",
    "sns.scatterplot(x=dtest_list, y=mse_val.mean(axis=1), hue=dtest_list==d_min_mse, s=100, legend=False);\n",
    "\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold MSE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select by highest R2 (instead of lowest MSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = np.argmax(r2_val.mean(axis=1))\n",
    "d_max_r2 = dtest_list[idx_max]\n",
    "d_max_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_val.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=dtest_list, y=r2_val.mean(axis=1));\n",
    "sns.scatterplot(x=dtest_list, y=r2_val.mean(axis=1), hue=dtest_list==d_max_r2, s=100, legend=False);\n",
    "\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold R2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can re-fit a model of degree `d_min_mse` or `d_max_r2` on the *entire* training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dopt =  x_train_trans[:, :d_max_r2]\n",
    "reg_dopt = LinearRegression().fit(x_train_dopt, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and evaluate it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_dopt = x_test**np.arange(1,d_max_r2+1)\n",
    "y_hat = reg_dopt.predict(x_test_dopt)\n",
    "mse_dopt = metrics.mean_squared_error(y_test, y_hat)\n",
    "r2_dopt  = metrics.r2_score(y_test, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_dopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_dopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection using 1-SE “rule”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the minimum K-fold CV error for model selection, we sometimes will still select an overly complex model <sup>\\[2\\]</sup>.\n",
    "\n",
    "As an alternative, we can use the “one standard error rule” <sup>\\[3\\]</sup>. According to this “rule”, we choose the least complex model whose error is no more than one standard error above the error of the best model - i.e. the simplest model whose performance is comparable to the best model.\n",
    "\n",
    "<small>\\[2\\] See [Cawley & Talbot (J of Machine Learning Research, 2010)](http://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf) for more on this.</small>\n",
    "\n",
    "<small>\\[3\\] See Chapter 7 of [Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this rule as follows:\n",
    "\n",
    "-   Find the MSE for each fold for each model candidate\n",
    "-   For each model candidate, compute the mean and standard error of the MSE over the $K$ folds. We will compute the standard error as $$\\frac{\\sigma_{\\text{MSE}}}{\\sqrt{K-1}}$$ where $\\sigma_{\\text{MSE}}$ is the standard deviation of the MSE over the $K$ folds.\n",
    "-   Find the model with the smallest mean MSE (across the $K$ folds). Compute the *target* as mean MSE + SE for this model.\n",
    "-   Select the least complex model whose mean MSE is below the target.\n",
    "\n",
    "This works for any metric that is a “lower is better” metric. If you are using a “higher is better” metric, such as R2, for example, you would modify the last two steps:\n",
    "\n",
    "-   Find the model with the **largest** mean R2 (across the $K$ folds). Compute the **mean R2 - SE of R2** for this model. Call this quantity the *target*.\n",
    "-   Select the least complex model whose mean R2 is **above** the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_min = np.argmin(mse_val.mean(axis=1))\n",
    "target = mse_val[idx_min,:].mean() + mse_val[idx_min,:].std()/np.sqrt(nfold-1)\n",
    "# np.where returns indices of values where condition is satisfied\n",
    "idx_one_se = np.where(mse_val.mean(axis=1) <= target)\n",
    "d_one_se = np.min(dtest_list[idx_one_se])\n",
    "d_one_se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use\n",
    "\n",
    "    np.min(dtest_list[idx_one_se])\n",
    "\n",
    "because in this case, the smallest value of `dtest_list` is the one with the least complexity. In other circumstances, we might need a different rule to select the least complex model from the list of candidate models whose score is within one standard error of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x=dtest_list, y=mse_val.mean(axis=1), yerr=mse_val.std(axis=1)/np.sqrt(nfold-1));\n",
    "plt.hlines(y=target, xmin=np.min(dtest_list), xmax=np.max(dtest_list), ls='dotted')\n",
    "sns.scatterplot(x=dtest_list, y=mse_val.mean(axis=1), hue=dtest_list==d_one_se, s=100, legend=False);\n",
    "\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold MSE\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = np.argmax(r2_val.mean(axis=1))\n",
    "target_r2 = r2_val[idx_max,:].mean() - r2_val[idx_max,:].std()/np.sqrt(nfold-1)\n",
    "# np.where returns indices of values where condition is satisfied\n",
    "idx_one_se_r2 = np.where(r2_val.mean(axis=1) >= target_r2)\n",
    "d_one_se_r2 = np.min(dtest_list[idx_one_se_r2])\n",
    "d_one_se_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x=dtest_list, y=r2_val.mean(axis=1), yerr=r2_val.std(axis=1)/np.sqrt(nfold-1));\n",
    "plt.hlines(y=target_r2, xmin=np.min(dtest_list), xmax=np.max(dtest_list), ls='dotted')\n",
    "sns.scatterplot(x=dtest_list, y=r2_val.mean(axis=1), hue=dtest_list==d_one_se_r2, s=100, legend=False);\n",
    "\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold R2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake: using the test set for model selection\n",
    "\n",
    "In the examples above, we learned how to use K-fold CV to select a model using a validation set that is split off from the training data.\n",
    "\n",
    "We understand from a previous lesson that we should not use the training set for model selection - the training error will decrease with model complexity, so this approach would always choose the most complex model (rather than one that has the best performance on data not used to fit the model).\n",
    "\n",
    "However, you may feel tempted to use the test set for model selection (i.e. choose the model that has the best performance on the test set). But, this would be a mistake! Using the test set for model selection is a form of data leakage. If you select the model with best performance on the test set, you risk overfitting to noise in the test data, and since you have “contaminated” your test set by using it in this way, you no longer have a held-out test set on which to evaluate your final model. Your evaluation on this “contaminated” test set will be an overly optimistic evaluation.\n",
    "\n",
    "**Note**: This example is from the section “The Wrong and Right Way to Do Cross-validation” in the textbook *Elements of Statistical Learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate this, we will use an extreme example: we will generate a very small dataset with 20 samples, and a very large number of features - 10,000 features! - where\n",
    "\n",
    "$$y = x_0 + x_1 + \\epsilon$$\n",
    "\n",
    "that is,\n",
    "\n",
    "-   the first two features, columns `0` and `1`, are relevant to the target variable\n",
    "-   the remaining features are just noise.\n",
    "\n",
    "(It’s an extreme example because the number of samples is very small, and the number of features is relatively so large! But this extreme example will help us illustrate our point.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_y(x):\n",
    "    # y is sum of columns 0 and 1 from X + some noise\n",
    "    y = x[:, 0] + x[:, 1]\n",
    "    y += np.random.normal(0, .01, y.shape)\n",
    "    return y\n",
    "\n",
    "def generate_x(n):\n",
    "    return np.random.uniform(0, 3, (n, 10000))\n",
    "\n",
    "# generate data, split into training and test\n",
    "X = generate_x(20)\n",
    "y = generate_y(X)\n",
    "Xtr, Xts, ytr, yts = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we are going to train a model on only *two* features. However, assuming we do not know in advance which features are relevant and which features are not, we need to use some sort of feature selection.\n",
    "\n",
    "First, we wil do feature selection the *wrong* way:\n",
    "\n",
    "-   For each feature, we will train a simple linear regression using only that feature.\n",
    "-   We will compute the R2 score of that simple linear regression on the *test* data. This will be considered the “score” of that feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = Xtr.shape[1]\n",
    "score_ts = np.zeros(num_features)\n",
    "for i in tqdm(range(num_features), desc=\"Scoring Features\"):\n",
    "    # get subset of data for the feature\n",
    "    Xtr_subset = Xtr[:, [i]]\n",
    "    Xts_subset = Xts[:, [i]]\n",
    "    # train a model on this feature\n",
    "    model = LinearRegression().fit(Xtr_subset, ytr)\n",
    "    # score this model using the test set\n",
    "    y_pred = model.predict(Xts_subset)\n",
    "    score_ts[i] = metrics.r2_score(yts, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the first two features (`0` and `1`) are the only meaningful features, they will not necessarily be the only ones with a high “score” using this method, because of the noise in the data. Some of the “irrelevant” features will have scores as high or higher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.stem(np.arange(0, 10000),score_ts, bottom=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we now find the two features with the highest test R2 score, and train a multiple regression model using those two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features with highest score\n",
    "best_two_features_ts = np.argsort(score_ts)[-2:]  # Last two (sorted in ascending order)\n",
    "print(best_two_features_ts) # Is it 0 and 1? if not... uh oh..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ts = LinearRegression().fit(Xtr[:, best_two_features_ts], ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is likely to have a reasonably high R2 score on the test data -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_ts.predict(Xts[:, best_two_features_ts])\n",
    "print( metrics.r2_score(yts, y_pred) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, that doesn’t mean that it will really do well at making predictions for new data not used at all in the training process - let’s generate some new data using the same functions, and try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = generate_x(20)\n",
    "y_new = generate_y(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on actual new data, score is very low\n",
    "y_new_pred = model_ts.predict(X_new[:, best_two_features_ts])\n",
    "print( metrics.r2_score(y_new, y_new_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach, in which we used the test set for model selection, had a major problem: although the model we selected was actually *not* a good model, the evaluation was “overly optimistic” - it implied that the model *was* good.\n",
    "\n",
    "This is because we “contaminated” the test set by using it for model selection. Let’s try again with the correct approach, where we use a separate validation set split off from the training data for model selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(Xtr, ytr, test_size=4, random_state=2)\n",
    "\n",
    "# \"Score\" each feature by fitting on the training set and evaluating on the test set\n",
    "num_features = X_train.shape[1]\n",
    "score_vl = np.zeros(num_features)\n",
    "for i in tqdm(range(num_features), desc=\"Scoring Features\"):\n",
    "    # get subset of data for the feature\n",
    "    Xtr_subset = X_train[:, [i]]\n",
    "    Xvl_subset = X_val[:, [i]]\n",
    "    # train a model on this feature\n",
    "    model = LinearRegression().fit(Xtr_subset, y_train)\n",
    "    # score this model using the test set\n",
    "    y_pred = model.predict(Xvl_subset)\n",
    "    score_vl[i] = metrics.r2_score(y_val, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features with highest score\n",
    "best_two_features_vl = np.argsort(score_vl)[-2:]  # Last two (sorted in ascending order)\n",
    "print(best_two_features_vl) # Is it 0 and 1? if not... uh oh..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vl = LinearRegression().fit(Xtr[:, best_two_features_vl], ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we evaluate *this* model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_vl.predict(Xts[:, best_two_features_vl])\n",
    "print( metrics.r2_score(yts, y_pred) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we understand (correctly!) that the model is not useful. The evaluation on the “clean” held-out test set is similar to the model performance on really “new” data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new_pred = model_vl.predict(X_new[:, best_two_features_vl])\n",
    "print( metrics.r2_score(y_new, y_new_pred) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example highlights the danger of using the test set in any meaningful way before the final evaluation (including training, data processing, or model selection) - with a “contaminated” test set, our evaluation may be overly optimistic and we will not understand the true performance of our model.\n",
    "\n",
    "Using the test set for model selection is not the only mistake that can lead to data leakage and an overly optimistic evaluation, though! The next section describes another…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the course of COVID with a “cubic model”\n",
    "\n",
    "As part of the materials for this lesson, you read about some attempts early in the COVID-19 pandemic to predict how the number of cases or deaths would evolve. You were asked to consider:\n",
    "\n",
    "> The forecasts produced by these models were all very wrong, but they appeared to fit the data well! What was wrong with the approach used to produce these models? How did they miscalculate so badly?\n",
    "\n",
    "Now, we’ll take that process apart, see what went wrong, and see what we could have done differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will get U.S. COVID data and read it in to our notebook environment. We’ll also add a field called `daysElapsed` which will count the number of days since March 1, 2020 for each row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://covidtracking.com/data/download/national-history.csv -O national-history.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('national-history.csv')\n",
    "df.date = pd.to_datetime(df.date)\n",
    "df = df.sort_values(by=\"date\")\n",
    "df = df.assign(daysElapsed =  (df.date - pd.to_datetime('2020-03-01')).dt.days)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s assume that we are making this prediction sometime near the beginning of May 2020 (like in the reading), well into the first wave in the U.S., and we want to predict when this wave will fall off (deaths go back to zero).\n",
    "\n",
    "We’ll use all of the data up to May 2020 for training, since that is what is available at “training time”. But afterwards, we’ll go back and see how well our model did by comparing its predictions for May and June 2020 to the real course of the pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df[(df.date <= '2020-05-01') & (df.date > '2020-03-01')]\n",
    "df_ts = df[(df.date < '2020-06-30') & (df.date >= '2020-05-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df_tr.daysElapsed, y=df_tr.deathIncrease);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we will use a polynomial basis to fit a linear regression model, so let’s generate “transformed” versions of our feature: days since the beginning of March 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_poly = df_tr.daysElapsed.values.reshape(-1,1)**np.arange(1, 5)\n",
    "df_ts_poly = df_ts.daysElapsed.values.reshape(-1,1)**np.arange(1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now go ahead and fit a model to our training data, then compute the R2 score for the model. It seems like a good fit on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_covid = LinearRegression().fit(df_tr_poly, df_tr.deathIncrease)\n",
    "deathIncrease_fitted = reg_covid.predict(df_tr_poly)\n",
    "metrics.r2_score(df_tr.deathIncrease, deathIncrease_fitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and looks reasonably good in this visualization (although of course, we recognize that negative deaths should be impossible):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df_tr.daysElapsed, y=df_tr.deathIncrease);\n",
    "sns.lineplot(x=df_tr.daysElapsed, y=deathIncrease_fitted);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But despite looking good on training data, the model does a terrible job of predicting the end of the first wave. The model predicts that deaths fall to zero around day 70, but we can see that the actual fall off is much slower, and there are still hundreds of deaths each day at day 120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deathIncrease_fitted_future = reg_covid.predict(df_ts_poly)\n",
    "\n",
    "sns.scatterplot(x=df_tr.daysElapsed, y=df_tr.deathIncrease);\n",
    "sns.lineplot(x=df_tr.daysElapsed, y=deathIncrease_fitted);\n",
    "\n",
    "sns.scatterplot(x=df_ts.daysElapsed, y=df_ts.deathIncrease);\n",
    "sns.lineplot(x=df_ts.daysElapsed, y=deathIncrease_fitted_future);\n",
    "\n",
    "plt.ylim(0, 3000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the R2 score shows that this model has very poor performance on the test set of future data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(df_ts.deathIncrease, deathIncrease_fitted_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R2 score on the training set didn’t tell us about the model’s performance on its “true” task: making predictions about the future. This is a “worst case scenario” for model evaluation:\n",
    "\n",
    "-   **Best case**: Model does well in evaluation, model does well when deployed for its “true” task.\n",
    "-   **Second best case**: Model does poorly in evaluation, but at least you know not to deploy the model for its “true” task.\n",
    "-   **Worst case**: Model does well in evaluation, you deploy the model for its “true” task, and then the model does very poorly - in this case you make bad decisions, lose credibility, etc. because you trust the model output. You would be much better off if you understood from the evaluation that the model is not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s likely that the examples in the “Predicting the course of COVID with a “cubic model”” case study only evaluated their models on the same data used for training, which is why they vastly overestimated its ability. But, even if they had tried to evaluate on a held-out validation set, if they had not split the data in a way that respects its structure, they would *still* vastly overestimate the capabilities of the model.\n",
    "\n",
    "Let’s see how. Since this is a small dataset, instead of a single train/test split, we’ll evaluate the model using KFold CV with five splits, and we’ll report the average R2 score on the “held-out” set across folds.\n",
    "\n",
    "(Note that in this case, we are using K-fold CV only for model evaluation, not model selection.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfold = 5\n",
    "kf = KFold(shuffle=True, n_splits=nfold)\n",
    "\n",
    "r2_badcv = np.zeros(nfold)\n",
    "\n",
    "fig, axs = plt.subplots(1, nfold, sharex=True, sharey=True)\n",
    "fig.set_figwidth(15);\n",
    "fig.set_figheight(2);\n",
    "\n",
    "\n",
    "for isplit, (train_idx, val_idx) in enumerate(kf.split(df_tr_poly)):\n",
    "    x_train_kfold, x_val_kfold = df_tr_poly[train_idx], df_tr_poly[val_idx]\n",
    "    y_train_kfold, y_val_kfold = df_tr.deathIncrease.iloc[train_idx], df_tr.deathIncrease.iloc[val_idx]\n",
    "\n",
    "    model = LinearRegression().fit(x_train_kfold, y_train_kfold)\n",
    "    y_pred = model.predict(x_val_kfold)\n",
    "    r2_badcv[isplit] = metrics.r2_score(y_val_kfold, y_pred)\n",
    "\n",
    "    _ = sns.scatterplot(x=x_train_kfold[:, 0], y=y_train_kfold, ax=axs[isplit]);\n",
    "    _ = sns.scatterplot(x=x_val_kfold[:, 0], y=y_val_kfold, ax=axs[isplit]);\n",
    "    _ = axs[isplit].set_title(f\"Fold {isplit+1}\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_badcv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model makes predictions on the “validation” set in the example above, it has already been trained on the data points immediately before and after each validation sample.\n",
    "\n",
    "This is a much easier task than the “true” task that the model will perform - making predictions for a sequence of consecutive future dates.\n",
    "\n",
    "To evaluate the performance of the model, we should make sure that the validation task mimics this “true” task.\n",
    "\n",
    "One possible approach would be to create multiple “folds”, where in each fold, the number of samples in the training set increases (and the validation set is always the ten days after the training set - we are validating whether our model can predict deaths ten days into the future).\n",
    "\n",
    "Here’s what that might look like (blue dots are training data, orange dots are validation data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfold = 5\n",
    "tscv = TimeSeriesSplit(n_splits=nfold, test_size=10)\n",
    "\n",
    "r2_tscv = np.zeros(nfold)\n",
    "\n",
    "fig, axs = plt.subplots(1, nfold, sharex=True, sharey=True)\n",
    "fig.set_figwidth(15);\n",
    "fig.set_figheight(2);\n",
    "\n",
    "\n",
    "for isplit, (train_idx, val_idx) in enumerate(tscv.split(df_tr_poly)):\n",
    "    x_train_kfold, x_val_kfold = df_tr_poly[train_idx], df_tr_poly[val_idx]\n",
    "    y_train_kfold, y_val_kfold = df_tr.deathIncrease.iloc[train_idx], df_tr.deathIncrease.iloc[val_idx]\n",
    "\n",
    "    model = LinearRegression().fit(x_train_kfold, y_train_kfold)\n",
    "    y_pred = model.predict(x_val_kfold)\n",
    "    r2_tscv[isplit] = metrics.r2_score(y_val_kfold, y_pred)\n",
    "\n",
    "    _ = sns.scatterplot(x=x_train_kfold[:, 0], y=y_train_kfold, ax=axs[isplit]);\n",
    "    _ = sns.scatterplot(x=x_val_kfold[:, 0], y=y_val_kfold, ax=axs[isplit]);\n",
    "    _ = axs[isplit].set_title(f\"Fold {isplit+1}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this validation approach, we find (correctly) that the polynomial model is *not* good at predicting COVID cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_tscv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This case study highlights, the importance of evaluating on a held-out test set not used for training, but also, of making sure that the evaluation task is comparable to the “true” task that the model will be used for!\n",
    "\n",
    "In the evaluation with shuffled data, the R2 score seems to be high, but this is only due to data leakage. The evaluation is not valid, and the model is really not useful. The correct evaluation, with a time series split, shows that the model does not have any predictive benefit."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 }
}
