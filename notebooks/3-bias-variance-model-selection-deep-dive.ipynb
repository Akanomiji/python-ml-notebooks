{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias/variance and model selection in depth\n",
    "==========================================\n",
    "\n",
    "*Fraida Fund*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribution\n",
    "\n",
    "Parts of this notebook borrow from, or are inspired by, the following\n",
    "sources:\n",
    "\n",
    "-   The Introduction section is based on [Hyperparameters and Model\n",
    "    Validation](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html),\n",
    "    in the Python Data Science Handbook by Jake VanderPlas\n",
    "-   The section on polynomial models is based on [the model order\n",
    "    selection demo\n",
    "    notebook](https://colab.research.google.com/github/sdrangan/introml/blob/master/unit04_model_sel/demo_polyfit.ipynb)\n",
    "    by Prof. Sundeep Rangan, and some of the text in that section is\n",
    "    copied from that notebook.\n",
    "-   The section on uninformative features is based on [Cross Validation:\n",
    "    The Right and Wrong\n",
    "    Way](http://nbviewer.ipython.org/urls/raw.github.com/cs109/content/master/lec_10_cross_val.ipynb)\n",
    "    from [Harvard CS109 Data Science](https://github.com/cs109/content)\n",
    "-   The simulation plots are based on the `scikit-learn` example [Single\n",
    "    estimator versus bagging: bias-variance\n",
    "    decomposition](https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "------------\n",
    "\n",
    "As a distinguished machine learning expert, you are hired as a\n",
    "consultant to solve a difficult problem for a client. You apply the best\n",
    "<sup>\\[1\\]</sup> machine learning model you know, and present the\n",
    "results to the client. The response?\n",
    "\n",
    "> Not good enough! We expect much better performance.\n",
    "\n",
    "As a machine learning expert, what should you do to improve your\n",
    "results?\n",
    "\n",
    "-   Find and correct a problem with the data (garbage in, garbage out!)\n",
    "-   Find and correct a problem with the model implementation/training\n",
    "-   Use a more complicated model (more flexible)\n",
    "-   Use a less complicated model (less flexible)\n",
    "-   Get more training samples\n",
    "-   Get more data to add features to each sample - for example, you\n",
    "    could join two related datasets to get more potentially predictive\n",
    "    features\n",
    "-   Add additional features using transformed versions of the features\n",
    "    you already have\n",
    "\n",
    "Each of these possible solutions has the potential to improve results,\n",
    "or to have no effect. In some cases, they can actually make the\n",
    "performance even worse.\n",
    "\n",
    "How do you know which one to try?\n",
    "\n",
    "<small>\\[1\\] There is actually no “best” machine learning model in\n",
    "general - various models will perform better on different\n",
    "problems.</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first instinct may be to try all of these potential solutions, and\n",
    "see which one improves performance. But that could be very expensive -\n",
    "\n",
    "-   Your time costs \\$ (you are getting paid for consulting on this\n",
    "    problem!)\n",
    "-   You may need to use metered computing resources to train the model,\n",
    "    which costs \\$\n",
    "-   Collecting data, especially labeled data, often costs \\$\n",
    "-   Your client is losing \\$ due to using a sub-optimal model in\n",
    "    production while waiting for you to improve it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a machine learning expert, your knowledge of the theoretical basics\n",
    "of machine learning, and your ability to *connect that knowledge* to\n",
    "practical solutions for real problems, should prepare you for this\n",
    "moment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our previous work in this course has prepared us to evaluate some of\n",
    "these “next steps” to see whether they would be appropriate.\n",
    "\n",
    "**Find and correct a problem with the data (garbage in, garbage out!)**:\n",
    "\n",
    "-   We practiced using exploratory data analysis as a first step toward\n",
    "    identifying problems or wrong assumptions related to the data.\n",
    "-   We also saw that when using models with good *interpretability*, we\n",
    "    can inspect the fitted model as another step toward finding problems\n",
    "    with the data. For example, in a case study on linear regression, we\n",
    "    saw that the coefficients for certain features were high when common\n",
    "    sense dictated that those features should not have any effect on the\n",
    "    target variable.\n",
    "\n",
    "**Find and correct a problem with the model implementation/training**:\n",
    "\n",
    "-   We looked at, and will continue to look at, some problems that can\n",
    "    arise with model training, for example learning rate too high/too\n",
    "    low in gradient descent.\n",
    "\n",
    "**Get more data to add features to each sample**:\n",
    "\n",
    "-   We practiced inspecting the residuals. Plots of residuals against\n",
    "    $y$ or against each of the features are a good way to see a 2D view\n",
    "    of the results of the model, even when the dimension of the data is\n",
    "    large.\n",
    "-   When there appears to be a pattern in the residuals, this indicates\n",
    "    something “learnable” in the data that isn’t captured by our model.\n",
    "    -   For example, suppose we find that the residuals appear to be a\n",
    "        function of $y$, but not of any feature $x$. This suggests that\n",
    "        $y$ is partly a function of a “missing” feature, which we should\n",
    "        try to identify (using domain knowledge) and add to our model.\n",
    "    -   Similarly, if the residuals appear to be a function of a feature\n",
    "        $x$ that is in our data, but we are not using to train the\n",
    "        model, we should absolutely add that feature to our model.\n",
    "-   Another thing to look for in a residuals plot: “outliers” or extreme\n",
    "    values. These can sometimes indicate problems with the data,such as\n",
    "    samples affected by measurement or recording error. (In this case,\n",
    "    you might want to exclude these samples, and also figure out how to\n",
    "    exclude them in production.) These can also indicate that the data\n",
    "    is drawn from two distributions, where samples from the minority\n",
    "    distribution appear as outliers. (In this case, you wouldn’t want to\n",
    "    exclude these samples. You would want to add a feature to help you\n",
    "    distinguish between the two populations, and then re-train your\n",
    "    model with the added feature.)\n",
    "\n",
    "**Add additional features using transformed versions of the features you\n",
    "already have**:\n",
    "\n",
    "-   If the plot of residuals against a feature $x$ appears to have a\n",
    "    non-linear pattern, we should consider adding a transformed version\n",
    "    of that feature to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s a good start. But to help us identify whether some of the other\n",
    "steps would be helpful, we need to understand *bias* and *variance*.\n",
    "Then, we’ll be on our way towards understanding when to:\n",
    "\n",
    "-   Use a more complicated model (more flexible)\n",
    "-   Use a less complicated model (less flexible)\n",
    "-   Get more training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting and underfitting: Polynomial data\n",
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the concept of overfitting, we will consider data\n",
    "generated by a polynomial of order $d$, with some added Gaussian noise:\n",
    "\n",
    "$$y_i = w_0 + w_1 x_{1} + \\cdots + w_d x_{i}^d + \\epsilon $$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "We can fit this model using a linear basis function regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function that generates polynomial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_polynomial_regression_data(n=100, xrange=[-1,1], coefs=[1,0.5,0,2], sigma=0.5):\n",
    "  x = np.random.uniform(xrange[0], xrange[1], n)\n",
    "  y = np.polynomial.polynomial.polyval(x,coefs) + sigma * np.random.randn(n)\n",
    "\n",
    "  return x.reshape(-1,1), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs=[1,0.5,0,2]\n",
    "n_samples = 100\n",
    "sigma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = generate_polynomial_regression_data(n=n_samples, coefs=coefs, sigma=sigma)\n",
    "x_train, y_train = generate_polynomial_regression_data(n=n_samples, coefs=coefs, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, label='Test points');\n",
    "plt.xlabel(\"x\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.title(\"Data generated from $1 + 0.5x + 2x^3$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, since the data is generated synthetically, we know the\n",
    "“true” function that generated the data. We can plot this function along\n",
    "with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, label='Test points');\n",
    "plt.xlabel(\"x\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.title(\"Data generated from $1 + 0.5x + 2x^3$\");\n",
    "\n",
    "# Plot true function\n",
    "x_p = np.linspace(-1,1,100);\n",
    "y_p = np.polynomial.polynomial.polyval(x_p, coefs);\n",
    "sns.lineplot(x=x_p,y=y_p, color='red', label='True function');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use “transformed” features to fit a linear regression to this\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_d3 = np.column_stack( [x_train**d for d in np.arange(1,4)])\n",
    "print(x_train_d3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_poly3 = LinearRegression().fit(x_train_d3,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_poly3.intercept_)\n",
    "print(reg_poly3.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, label='Test points');\n",
    "plt.xlabel(\"x\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.title(\"Data generated from $1 + 0.5x + 2x^3$\");\n",
    "\n",
    "# Plot true function\n",
    "x_p = np.linspace(-1,1,100);\n",
    "y_p = np.polynomial.polynomial.polyval(x_p, coefs);\n",
    "sns.lineplot(x=x_p,y=y_p, color='red', label='True function');\n",
    "\n",
    "# Plot fitted function\n",
    "x_p = np.linspace(-1,1,100);\n",
    "x_p_d3 = np.column_stack( [x_p.reshape(-1,1)**d for d in np.arange(1,4)])\n",
    "y_p = reg_poly3.predict(x_p_d3);\n",
    "sns.lineplot(x=x_p, y=y_p, color='green', label='Model with d=3');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the error on the training set and on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_poly3_train = metrics.mean_squared_error(y_train, reg_poly3.predict(x_train_d3))\n",
    "mse_poly3_train/np.var(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_d3 = np.column_stack( [x_test**d for d in np.arange(1,4)])\n",
    "mse_poly3 = metrics.mean_squared_error(y_test, reg_poly3.predict(x_test_d3))\n",
    "mse_poly3/np.var(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, though, we don’t know the “true” model order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we try fitting a linear regression (using transformed features)\n",
    "to this model, starting with $d=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_poly1 = LinearRegression().fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, label='Test points');\n",
    "plt.xlabel(\"x\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.title(\"Data generated from $1 + 0.5x + 2x^3$\");\n",
    "\n",
    "# Plot true function\n",
    "x_p = np.linspace(-1,1,100);\n",
    "y_p = np.polynomial.polynomial.polyval(x_p, coefs);\n",
    "sns.lineplot(x=x_p,y=y_p, color='red', label='True function');\n",
    "\n",
    "# Plot fitted function\n",
    "x_p = np.linspace(-1,1,100);\n",
    "y_p = reg_poly1.predict(x_p.reshape(-1,1));\n",
    "sns.lineplot(x=x_p, y=y_p, color='green', label='Model with d=1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of *under-fitting* or *under-modeling*. The estimated\n",
    "function is not able to capture the complexity of the relation between\n",
    "$x$ and $y$ - it is not flexible enough.\n",
    "\n",
    "We can compute the error of this model, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_poly1_train = metrics.mean_squared_error(y_train, reg_poly1.predict(x_train))\n",
    "mse_poly1_train/np.var(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_poly1 = metrics.mean_squared_error(y_test, reg_poly1.predict(x_test))\n",
    "mse_poly1/np.var(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has higher error than the model with $d=3$, which is what we\n",
    "expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose that we tried a model order that was too high, say $d=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_d10 = np.column_stack( [x_train**d for d in np.arange(1,11)])\n",
    "print(x_train_d10.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_poly10 = LinearRegression().fit(x_train_d10,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, label='Test points');\n",
    "plt.xlabel(\"x\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.title(\"Data generated from $1 + 0.5x + 2x^3$\");\n",
    "\n",
    "# Plot true function\n",
    "x_p = np.linspace(-1,1,100);\n",
    "y_p = np.polynomial.polynomial.polyval(x_p, coefs);\n",
    "sns.lineplot(x=x_p,y=y_p, color='red', label='True function');\n",
    "\n",
    "# Plot fitted function\n",
    "x_p = np.linspace(-1,1,100)\n",
    "x_p_d10 = np.column_stack( [x_p.reshape(-1,1)**d for d in np.arange(1,11)]) \n",
    "y_p = reg_poly10.predict(x_p_d10);\n",
    "sns.lineplot(x=x_p, y=y_p, color='green', label='Model with d=10');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test error of this model is higher than the “true” model order, too.\n",
    "But, the training error is smaller!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_poly10_train = metrics.mean_squared_error(y_train, reg_poly10.predict(x_train_d10))\n",
    "mse_poly10_train/np.var(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_d10 = np.column_stack( [x_test**d for d in np.arange(1,11)])\n",
    "mse_poly10 = metrics.mean_squared_error(y_test, reg_poly10.predict(x_test_d10))\n",
    "mse_poly10/np.var(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called *over-fitting* or *over-modeling*. Because the model is\n",
    "very flexible, it is fitting the noise in the data and not the\n",
    "underlying relationship $y=t(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compare the estimated function with the true function we can see\n",
    "the overfitting and underfitting clearly. But, in a real problem, we\n",
    "would not have access to the true function (otherwise, we wouldn’t need\n",
    "to be estimating it). The question then is if we can determine the\n",
    "correct model order from data.\n",
    "\n",
    "This problem is known as the *model order selection* problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One (bad) idea is for each model order to measure the MSE on the\n",
    "training data and select $d$ that minimizes the MSE. To do this, the\n",
    "code below loops over a model order `d = 1,2,...,14` and for each model\n",
    "order, fits a model and measures the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest_list = np.arange(1,15)\n",
    "mse_tr = np.zeros(len(dtest_list))\n",
    "mse_test = np.zeros(len(dtest_list))\n",
    "\n",
    "\n",
    "for didx, dtest in enumerate(dtest_list):\n",
    "\n",
    "    # get transformed features\n",
    "    x_train_dtest = np.column_stack( [x_train**d for d in np.arange(1,(dtest+1))])\n",
    "    x_test_dtest = np.column_stack( [x_test**d for d in np.arange(1,(dtest+1))])\n",
    "\n",
    "    # fit data\n",
    "    reg_dtest = LinearRegression().fit(x_train_dtest,y_train)\n",
    "    \n",
    "    # measure MSE on training data\n",
    "    y_hat = reg_dtest.predict(x_train_dtest)\n",
    "    mse_tr[didx] = metrics.mean_squared_error(y_train, y_hat)/np.var(y_train)\n",
    "\n",
    "    # measure MSE on test data\n",
    "    y_hat_test = reg_dtest.predict(x_test_dtest)\n",
    "    mse_test[didx] = metrics.mean_squared_error(y_test, y_hat_test)/np.var(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=dtest_list,y=mse_tr, marker=\"o\", label=\"Training\");\n",
    "sns.lineplot(x=dtest_list,y=mse_test, marker=\"o\", label=\"Test\");\n",
    "plt.xlabel('Model order');\n",
    "plt.ylabel('MSE/var');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase $d$, the training MSE always decreases. So minimizing MSE\n",
    "on the training data leads to selecting a very high $d$ which in turn\n",
    "results in over-fitting.\n",
    "\n",
    "On the test data, we observe that the test MSE is high at first (due to\n",
    "high bias), then decreases, then increases again (due to high variance).\n",
    "The model with the smallest MSE on this particular test set may or may\n",
    "not be the model that best reflects the true relationship. (The\n",
    "performance on this test set is still subject to the random draw of\n",
    "samples in the test set!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting and underfitting: uninformative features\n",
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting and underfitting can occur even with a “regular” linear\n",
    "regression on the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous lesson, we introduced a Python function that generates\n",
    "data using\n",
    "\n",
    "$$y_i = w_0 + w_1 x_{i,1} + \\ldots + w_d x_{i,d} + \\epsilon_i $$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will introduce a slight variation on this function: we will add\n",
    "the ability to include *uninformative* features. These features are not\n",
    "related to the target variable $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_regression_data(n=100, d=1, coef=[5], intercept=1, sigma=0, uninformative=0):\n",
    "  x = np.random.uniform(-1,1,size=(n,d-uninformative))\n",
    "  y = (np.dot(x, coef) + intercept).squeeze() + sigma * np.random.randn(n)\n",
    "  if uninformative:\n",
    "    x = np.column_stack((x, np.random.randn(n,uninformative)))\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let’s generate some data with 2 informative features (with\n",
    "some noise), and 2 uninformative features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_linear_regression_data(n=100, d=4, coef=[5,5], intercept=1, sigma=1, uninformative=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(10,10));\n",
    "for i in range(4):\n",
    "  p = plt.subplot(2,2,i+1);\n",
    "  p = plt.scatter(x[:,i],  y);\n",
    "  p = plt.xlabel(\"x\" + str(i+1));\n",
    "  p = plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we fit a linear regression model, it is apparent that $x_3$ and\n",
    "$x_4$ do not affect $y$ - the coefficients $w_3$ and $w_4$ are close to\n",
    "zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_demo = LinearRegression().fit(x, y)\n",
    "print(\"Coefficient list: \", reg_demo.coef_)\n",
    "print(\"Intercept: \" , reg_demo.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have many more uninformative features, though, and lots of\n",
    "noise relative to the true relationship?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "d = 95\n",
    "informative = 20\n",
    "coef = np.repeat(1,informative)\n",
    "sigma = 1\n",
    "intercept = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_regression_data(n=n_samples, d=d, coef=coef, intercept=intercept, sigma=sigma, uninformative=d-informative)\n",
    "x_test, y_test = generate_linear_regression_data(n=n_samples, d=d, coef=coef, intercept=intercept, sigma=sigma, uninformative=d-informative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_all = LinearRegression().fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the coefficient value for each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.stem(np.arange(0, d),reg_all.coef_, bottom=0, use_line_collection=True);\n",
    "plt.xticks(np.arange(0, d, 1.0), rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the informative features don’t necessarily have\n",
    "coefficients with the largest magnitudes. The random “relationship”\n",
    "between an uninformative feature $x$ and $y$ appears, in some cases, as\n",
    "strong or stronger than the true relationship between an informative $x$\n",
    "and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compute R2 on the training set, everything looks great:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = reg_all.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_train, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 is great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what happens if we try to use our model to predict $y$ for some new,\n",
    "unseen data, from the same distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = reg_all.predict(x_test)\n",
    "metrics.r2_score(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s not good at all! Due to severe *over-fitting*, the model performs\n",
    "much worse on the test set than it did on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that, to avoid overfitting, we decide to use only a subset of\n",
    "features. We don’t know a priori which features are the most\n",
    "informative. This problem is known as the *feature selection problem*.\n",
    "\n",
    "Perhaps we may decide to try adding one feature at a time to our model,\n",
    "in order, and measure the MSE.\n",
    "\n",
    "(Obviously, there are better ways to do feature selection! But for our\n",
    "demo, this will suffice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest_list = np.arange(1,d)\n",
    "r2_tr = np.zeros(len(dtest_list))\n",
    "r2_test = np.zeros(len(dtest_list))\n",
    "\n",
    "column_order = np.random.permutation(x_train.shape[1])\n",
    "x_train_shuffle = x_train[:, column_order]\n",
    "x_test_shuffle  = x_test[:, column_order]\n",
    "\n",
    "\n",
    "for didx, dtest in enumerate(dtest_list):\n",
    "\n",
    "    # get data with the right number of features\n",
    "    x_train_dtest = x_train_shuffle[:, :dtest]\n",
    "    x_test_dtest = x_test_shuffle[:, :dtest]\n",
    "\n",
    "    # fit data\n",
    "    reg_dtest = LinearRegression().fit(x_train_dtest,y_train)\n",
    "    \n",
    "    # measure R2 on training data\n",
    "    y_hat = reg_dtest.predict(x_train_dtest)\n",
    "    r2_tr[didx] = metrics.r2_score(y_train, y_hat)\n",
    "\n",
    "    # measure R2 on test data\n",
    "    y_hat_test = reg_dtest.predict(x_test_dtest)\n",
    "    r2_test[didx] = metrics.r2_score(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=dtest_list,y=r2_tr, label=\"Training\");\n",
    "sns.lineplot(x=dtest_list,y=r2_test, label=\"Test\");\n",
    "plt.xlabel('Model order');\n",
    "plt.ylabel('R2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous example, where the model performed better on the\n",
    "training set with higher $d$, we see again that adding complexity\n",
    "reduces the training error and increases R2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias and variance\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are trying to estimate some value $y$, which is related\n",
    "to $x$ by some “true” function, $t(x)$.\n",
    "\n",
    "In general, $t(x)$ is not known to us. We do, however, have some “noisy”\n",
    "samples of $y$, generated from\n",
    "\n",
    "$$y = t(x) + \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "We use these training samples to “learn” a function $f(x,w)$\n",
    "parameterized by $w$ such that for samples in the training set,\n",
    "\n",
    "$$y \\approx f(x,w)$$\n",
    "\n",
    "Our ultimate goal, however, is to learn the function that will best\n",
    "approximate samples that we have not yet seen, but that have also been\n",
    "generated from\n",
    "\n",
    "$$y = t(x) + \\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What might go wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "We might try to learn a class of function that is simply not capable of\n",
    "expressing $t(x)$.\n",
    "\n",
    "We observed this in the polynomial example, when we tried learning\n",
    "\n",
    "$$t(x) = w_0 + w_1 x + w_2 x^2 + w_3 x^3$$\n",
    "\n",
    "using\n",
    "\n",
    "$$f(x,w) = w_0 + w_1 x$$\n",
    "\n",
    "(or any $d < 3$.)\n",
    "\n",
    "We also observed this in the example with many features, when we tried\n",
    "learning\n",
    "\n",
    "$$t(x) = w_0 + w_1 x_1 + \\ldots + w_{20} x_{20}$$\n",
    "\n",
    "using a smaller number of features, e.g.\n",
    "\n",
    "$$f(x,w) = w_0 + w_1 x_1 + \\ldots + w_{10} x_{10}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance\n",
    "\n",
    "We might learn the right class of function, but due to the stochastic\n",
    "noise in the training samples, and the random draw of training samples,\n",
    "our parameter estimates are different from the “true” values.\n",
    "\n",
    "We observed this in the polynomial example, when we tried learning\n",
    "\n",
    "$$t(x) = w_0 + w_1 x + w_2 x^2 + w_3 x^3$$\n",
    "\n",
    "using\n",
    "\n",
    "$$f(x,w) = w_0 + w_1 x + \\ldots + w_d x^d$$\n",
    "\n",
    "with $d \\geq 3$. The more complex polynomial *is* capable of expressing\n",
    "$t(x)$ - if we would learn exactly the correct coefficients for\n",
    "$w_i, 0 \\leq i \\leq 3$ and $w_i = 0$ for $i > 3$. But, we had some error\n",
    "in the coefficient estimates, because our coefficient estimates are\n",
    "tuned to the noise in the training data. The more coefficients we learn,\n",
    "the more potential for error.\n",
    "\n",
    "We also observed this in the example with many features, when we tried\n",
    "learning\n",
    "\n",
    "$$t(x) = w_0 + w_1 x_1 + \\ldots + w_{20} x_{20}$$\n",
    "\n",
    "using\n",
    "\n",
    "$$f(x,w) = w_0 + w_1 x_1 + \\ldots + w_d x_d$$\n",
    "\n",
    "with $d \\geq 20$. Again, the model with extra uninformative features is\n",
    "*capable* of expressing $t(x)$ - if we would learn exactly the correct\n",
    "coefficients for $w_i, 0 \\leq i \\leq 20$ and $w_i = 0$ for $i > 20$.\n",
    "But, we had some error in the coefficient estimates, because our\n",
    "coefficient estimates are tuned to the noise in the training data. The\n",
    "more coefficients we learn, the more potential for error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Irreducible error\n",
    "\n",
    "Even if we learn exactly the right function, $f(x,w) = t(x)$, and we\n",
    "also learn the correct parameters, but still have error in our estimate\n",
    "of $y$ because of the stochastic “noise” in the data. We cannot “learn”\n",
    "$\\epsilon$, it is unpredictable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposition of test MSE\n",
    "\n",
    "We know that the expected MSE on a new, unseen test point, can be\n",
    "decomposed into those three factors. Let us denote $E[f(x,w)]$ as\n",
    "$\\bar{f}(x,w)$, then\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Test MSE} &= E [(y - \\hat{y})^2] \\\\\n",
    "&= E [(t(x) + \\epsilon - f(x,w))^2] \\\\\n",
    "&= E (\\epsilon)^2 + E [(t(x) - f(x,w))^2] \\\\\n",
    "&=  E (\\epsilon)^2 + E[t(x) - \\bar{f}(x,w)]^2 + E[(f(x,w) - \\bar{f}(x,w))^2] \\\\\n",
    "&= \\sigma^2 + \\text{Bias}^2 + \\text{Var}(f(x,w))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where the expectation is over the draw of the training set and the\n",
    "noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   The $\\sigma^2$ term captures the random noise in the data, which\n",
    "    cannot be “learned”\n",
    "-   The $E[t(x) - \\bar{f}(x,w)]^2$ term captures any systematic\n",
    "    difference between the function learned by our model and the true\n",
    "    function, as will occur when the model cannot express the true\n",
    "    function.\n",
    "-   The $E[(f(x,w) - \\bar{f}(x,w))^2]$ term captures the difference\n",
    "    between the coefficient estimate due to a particular random training\n",
    "    set, and the average coefficient estimate - in other words, the\n",
    "    variance of the prediction over many fitted instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and variance for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall state here the following results, without proof, for a linear\n",
    "regression trained using least squares, with $n$ samples and having $d$\n",
    "parameters. We also assume that $n \\geq d$ and the data matrix is full\n",
    "rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bias**: If there is no under-modeling, then on average, the estimate\n",
    "of $y$ is unbiased, i.e.\n",
    "\n",
    "$$\\text{Bias}(f(x,w)) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance**: On average, the variance increases linearly with the\n",
    "number of parameters $d$ and inversely with the number of samples used\n",
    "for training $n$:\n",
    "\n",
    "$$\\text{Var}(f(x,w)) = \\frac{d}{n} \\sigma^2 $$\n",
    "\n",
    "where \\$ \\\\sigma^2\\$ is the stochastic noise variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your understanding with simulation\n",
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following pairs of regression models, answer:\n",
    "\n",
    "-   Which model in each pair will have greater expected variance of its\n",
    "    estimate of $y$?\n",
    "-   Which model in each pair will have greater bias?\n",
    "\n",
    "(Assume that the two models in each pair are identical except for the\n",
    "differences that are specified.)\n",
    "\n",
    "Then, we’ll check our answer with a simulation experiment, in which we:\n",
    "\n",
    "-   Draw a test set from the distribution of the data\n",
    "-   Many times:\n",
    "    -   Draw a training set from the distribution of the data\n",
    "    -   Fit each model on the training set\n",
    "    -   Compute error on the test set\n",
    "\n",
    "**Note**: we can’t do this when applying machine learning to a real\n",
    "problem with real data! Why not?\n",
    "\n",
    "**Note**: this procedure won’t give us the *exact* bias and variance of\n",
    "the model that we could compute using the closed form expression - why\n",
    "not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant model vs mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that data is generated as\n",
    "\n",
    "$$y = 1 + \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "We will consider two models:\n",
    "\n",
    "**Model A** is a constant model:\n",
    "\n",
    "$$\\hat{y} = 1$$\n",
    "\n",
    "**Model B** is the mean of the training samples.\n",
    "\n",
    "$$\\hat{y} = \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 1000\n",
    "n_test = 50\n",
    "n_train = 5\n",
    "sigma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data once\n",
    "x_test = np.random.uniform(-1, 1, size=(n_test, 1))\n",
    "y_test = np.ones(n_test) + sigma*np.random.randn(n_test)\n",
    "y_test_no_noise = np.ones(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare some matrices for storing simulation results\n",
    "y_predict = np.zeros((n_test, n_repeat, 2))\n",
    "\n",
    "y_test_err = np.zeros((n_test, n_repeat, 2))\n",
    "y_test_bias = np.zeros((n_test, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in range(n_repeat):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  # note: we don't need X data since y is independent of X\n",
    "  y_train = np.ones(n_train) + sigma*np.random.randn(n_train)\n",
    "\n",
    "  # model A: predict constant always\n",
    "  y_predict[:, i, 0] = np.repeat(1, n_test) \n",
    "  # model B: predict mean of training samples\n",
    "  y_predict[:, i, 1] = np.mean(y_train)\n",
    "\n",
    "  # overall squared error is due to difference between f_t(x) + epsilon, model prediction f(x,w) \n",
    "  y_test_err[:, i, 0] = (y_test - y_predict[:, i, 0])**2\n",
    "  y_test_err[:, i, 1] = (y_test - y_predict[:, i, 1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias is due to difference between f_t(x) and mean of model prediction f(x,w) \n",
    "y_test_bias[:, 0] = (y_test_no_noise - y_predict[:, :, 0].mean(axis=1))**2\n",
    "y_test_bias[:, 1] = (y_test_no_noise - y_predict[:, :, 1].mean(axis=1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irreducible error is due to the difference between f_t(x) and f_t(x) + epsilon\n",
    "y_test_noise_var = (y_test_no_noise - y_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(10,10));\n",
    "\n",
    "models = ['Model A', 'Model B']\n",
    "\n",
    "for midx, model in enumerate(models):\n",
    "  p = plt.subplot(2, 2, midx+1);\n",
    "  for i in range(n_repeat):\n",
    "    p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, i, midx], alpha=0.05, color='orange');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].mean(axis=1), color='red', label='Mean of models');\n",
    "  p = sns.scatterplot(x=x_test[0:50].squeeze(), y=y_test[0:50], label='Test data');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('y');\n",
    "  p = plt.title(model);\n",
    "  p = plt.ylim(y_test.min()*1.1, y_test.max()*1.1)\n",
    " \n",
    "  p = plt.subplot(2, 2, midx+3);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_err[:, :, midx].mean(axis=1), color='red', alpha=0.5, label='Total squared error', markers=True);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_bias[:, midx], color='cyan', alpha=0.5, label='Bias^2');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].var(axis=1), color='green', alpha=0.5, label='Variance');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_noise_var, color='purple', alpha=0.2, label='Irreducible error');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('Error');\n",
    "  p = plt.title(\"MSE: %f \\n Irreducible error: %f \\n Bias^2: %f \\n Variance: %f \" % \n",
    "                (y_test_err[:,:,midx].mean(axis=(0,1)), \n",
    "                 y_test_noise_var.mean(), \n",
    "                 (y_test_bias[:, midx]).mean(),\n",
    "                  y_predict[:, :, midx].var(axis=1).mean() ) )\n",
    "  p = plt.ylim(0, 1.1* y_test_err[:, :, :].mean(axis=(1)).max() )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "\n",
    "-   Are the specific values of bias and variance consistent with our\n",
    "    expectations?\n",
    "-   What would happen if \\$\\\\sigma = 0 \\$?\n",
    "-   What would happen if we used $\\hat{y} = 2$ for the constant model?\n",
    "-   What would happen if we would use $\\hat{y} = \\bar{y} + 2$ instead of\n",
    "    $\\bar{y}$ for the mean model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean vs. linear model with d=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that data is generated as\n",
    "\n",
    "$$y = 1 + \\epsilon$$\n",
    "\n",
    "where $\\epsilon \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "We will consider two models:\n",
    "\n",
    "**Model A** is a linear model with $d=1$ (using an uninformative\n",
    "feature, since $y$ is not a function of any $x$), trained using least\n",
    "squares:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x$$\n",
    "\n",
    "**Model B** is the mean of the training samples.\n",
    "\n",
    "$$\\hat{y} = \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 1000\n",
    "n_test = 50\n",
    "n_train = 5\n",
    "sigma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data once\n",
    "x_test = np.random.uniform(-1, 1, size=(n_test, 1))\n",
    "y_test = np.ones(n_test) + sigma*np.random.randn(n_test)\n",
    "y_test_no_noise = np.ones(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare some matrices for storing simulation results\n",
    "y_predict = np.zeros((n_test, n_repeat, 2))\n",
    "\n",
    "y_test_err = np.zeros((n_test, n_repeat, 2))\n",
    "y_test_bias = np.zeros((n_test, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in range(n_repeat):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  y_train = np.ones(n_train) + sigma*np.random.randn(n_train)\n",
    "  x_train = np.random.uniform(-1,1, size=(n_train, 1))\n",
    "\n",
    "  # model A: fit linear model\n",
    "  reg_a = LinearRegression().fit(x_train, y_train)\n",
    "  y_predict[:, i, 0] = reg_a.predict(x_test)\n",
    "  # model B: predict mean of training samples\n",
    "  y_predict[:, i, 1] = np.mean(y_train)\n",
    "\n",
    "  # overall squared error is due to difference between f_t(x) + epsilon, model prediction f(x,w) \n",
    "  y_test_err[:, i, 0] = (y_test - y_predict[:, i, 0])**2\n",
    "  y_test_err[:, i, 1] = (y_test - y_predict[:, i, 1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias is due to difference between f_t(x) and mean of model prediction f(x,w) \n",
    "y_test_bias[:, 0] = (y_test_no_noise - y_predict[:, :, 0].mean(axis=1))**2\n",
    "y_test_bias[:, 1] = (y_test_no_noise - y_predict[:, :, 1].mean(axis=1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irreducible error is due to the difference between f_t(x) and f_t(x) + epsilon\n",
    "y_test_noise_var = (y_test_no_noise - y_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(10,10));\n",
    "\n",
    "models = ['Model A', 'Model B']\n",
    "\n",
    "for midx, model in enumerate(models):\n",
    "  p = plt.subplot(2, 2, midx+1);\n",
    "  for i in range(n_repeat):\n",
    "    p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, i, midx], alpha=0.05, color='orange');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].mean(axis=1), color='red', label='Mean of models');\n",
    "  p = sns.scatterplot(x=x_test[0:50].squeeze(), y=y_test[0:50], label='Test data');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('y');\n",
    "  p = plt.title(model);\n",
    "  p = plt.ylim(y_test.min()*1.1, y_test.max()*1.1)\n",
    "\n",
    "  p = plt.subplot(2, 2, midx+3);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_err[:, :, midx].mean(axis=1), color='red', alpha=0.5, label='Total squared error', markers=True);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_bias[:, midx], color='cyan', alpha=0.5, label='Bias^2');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].var(axis=1), color='green', alpha=0.5, label='Variance');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_noise_var, color='purple', alpha=0.2, label='Irreducible error');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('Error');\n",
    "  p = plt.title(\"MSE: %f \\n Irreducible error: %f \\n Bias^2: %f \\n Variance: %f \" % \n",
    "                (y_test_err[:,:,midx].mean(axis=(0,1)), \n",
    "                 y_test_noise_var.mean(), \n",
    "                 (y_test_bias[:, midx]).mean(),\n",
    "                  y_predict[:, :, midx].var(axis=1).mean() ) )\n",
    "  p = plt.ylim(0, 1.1* y_test_err[:, :, :].mean(axis=(1)).max() )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model with 10 vs. 1000 training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model A**:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x_1$$\n",
    "\n",
    "with least squares estimate of parameters, $n = 10$ training samples.\n",
    "\n",
    "**Model B**:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x_1$$\n",
    "\n",
    "with least squares estimate of parameters, $n = 1000$ training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 1000\n",
    "n_test = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.zeros((n_test, n_repeat, 2))\n",
    "\n",
    "y_test_err = np.zeros((n_test, n_repeat, 2))\n",
    "y_test_bias = np.zeros((n_test, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = [5]\n",
    "intercept = 1\n",
    "sigma = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data once\n",
    "x_test, y_test = generate_linear_regression_data(n=n_test, d=1, coef=[5], intercept=1, sigma=sigma)\n",
    "y_test_no_noise = intercept + np.dot(x_test, coef) \n",
    "# note: y_test is f_t(x) + epsilon\n",
    "# note: y_test_no_noise is f_t(x) \n",
    "\n",
    "# noise is due to the difference between f_t(x) and f_t(x) + epsilon\n",
    "y_test_noise_var = (y_test_no_noise - y_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in range(n_repeat):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  x, y = generate_linear_regression_data(n=1000, d=1, coef=coef, intercept=intercept, sigma=sigma)\n",
    "  x_a, y_a = x[0:10], y[0:10]\n",
    "  reg_a = LinearRegression().fit(x_a, y_a)\n",
    "\n",
    "  x_b, y_b = x, y\n",
    "  reg_b = LinearRegression().fit(x_b, y_b)\n",
    "\n",
    "  y_predict[:, i, 0] = reg_a.predict(x_test)\n",
    "  y_predict[:, i, 1] = reg_b.predict(x_test)\n",
    "\n",
    "  # overall squared error is due to difference between f_t(x) + epsilon, model prediction f(x,w) \n",
    "  y_test_err[:, i, 0] = (y_test - y_predict[:, i, 0])**2\n",
    "  y_test_err[:, i, 1] = (y_test - y_predict[:, i, 1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias is due to difference between f_t(x) and mean of model prediction f(x,w) \n",
    "y_test_bias[:, 0] = (y_test_no_noise - y_predict[:, :, 0].mean(axis=1))**2\n",
    "y_test_bias[:, 1] = (y_test_no_noise - y_predict[:, :, 1].mean(axis=1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(10,10));\n",
    "\n",
    "models = ['Model A', 'Model B']\n",
    "\n",
    "for midx, model in enumerate(models):\n",
    "  p = plt.subplot(2, 2, midx+1);\n",
    "  for i in range(n_repeat):\n",
    "    p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, i, midx], alpha=0.05, color='orange');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].mean(axis=1), color='red', label='Mean of models');\n",
    "  p = sns.scatterplot(x=x_test.squeeze(), y=y_test, label='Test data');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('y');\n",
    "  p = plt.title(model);\n",
    "  p = plt.ylim(y_test.min()*1.1, y_test.max()*1.1)\n",
    "\n",
    "  p = plt.subplot(2, 2, midx+3);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_err[:, :, midx].mean(axis=1), color='red', alpha=0.5, label='Total squared error', markers=True);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_bias[:, midx], color='cyan', alpha=0.5, label='Bias^2');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].var(axis=1), color='green', alpha=0.5, label='Variance');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_noise_var, color='purple', alpha=0.2, label='Irreducible error');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('Error');\n",
    "  p = plt.title(\"MSE: %f \\n Irreducible error: %f \\n Bias^2: %f \\n Variance: %f \" % \n",
    "                (y_test_err[:,:,midx].mean(axis=(0,1)), \n",
    "                 y_test_noise_var.mean(), \n",
    "                 (y_test_bias[:, midx]).mean(),\n",
    "                  y_predict[:, :, midx].var(axis=1).mean() ) )\n",
    "  p = plt.ylim(0, 1.1* y_test_err[:, :, :].mean(axis=(1)).max() )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear vs. polynomial model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is generated as $y_i = 1 + 0.5x_i + 2x_i^3 + \\epsilon_i$.\n",
    "\n",
    "**Model A**:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x$$\n",
    "\n",
    "with least squares estimate of parameters.\n",
    "\n",
    "**Model B**:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x + w_2 x^2 + w_3 x^3$$\n",
    "\n",
    "with least squares estimate of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 1000\n",
    "n_test = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = np.zeros((n_test, n_repeat, 2))\n",
    "\n",
    "y_test_err = np.zeros((n_test, n_repeat, 2))\n",
    "y_test_bias = np.zeros((n_test, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs=[1,0.5,0,2]\n",
    "sigma = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test data once\n",
    "x_test, y_test = generate_polynomial_regression_data(n=n_test, coefs=coefs, sigma=sigma)\n",
    "y_test_no_noise = np.polynomial.polynomial.polyval(x_test, coefs).squeeze()\n",
    "# note: y_test is f_t(x) + epsilon\n",
    "# note: y_test_no_noise is f_t(x) \n",
    "\n",
    "# noise is due to the difference between f_t(x) and f_t(x) + epsilon\n",
    "y_test_noise_var = (y_test_no_noise - y_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in range(n_repeat):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  x, y = generate_polynomial_regression_data(n=100, coefs=coefs, sigma=sigma)\n",
    "\n",
    "  x_a, y_a = x, y\n",
    "  reg_a = LinearRegression().fit(x_a, y_a)\n",
    "\n",
    "  x_b_d3 = np.column_stack( [x.reshape(-1,1)**d for d in np.arange(1,4)])\n",
    "  y_b = y\n",
    "  reg_b = LinearRegression().fit(x_b_d3, y_b)\n",
    "\n",
    "  y_predict[:, i, 0] = reg_a.predict(x_test)\n",
    "  x_test_d3 = np.column_stack( [x_test.reshape(-1,1)**d for d in np.arange(1,4)])\n",
    "  y_predict[:, i, 1] = reg_b.predict(x_test_d3)\n",
    "\n",
    "  # overall squared error is due to difference between f_t(x) + epsilon, model prediction f(x,w) \n",
    "  y_test_err[:, i, 0] = (y_test - y_predict[:, i, 0])**2\n",
    "  y_test_err[:, i, 1] = (y_test - y_predict[:, i, 1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias is due to difference between f_t(x) and mean of model prediction f(x,w) \n",
    "y_test_bias[:, 0] = (y_test_no_noise - y_predict[:, :, 0].mean(axis=1))**2\n",
    "y_test_bias[:, 1] = (y_test_no_noise - y_predict[:, :, 1].mean(axis=1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(10,10));\n",
    "\n",
    "models = ['Model A', 'Model B']\n",
    "\n",
    "for midx, model in enumerate(models):\n",
    "  p = plt.subplot(2, 2, midx+1);\n",
    "  for i in range(n_repeat):\n",
    "    p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, i, midx], alpha=0.05, color='orange');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].mean(axis=1), color='red', label='Mean of models');\n",
    "  p = sns.scatterplot(x=x_test.squeeze(), y=y_test, label='Test data');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('y');\n",
    "  p = plt.title(model);\n",
    "  p = plt.ylim(y_test.min()*1.1, y_test.max()*1.1)\n",
    "\n",
    "  p = plt.subplot(2, 2, midx+3);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_err[:, :, midx].mean(axis=1), color='red', alpha=0.5, label='Total squared error', markers=True);\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_bias[:, midx], color='cyan', alpha=0.5, label='Bias^2');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_predict[:, :, midx].var(axis=1), color='green', alpha=0.5, label='Variance');\n",
    "  p = sns.lineplot(x=x_test.squeeze(), y=y_test_noise_var, color='purple', alpha=0.2, label='Irreducible error');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('Error');\n",
    "  p = plt.title(\"MSE: %f \\n Irreducible error: %f \\n Bias^2: %f \\n Variance: %f \" % \n",
    "                (y_test_err[:,:,midx].mean(axis=(0,1)), \n",
    "                 y_test_noise_var.mean(), \n",
    "                 (y_test_bias[:, midx]).mean(),\n",
    "                  y_predict[:, :, midx].var(axis=1).mean() ) )\n",
    "  p = plt.ylim(0, 1.1* y_test_err[:, :, :].mean(axis=(1)).max() )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "-   What would happen if you increase or decrease $\\sigma$ in this\n",
    "    example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection\n",
    "---------------\n",
    "\n",
    "We know that (in the “classic” view), the test error of a model tends to\n",
    "decrease and then increase as we increase model complexity. For low\n",
    "complexity, the bias dominates; for high complexity, the variance\n",
    "dominates.\n",
    "\n",
    "The training error, however, only decreases with increasing model\n",
    "complexity. If we use training error to select a model, we’ll select a\n",
    "model that overfits. And during training, when we select a model, only\n",
    "the training error is available to us.\n",
    "\n",
    "![Image from “Elements of Statistical\n",
    "Learning”](https://i.stack.imgur.com/alkeM.png)\n",
    "\n",
    "*Image source: Elements of Statistical Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is cross validation. Until now, we have been dividing our\n",
    "data into two parts:\n",
    "\n",
    "-   Training data: used to train the model\n",
    "-   Test data: used to evaluate the performance of our model on new,\n",
    "    unseen data\n",
    "\n",
    "Now, we will make one more split:\n",
    "\n",
    "-   Training data: used to train the model\n",
    "-   Validation data: used to select the model complexity (usually by\n",
    "    tuning some *hyperparameters*)\n",
    "-   Test data: used to evaluate the performance of our model on new,\n",
    "    unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we will refine this idea in order to reduce the dependence\n",
    "on the particular samples we choose for the training, and to increase\n",
    "the number of samples available for training. In K-fold cross\n",
    "validation, we split the data into $K$ parts, each part being\n",
    "approximately equal in size. For each split, we fit the data on $K-1$\n",
    "parts and test the data on the remaining part. Then, we average the\n",
    "score over the $K$ parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for $K=5$, it might look like this:\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection using best K-Fold CV score\n",
    "\n",
    "First, we will try to use K-fold CV to select a polynomial model to fit\n",
    "the data in our first example.\n",
    "\n",
    "We will use the `scikit-learn` module for K-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs=[1,0.5,0,2]\n",
    "n_samples = 500\n",
    "sigma = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate polynimal data\n",
    "x, y = generate_polynomial_regression_data(n=n_samples, coefs=coefs, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide into training and test set\n",
    "# (we will later divide training data again, into training and validation set)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a k-fold object\n",
    "nfold = 10\n",
    "kf = KFold(n_splits=nfold,shuffle=True)\n",
    "\n",
    "# model orders to be tested\n",
    "dtest_list = np.arange(1,10)\n",
    "nd = len(dtest_list)\n",
    "\n",
    "mse_val = np.zeros((nd,nfold))\n",
    "r2_val  = np.zeros((nd,nfold))\n",
    "\n",
    "# loop over the folds\n",
    "# the first loop variable tells us how many out of nfold folds we have gone through\n",
    "# the second loop variable tells us how to split the data\n",
    "for isplit, idx in enumerate(kf.split(x_train)):\n",
    "        \n",
    "    # these are the indices for the training and validation indices\n",
    "    # for this iteration of the k folds\n",
    "    idx_tr, idx_val = idx \n",
    "\n",
    "    x_train_kfold = x_train[idx_tr]\n",
    "    y_train_kfold = y_train[idx_tr]\n",
    "    x_val_kfold = x_train[idx_val]\n",
    "    y_val_kfold = y_train[idx_val]\n",
    "\n",
    "    for didx, dtest in enumerate(dtest_list):\n",
    "\n",
    "      # get transformed features\n",
    "      x_train_dtest = np.column_stack( [x_train_kfold**d for d in np.arange(1,(dtest+1))])\n",
    "      x_val_dtest = np.column_stack( [x_val_kfold**d for d in np.arange(1,(dtest+1))])\n",
    "\n",
    "      # fit data\n",
    "      reg_dtest = LinearRegression().fit(x_train_dtest, y_train_kfold)\n",
    "      \n",
    "      # measure MSE on validation data\n",
    "      y_hat = reg_dtest.predict(x_val_dtest)\n",
    "      mse_val[didx, isplit] = metrics.mean_squared_error(y_val_kfold, y_hat)\n",
    "      r2_val[didx, isplit] = metrics.r2_score(y_val_kfold, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=dtest_list, y=mse_val.mean(axis=1), marker='o');\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold MSE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see which model order gave us the lowest MSE on the validation\n",
    "data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_min = np.argmin(mse_val.mean(axis=1))\n",
    "d_min_mse = dtest_list[idx_min]\n",
    "d_min_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=dtest_list, y=mse_val.mean(axis=1));\n",
    "sns.scatterplot(x=dtest_list, y=mse_val.mean(axis=1), hue=dtest_list==d_min_mse, s=100, legend=False);\n",
    "\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold MSE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select by highest R2 (instead of lowest MSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = np.argmax(r2_val.mean(axis=1))\n",
    "d_max_r2 = dtest_list[idx_max]\n",
    "d_max_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=dtest_list, y=r2_val.mean(axis=1));\n",
    "sns.scatterplot(x=dtest_list, y=r2_val.mean(axis=1), hue=dtest_list==d_max_r2, s=100, legend=False);\n",
    "\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold R2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection using 1-SE “rule”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the minimum K-fold CV error for model selection, we sometimes\n",
    "will still select an overly complex model <sup>\\[2\\]</sup>.\n",
    "\n",
    "As an alternative, we can use the “one standard error rule”\n",
    "<sup>\\[3\\]</sup>. According to this “rule”, we choose the least complex\n",
    "model whose error is no more than one standard error above the error of\n",
    "the best model - i.e. the simplest model whose performance is comparable\n",
    "to the best model.\n",
    "\n",
    "<small>\\[2\\] See [Cawley & Talbot (J of Machine Learning Research,\n",
    "2010)](http://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf) for\n",
    "more on this.</small>\n",
    "\n",
    "<small>\\[3\\] See Chapter 7 of [Elements of Statistical\n",
    "Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) </small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this rule as follows:\n",
    "\n",
    "-   Find the MSE for each fold for each model candidate\n",
    "-   For each model candidate, compute the mean and standard error of the\n",
    "    MSE over the $K$ folds. We will compute the standard error as\n",
    "    $$\\frac{\\sigma_{\\text{MSE}}}{\\sqrt{K-1}}$$ where\n",
    "    $\\sigma_{\\text{MSE}}$ is the standard deviation of the MSE over the\n",
    "    $K$ folds.\n",
    "-   Find the model with the smallest mean MSE (across the $K$ folds).\n",
    "    Compute the *target* as mean MSE + SE for this model.\n",
    "-   Select the least complex model whose mean MSE is below the target.\n",
    "\n",
    "This works for any metric that is a “lower is better” metric. If you are\n",
    "using a “higher is better” metric, such as R2, for example, you would\n",
    "modify the last two steps:\n",
    "\n",
    "-   Find the model with the **largest** mean R2 (across the $K$ folds).\n",
    "    Compute the **mean R2 - SE of R2** for this model. Call this\n",
    "    quantity the *target*.\n",
    "-   Select the least complex model whose mean R2 is **above** the\n",
    "    target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_min = np.argmin(mse_val.mean(axis=1))\n",
    "target = mse_val[idx_min,:].mean() + mse_val[idx_min,:].std()/np.sqrt(nfold-1)\n",
    "# np.where returns indices of values where condition is satisfied\n",
    "idx_one_se = np.where(mse_val.mean(axis=1) < target)\n",
    "d_one_se = np.min(dtest_list[idx_one_se])\n",
    "d_one_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x=dtest_list, y=mse_val.mean(axis=1), yerr=mse_val.std(axis=1)/np.sqrt(nfold-1));\n",
    "plt.hlines(y=target, xmin=np.min(dtest_list), xmax=np.max(dtest_list), ls='dotted')\n",
    "sns.scatterplot(x=dtest_list, y=mse_val.mean(axis=1), hue=dtest_list==d_one_se, s=100, legend=False);\n",
    "\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold MSE\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_max = np.argmax(r2_val.mean(axis=1))\n",
    "target_r2 = r2_val[idx_max,:].mean() - r2_val[idx_max,:].std()/np.sqrt(nfold-1)\n",
    "# np.where returns indices of values where condition is satisfied\n",
    "idx_one_se_r2 = np.where(r2_val.mean(axis=1) > target_r2)\n",
    "d_one_se_r2 = np.min(dtest_list[idx_one_se_r2])\n",
    "d_one_se_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x=dtest_list, y=r2_val.mean(axis=1), yerr=r2_val.std(axis=1)/np.sqrt(nfold-1));\n",
    "plt.hlines(y=target_r2, xmin=np.min(dtest_list), xmax=np.max(dtest_list), ls='dotted')\n",
    "sns.scatterplot(x=dtest_list, y=r2_val.mean(axis=1), hue=dtest_list==d_one_se_r2, s=100, legend=False);\n",
    "\n",
    "plt.xlabel(\"Model order\");\n",
    "plt.ylabel(\"K-fold R2\");"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
