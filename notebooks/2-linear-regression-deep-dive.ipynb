{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression in depth\n",
    "\n",
    "*Fraida Fund*"
   ],
   "id": "ea5af6a6-3107-461d-afbf-ec44e9e7ed6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# for 3d interactive plots\n",
    "from ipywidgets import interact, fixed, widgets\n",
    "from mpl_toolkits import mplot3d"
   ],
   "id": "a09b7f0e-0c2b-4d06-9f76-5ca868d181d4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generated by a linear function"
   ],
   "id": "cd5c37dd-3022-4604-924a-799efa45444d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a process that generates data as\n",
    "\n",
    "$$y_i = w_0 + w_1 x_{i,1} + \\ldots + w_d x_{i,d} + \\epsilon_i $$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$."
   ],
   "id": "66b0bf37-37f0-4780-a1d6-6aafcc5590b7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in this example, we use a “stochastic error” term. This is not to be confused with a residual term which can include systematic, non-random error.\n",
    "\n",
    "-   stochastic error: difference between observed value and “true” value. These random errors are independent, not systematic, and cannot be “learned” by any machine learning model.\n",
    "-   residual: difference between observed value and estimated value. These errors are typical *not* independent, and they can be systematic."
   ],
   "id": "48782f40-b206-4bad-8199-c3713adede30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a function to generate this kind of data"
   ],
   "id": "0aee70cf-1b96-4968-9a81-dbc83d6c2e54"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_regression_data(n=100, d=1, coef=[5], intercept=1, sigma=0):\n",
    "  x = np.random.randn(n,d)\n",
    "  y = (np.dot(x, coef) + intercept).squeeze() + sigma * np.random.randn(n)\n",
    "  return x, y"
   ],
   "id": "fe2773e3-ea0b-4a50-aeea-57e34d868916"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and some default values we’ll use:"
   ],
   "id": "cdc5c5e5-8632-4faa-ba7e-668ee6b6880d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "coef = [5]\n",
    "intercept = 1"
   ],
   "id": "d0d3c84d-f5b1-4dd8-87bd-34cd478ff580"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear regression"
   ],
   "id": "cf55107b-8f67-48e6-9052-c77cf5df76ff"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ],
   "id": "0b3a83df-57c1-48c5-ae39-27c2927e4e1c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_regression_data(n=n_samples, d=1, coef=coef, intercept=intercept)\n",
    "x_test,  y_test  = generate_linear_regression_data(n=50, d=1, coef=coef, intercept=intercept)"
   ],
   "id": "8d07603e-8ce1-45ee-aa5f-898185527bc9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_train.squeeze(), y=y_train, s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ],
   "id": "e6719713-8409-43f7-b3ff-e78b41a20b3a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we generated `x` as a 2D array with `n_samples` rows and 1 column, but to plot it we need a 1D array. In our “crash course” lecture, we introduced the `squeeze()` function that removes any dimension with size 1, so the result here is a 1D array. We could also have used `x_train.reshape(-1,)`."
   ],
   "id": "cce21342-ed41-42e4-93b3-c59d31786f16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression"
   ],
   "id": "37efbd4b-722e-43b7-a72d-5e15b5c4105c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the “classical machine learning” part of this course, we’ll use `scikit-learn` implementations of most ML models. These all follow the same standard format, so once you learn how to use one, you know the basic usage of all of them.\n",
    "\n",
    "The basic format is:\n",
    "\n",
    "``` python\n",
    "m = Model()        # create an instance of the model - whatever type it is\n",
    "m.fit(x_tr, y_tr)  # fit the model using the training data. Note: x_tr must be 2D\n",
    "                   # if x_tr is 1D, make it 2D by passing x_tr.reshape((-1,1)) or x_tr[:,None]\n",
    "\n",
    "y_tr_hat = m.predict(x_tr)  # now get model prediction on training data (note: x_tr still must be 2D!)\n",
    "y_ts_hat = m.predict(x_ts)  # also get model prediction on test data (note: x_ts must be 2D!)\n",
    "\n",
    "\n",
    "metrics.mean_squared_error(y_ts, y_ts_hat)  # for regression: get error on the test data\n",
    "metrics.r2_score(y_ts, y_ts_hat)            # or R2 on the test data\n",
    "m.score(x_ts, y_ts)                         # another way to get test data performance (R2 for regression)\n",
    "```\n",
    "\n",
    "Many models have some additional arguments you can set, or parameters you can check after fitting - check the documentation for details. For example, you can review the [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model documentation.\n",
    "\n",
    "Here’s how this would apply for a `LinearRegression()` model."
   ],
   "id": "31ce7fe8-94be-4952-8d19-54d87d06a63b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_simple = LinearRegression().fit(x_train, y_train)\n",
    "print(\"Intercept: \" , reg_simple.intercept_)\n",
    "print(\"Coefficient list: \", reg_simple.coef_)"
   ],
   "id": "5cd2e6f5-9966-4e65-a059-7db63fdb1c12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_train), np.max(x_train)]\n",
    "y_line = x_line*reg_simple.coef_ + reg_simple.intercept_\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, s=50, color=sns.color_palette()[3]);\n",
    "sns.scatterplot(x=x_train.squeeze(), y=y_train, s=50);\n",
    "sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[1]);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ],
   "id": "236eb2c5-2f36-49b6-8b61-a34e6e7a7104"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: other ways to do the same thing...\n",
    "# first, add a ones column to design matrix\n",
    "x_tilde = np.hstack((np.ones((n_samples, 1)), x_train))\n",
    "\n",
    "# using matrix operations to find w = (X^T X)^{-1} X^T y\n",
    "print( (np.linalg.inv((x_tilde.T @ x_tilde)) @ x_tilde.T @ y_train) )\n",
    "\n",
    "# using solve on normal equations: X^T X w = X^T y\n",
    "# solve only works on matrix that is square and of full-rank\n",
    "# see https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html\n",
    "print( np.linalg.solve(x_tilde.T @ x_tilde, x_tilde.T @ y_train) )\n",
    "\n",
    "# using the lstsq solver \n",
    "# problem may be under-, well-, or over-determined\n",
    "# see https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html\n",
    "print( np.linalg.lstsq(x_tilde,y_train,rcond=0)[0] ) "
   ],
   "id": "f7ee0b6c-1bd5-4884-87b3-682dcfc8c2de"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mean-removed equivalent\n",
    "\n",
    "Quick digression - what if we don’t want to bother with intercept?"
   ],
   "id": "5dd1f242-d328-4f83-9843-fa4248573e4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mr = x_train - np.mean(x_train)\n",
    "y_train_mr = y_train - np.mean(y_train)\n",
    "sns.scatterplot(x=x_train_mr.squeeze(), y=y_train_mr, s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ],
   "id": "fb8cb8ac-29c8-4e09-bafe-1e14eb8e6c4e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that now the data is mean removed - zero mean in every dimension. (Removing the mean is also called *centering* the data.)\n",
    "\n",
    "This time, the fitted linear regression has 0 intercept:\n",
    "\n",
    "(We could have specified `fit_intercept=False` as an argument to the model, but we didn’t so that we could see for ourselves that the intercept is zero!)"
   ],
   "id": "1c80449f-c71a-4eba-ab4d-e23ca3e9ddca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_mr = LinearRegression().fit(x_train_mr, y_train_mr)\n",
    "print(\"Intercept: \" , reg_mr.intercept_)\n",
    "print(\"Coefficient list: \", reg_mr.coef_)"
   ],
   "id": "01463248-abc1-4ee8-bdb8-31443ad878ad"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: when pre-processing data (for example, scaling, or removing the mean), we will always use the training data *only* to get the pre-processing parameters. For example, to get the mean-removed test data we would use\n",
    "\n",
    "``` python\n",
    "x_test_mr = x_test - np.mean(x_train)\n",
    "y_test_mr = y_test - np.mean(y_train)\n",
    "```"
   ],
   "id": "bca49245-cd26-4309-aff4-d8c6178d3be4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict some new points\n",
    "\n",
    "OK, now we can predict some new points:"
   ],
   "id": "0fb2bc8d-5d14-4377-9ef4-fd96cc291e41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = reg_simple.predict(x_test)"
   ],
   "id": "eb4a625d-12ae-4952-9908-52493895a7c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_simple.coef_ + reg_simple.intercept_"
   ],
   "id": "9958c58e-259e-4eb1-976b-8fdfd0b3cdec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[1]);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test_hat, s=50, color=sns.color_palette()[2]);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ],
   "id": "3d7b894e-56b7-407b-84ea-a4750e84a632"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute MSE"
   ],
   "id": "9c8f1447-e872-4d70-849a-5c617d44914d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we will compute the MSE on the test data (*not* the data used to find the parameters).\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - (w_0 + w_1 x_i)) ^2$$\n",
    "\n",
    "Use $\\hat{y}_i = w_0 + w_1 x_i$, then\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i) ^2$$"
   ],
   "id": "c04aedc2-76e8-45e6-9a59-ac068aeb53d6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the `numpy` way:"
   ],
   "id": "7bd9dffb-6d1b-437e-a6f2-70407c690028"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = reg_simple.intercept_ + np.dot(x_test,reg_simple.coef_)\n",
    "mse_simple = 1.0/(len(y_test)) * np.sum((y_test - y_test_hat)**2)\n",
    "mse_simple"
   ],
   "id": "3f802930-8c8d-40fc-9a5d-99964c6f115b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the `scikit-learn` way:"
   ],
   "id": "2a93b211-9dbe-45ca-a80b-d3bc1d7c62f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to do the same thing using sklearn\n",
    "y_test_hat = reg_simple.predict(x_test)\n",
    "metrics.mean_squared_error(y_test, y_test_hat)"
   ],
   "id": "aa1b023e-676e-4895-aef3-0e34a9e896f7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize MSE for different coefficients"
   ],
   "id": "4113c7d1-a622-4e51-bf67-bb1f4c83b46e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(2, 8, 0.5)\n",
    "\n",
    "x_line_c = np.array([np.min(x_train), np.max(x_train)])\n",
    "y_line_c = coefs.reshape(-1,1)*x_line_c.reshape(1,-1) + reg_simple.intercept_\n",
    "\n",
    "p = sns.scatterplot(x=x_test.squeeze(), y=y_test_hat, s=50);\n",
    "p = plt.xlabel('x')\n",
    "p = plt.ylabel('y')\n",
    "for idx, c in enumerate(coefs):\n",
    "  p = sns.lineplot(x=x_line_c, y=y_line_c[idx], color=sns.color_palette()[1], alpha=0.2);"
   ],
   "id": "724a71f4-0b5f-4904-b29c-ca057d22522b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat_c = coefs.reshape(-1,1)*x_test.reshape(1,-1) + reg_simple.intercept_\n",
    "mses_c = 1.0/(len(y_test)) * np.sum((y_test - y_test_hat_c)**2, axis=1)\n",
    "\n",
    "sns.lineplot(x=coefs, y=mses_c);\n",
    "sns.scatterplot(x=coefs, y=mses_c, s=50);\n",
    "sns.scatterplot(x=reg_simple.coef_, y=mse_simple, color=sns.color_palette()[1], s=100);\n",
    "p = plt.xlabel('w1');\n",
    "p = plt.ylabel('Test MSE');"
   ],
   "id": "588031b7-4004-46de-ae51-18c251abde48"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance, explained variance, R2"
   ],
   "id": "6016edfc-c7e7-41a6-a09f-308be05da363"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick reminder:\n",
    "\n",
    "Mean of $x$ and $y$:\n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i, \\quad \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$$\n",
    "\n",
    "Variance of $x$ and $y$:\n",
    "\n",
    "$$\\sigma_x^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x}) ^2, \\quad \\sigma_y^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y}) ^2$$\n",
    "\n",
    "Covariance of $x$ and $y$:\n",
    "\n",
    "$$\\sigma_{xy} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})$$"
   ],
   "id": "8ca67b6d-f282-4ffc-9ea5-2379293fc9b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_y = 1.0/len(y_test) * np.sum((y_test - np.mean(y_test))**2) # or use np.var()\n",
    "var_y"
   ],
   "id": "d5f04d78-62d7-4eda-8ea6-18064c160d91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_y = np.mean(y_test)\n",
    "mean_y"
   ],
   "id": "c8b92614-565f-4109-8a38-b944b7448eb4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of $y$ is the mean sum of the squares of the distances from each $y_i$ to $\\bar{y}$. These distances are illustrated here:\n",
    "\n",
    "-   the horizontal line shows $\\bar{y}$\n",
    "-   each vertical line is a distance from a $y_i$ to $\\bar{y}$"
   ],
   "id": "a1944cc6-df85-4fb5-a6d1-eb44e8d9a391"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hlines(y=mean_y, xmin=np.min(x_test), xmax=np.max(x_test));\n",
    "plt.vlines(x_test, ymin=mean_y, ymax=y_test, alpha=0.5, color=sns.color_palette()[3]);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, color=sns.color_palette()[2], s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ],
   "id": "be89f7d1-1087-426e-8106-4954526e8251"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s look at a similar kind of plot, but with distances to the regression line instead of the to mean line:\n",
    "\n",
    "-   In the previous plot, each vertical line was a $y_i - \\bar{y}$\n",
    "-   In the following plot, each vertical line is a $y_i - \\hat{y}_i$\n",
    "\n",
    "(where $\\hat{y}_i$ is the prediction of the linear regression for a given sample $i$)"
   ],
   "id": "7007b31a-df9e-49cc-ae01-bfe47322a5ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, y_test_hat);\n",
    "plt.vlines(x_test, ymin=y_test, ymax=y_test_hat, color=sns.color_palette()[3], alpha=0.5);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, color=sns.color_palette()[2], s=50);\n",
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_simple.coef_ + reg_simple.intercept_\n",
    "sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[1]);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ],
   "id": "0d9de193-9342-430b-81ab-4ec6a42c9f2d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two plots together show how well the variance of $y$ is “explained” by the linear regression model:\n",
    "\n",
    "-   The total variance of $y$ is shown in the first plot, where each vertical line is\n",
    "\n",
    "$$y_i - \\bar{y}$$\n",
    "\n",
    "-   The *unexplained* variance of $y$ is shown in the second plot, where each vertical line is the error of the model,\n",
    "\n",
    "$$y_i - \\hat{y}_i$$\n",
    "\n",
    "In this example, *all* of the variance of $y$ is “explained” by the linear regression."
   ],
   "id": "ea898224-0fc7-4c5a-af22-695fe75ed07c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE for this example is 0, R2 is 1."
   ],
   "id": "6ada6069-2cfa-4010-8e7a-cface9127796"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(intercept_fit = widgets.FloatSlider(min=-8, max=8, step=0.5, value=1),\n",
    "   coef_fit = widgets.FloatSlider(min=-8, max=8, step=0.1, value=5),\n",
    "   show_residual=True)\n",
    "def plot_reg(intercept_fit, coef_fit, show_residual):\n",
    "    x_train, y_train = generate_linear_regression_data(n=20000, d=1, coef=5, intercept=1, sigma=0)\n",
    "    x_test,  y_test =  generate_linear_regression_data(n=10000, d=1, coef=5, intercept=1, sigma=0)\n",
    "    y_test_hat = intercept_fit + coef_fit*x_test\n",
    "    r2_test = metrics.r2_score(y_test, y_test_hat)\n",
    "    mse_test = metrics.mean_squared_error(y_test, y_test_hat)\n",
    "    x_line = np.array([-3, 3])\n",
    "    y_line = intercept_fit + coef_fit*x_line\n",
    "    plt.axhline(y=np.mean(y_train), color=sns.color_palette()[1]);\n",
    "    if show_residual:\n",
    "      plt.vlines(x_test[:100,], ymin=y_test[:100,], ymax=y_test_hat[:100,], alpha=0.5, color=sns.color_palette()[3]);\n",
    "    sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[2]);\n",
    "    sns.scatterplot(x=x_test[:100,].squeeze(), y=y_test[:100], s=50);\n",
    "    plt.xlabel('x');\n",
    "    plt.ylabel('y');\n",
    "    plt.ylim(-20,20)\n",
    "    plt.xlim(-3,3)\n",
    "    plt.title(\"MSE: %f\\nR2: %f\" % (mse_test, r2_test) )\n",
    "    plt.show()\n"
   ],
   "id": "0487aefe-cbd6-42a4-885b-ec52783b10e7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple linear regression with noise"
   ],
   "id": "a2f04751-1eeb-4e35-90ec-d846a5dd3c5f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ],
   "id": "c7a890ea-c5f8-488a-90b8-7eebcaca7c5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_regression_data(n=n_samples, d=1, coef=coef, intercept=intercept, sigma=2)\n",
    "x_test,  y_test =  generate_linear_regression_data(n=50, d=1, coef=coef, intercept=intercept, sigma=2)"
   ],
   "id": "3a21a1ea-c472-413a-9610-b808217b3c8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x_train.squeeze(), y=y_train, s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ],
   "id": "03dc6bc6-51a4-476e-998d-95851ea7b80d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression"
   ],
   "id": "2146e218-2137-45a0-872e-d3dce8f122bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_noisy = LinearRegression().fit(x_train, y_train)\n",
    "print(\"Coefficient list: \", reg_noisy.coef_)\n",
    "print(\"Intercept: \" , reg_noisy.intercept_)"
   ],
   "id": "bd5ee385-266e-4c43-a7c2-672e9da24e74"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_train), np.max(x_train)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\n",
    "\n",
    "sns.scatterplot(x=x_train.squeeze(), y=y_train, s=50);\n",
    "sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[1]);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');\n"
   ],
   "id": "d95c3f77-ce66-4f66-ad5f-e55f5812f2a9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict some new points"
   ],
   "id": "47f986bb-33ba-4f19-bcb3-8975ead62a24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = reg_noisy.intercept_ + np.dot(x_test,reg_noisy.coef_)"
   ],
   "id": "7d064fe9-f14a-40b6-97e7-ca829eb2a5d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_"
   ],
   "id": "07cc6b43-eec8-4855-b118-48b0b21d4025"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[1]);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test_hat, color=sns.color_palette()[1], s=50);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, color=sns.color_palette()[2], s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');\n"
   ],
   "id": "496a19fa-8054-4bc0-b471-da88fa6bf90f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute MSE"
   ],
   "id": "aa546243-66dd-45de-83c3-f624b05119a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = reg_noisy.intercept_ + np.dot(x_test,reg_noisy.coef_)\n",
    "mse_noisy = 1.0/(len(y_test)) * np.sum((y_test - y_test_hat)**2)\n",
    "mse_noisy"
   ],
   "id": "c179ac94-0f5c-4f36-9210-b1c363890adf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE is higher than before! (When it was essentially zero.)\n",
    "\n",
    "Does this mean our estimate of $w_0$ and $w_1$ is not optimal?\n",
    "\n",
    "Since we generated the data, we know the “true” coefficient value and we can see how much the MSE would be with the true coefficient values."
   ],
   "id": "f6082732-3c82-4d03-8e37-449117f2af04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_perfect_coef = intercept + np.dot(x_test,coef)\n",
    "\n",
    "mse_perfect_coef = 1.0/(len(y_test_perfect_coef)) * np.sum((y_test_perfect_coef - y_test)**2)\n",
    "mse_perfect_coef"
   ],
   "id": "8a72130e-483e-4385-a40e-1ba0e6534ff1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes our linear regression doesn’t select the “true” coefficients?"
   ],
   "id": "875539d9-1eda-4439-ae41-0b56fe79cd7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = reg_noisy.intercept_ + np.dot(x_train,reg_noisy.coef_)\n",
    "mse_train_est = 1.0/(len(y_train)) * np.sum((y_train - y_train_hat)**2)\n",
    "mse_train_est"
   ],
   "id": "1ebd73e4-0b66-4090-94c3-26fd1ae3d088"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_perfect_coef = intercept + np.dot(x_train,coef)\n",
    "mse_train_perfect = 1.0/(len(y_train_perfect_coef)) * np.sum((y_train_perfect_coef - y_train)**2)\n",
    "mse_train_perfect"
   ],
   "id": "5d668347-f11d-4cbc-bd73-645f697740f3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “correct” coefficients had slightly higher MSE on the training set than the fitted coefficients. We fit parameters so that they are optimal on the *training* set, then we use the test set to understand how the model will generalize to new, unseen data."
   ],
   "id": "0702d0eb-3d2e-4510-8aa8-f9dd09eaf395"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that part of the MSE is due to noise in the data, and part is due to error in the parameter estimates.\n",
    "\n",
    "Soon - we will formalize this discussion of different sources of error:\n",
    "\n",
    "-   Error in parameter estimates\n",
    "-   “Noise” - any variation in data that is not a function of the $X$ that we use as input to the model\n",
    "-   Other error - for example, model (hypothesis class) not a good choice for the data"
   ],
   "id": "24d75ce2-9e48-4602-908e-6e0963195ea6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize MSE for different coefficients"
   ],
   "id": "5267b3cd-5ad5-489f-a0ed-ab1d3992b12f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(4.5, 5.5, 0.1)\n",
    "\n",
    "y_test_hat_c = reg_noisy.intercept_ + np.dot(x_test,coefs.reshape(1,-1))\n",
    "mses_test =  np.mean((y_test.reshape(-1,1) - y_test_hat_c)**2, axis=0)\n",
    "y_train_hat_c = reg_noisy.intercept_ + np.dot(x_train,coefs.reshape(1,-1))\n",
    "mses_train =  np.mean((y_train.reshape(-1,1) - y_train_hat_c)**2, axis=0)"
   ],
   "id": "e8b78a53-d532-4a79-aac4-40f7f9e6a532"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.lineplot(x=coefs, y=mses_train)\n",
    "sns.scatterplot(x=coefs, y=mses_train, s=50);\n",
    "sns.scatterplot(x=reg_noisy.coef_, y=mse_train_est, color=sns.color_palette()[1], s=100);\n",
    "plt.title(\"Training MSE vs. coefficient\");\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('MSE');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.lineplot(x=coefs, y=mses_test)\n",
    "sns.scatterplot(x=coefs, y=mses_test, s=50);\n",
    "sns.scatterplot(x=reg_noisy.coef_, y=mse_noisy, color=sns.color_palette()[1], s=100);\n",
    "plt.title(\"Test MSE vs. coefficient\");\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('MSE');"
   ],
   "id": "dfbd0acf-9d77-4d86-9c6e-298d48b5c490"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot on the left (for training MSE), the orange dot (our coefficient estimate) should always have minimum MSE, because we select parameters to minimize MSE on the training set.\n",
    "\n",
    "In the plot on the right (for test MSE), the orange dot might not have the minimum MSE, because the best coefficient on the training set might not be the best coefficient on the test set. This gives us some idea of how our model will generalize to new, unseen data. We may suspect that if the coefficient estimate is not perfect for *this* test data, it might have some error on other new, unseen data, too.\n",
    "\n",
    "If you re-run this notebook many times, you’ll get a new random sample of training and test data each time. Sometimes, the “true” coefficients may have smaller MSE on the test set than the estimated coefficients. On other runs, the estimated coefficients might have smaller MSE on the test set."
   ],
   "id": "75b83313-b297-4793-b513-87287bc41c3b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance, explained variance, R2"
   ],
   "id": "7754ac2c-b13f-4cd9-b6b6-845d0f0c440c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_y = 1.0/len(y_test) * np.sum((y_test - np.mean(y_test))**2)\n",
    "var_y"
   ],
   "id": "0acad3be-6c5d-4b6b-85f7-45cd4d2cde7d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_y = np.mean(y_test)\n",
    "mean_y"
   ],
   "id": "5956b877-4b05-403f-b662-007ccf9209b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\n",
    "plt.hlines(mean_y, xmin=np.min(x_test), xmax=np.max(x_test));\n",
    "plt.vlines(x_test, ymin=mean_y, ymax=y_test, color=sns.color_palette()[4], alpha=0.5);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, color=sns.color_palette()[2], s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ],
   "id": "c0bf00d6-95af-4b1e-b045-b9fb724dc79f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.vlines(x_test, ymin=y_test, ymax=y_test_hat, color=sns.color_palette()[1], alpha=0.5);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, color=sns.color_palette()[2], s=50);\n",
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\n",
    "sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[1]);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ],
   "id": "4150c4b3-db04-4803-93c2-bfaf01797520"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember:\n",
    "\n",
    "The total variance of $y$ is shown in the first plot, where each vertical line is $y_i - \\bar{y}$\n",
    "\n",
    "The *unexplained* variance of $y$ is shown in the second plot, where each vertical line is the error of the model, $y_i - \\hat{y}_i$\n",
    "\n",
    "In the next plot, we’ll combine them to get some intuition regarding the *fraction of unexplained variance*. The purple part of each vertical bar is the *unexplained* part, while the orange part is *explained* by the linear regression."
   ],
   "id": "4b85a8a4-5fe1-4569-8c7a-715593ba0fc8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\n",
    "\n",
    "plt.hlines(mean_y, xmin=np.min(x_test), xmax=np.max(x_test));\n",
    "plt.vlines(x_test, ymin=mean_y, ymax=y_test, color=sns.color_palette()[1]);\n",
    "plt.vlines(x_test, ymin=y_test, ymax=y_test_hat, color=sns.color_palette()[4]);\n",
    "sns.scatterplot(x=x_test.squeeze(), y=y_test, color=sns.color_palette()[2], s=50);\n",
    "sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[1]);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ],
   "id": "b403a5ca-bab0-450f-b2ff-45a4138f83e4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fraction of variance unexplained** is the ratio of the sum of squared distances from data to the regression line (sum of squared vertical distances in second plot), to the sum of squared distanced from data to the mean (sum of squared vertical distances in first plot):\n",
    "\n",
    "$$\\frac{MSE}{Var(y)} = \\frac{Var(y-\\hat{y})}{Var(y)} = \\frac{\\sum_{i=1}^n(y_i-\\hat y_i)^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2}$$"
   ],
   "id": "66ae47b9-53a6-45df-acbb-71c42eb3a361"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative interpretation: imagine we would develop a very simple ML model, in which we always predict $\\hat{y}_i = \\bar{y}_i$. Then, we use this model as a basis for comparison for other, more sophisticated models. The ratio above is the ratio of error of the regression model, to the error of a “prediction by mean” model.\n",
    "\n",
    "-   If this quantity is less than 1, our model is better than “prediction by mean”\n",
    "-   If this quantity is greater than 1, our model is worse than “prediction by mean”"
   ],
   "id": "e878a545-65d5-4a5d-9faa-0c5484866e38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvu = mse_noisy/var_y\n",
    "fvu"
   ],
   "id": "79df5d6c-74e7-455a-b621-bf6770d9785a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = 1 - fvu\n",
    "r2"
   ],
   "id": "53cfe189-d15a-4645-9c99-b3e153778e4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to do the same thing...\n",
    "metrics.r2_score(y_test, y_test_hat)"
   ],
   "id": "4f56c1ab-0de0-4baa-b883-37eb9d6b47e5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a negative R2 mean, in terms of a comparison to “prediction by mean”?"
   ],
   "id": "46cc4cf9-ed59-4828-bccb-118bf82c6b7b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on coefficient value, metrics"
   ],
   "id": "c9ce01de-1ddd-4a2a-a7f0-d3f8e41c4af9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(sigma = widgets.IntSlider(min=0, max=5, step=1, value=2),\n",
    "   coef = widgets.IntSlider(min=-5, max=5, step=1, value=2))\n",
    "def plot_reg(sigma, coef):\n",
    "    x_train, y_train = generate_linear_regression_data(n=20000, d=1, coef=coef, intercept=intercept, sigma=sigma)\n",
    "    x_test,  y_test =  generate_linear_regression_data(n=10000, d=1, coef=coef, intercept=intercept, sigma=sigma)\n",
    "    r_mod = LinearRegression().fit(x_train, y_train)\n",
    "    r2_test = r_mod.score(x_test, y_test)\n",
    "    mse_test = metrics.mean_squared_error(y_test, r_mod.predict(x_test))\n",
    "    x_line = np.array([-3, 3])\n",
    "    y_line = r_mod.predict(x_line.reshape(-1,1))\n",
    "    plt.axhline(y=np.mean(y_train), color=sns.color_palette()[1]);\n",
    "    sns.lineplot(x=x_line, y=y_line, color=sns.color_palette()[2]);\n",
    "    sns.scatterplot(x=x_test[:100,].squeeze(), y=y_test[:100], s=50);\n",
    "    plt.xlabel('x');\n",
    "    plt.ylabel('y');\n",
    "    plt.ylim(-20,20)\n",
    "    plt.xlim(-3,3)\n",
    "    plt.title(\"MSE: %f\\nR2: %f\" % (mse_test, r2_test) )\n",
    "    plt.show()"
   ],
   "id": "aec52ab4-83fb-4e18-b170-d2b45f828423"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in this data, the *only* source of error is the $\\epsilon$ in\n",
    "\n",
    "$$y_i = w_0 + w_1 x_{i,1} + \\ldots + w_d x_{i,d} + \\epsilon_i $$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$. If not for this, our regression would fit the data perfectly."
   ],
   "id": "f740b0f8-20c2-4e4b-9e24-025ba950c480"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the coefficient:\n",
    "\n",
    "> An increase in $x$ of 1 is, on average, associated with an increase in $y$ of about $w_1$.\n",
    "\n",
    "Note that it does not imply any causal relationship!"
   ],
   "id": "7463b75b-f88f-440c-a0d0-b058a767d411"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting MSE and R2:\n",
    "\n",
    "-   MSE shows us the variance of the data around the regression line (for data with this specific type of “noise”).\n",
    "-   MSE is a measure of the model error, not relative to any baseline. We can use it to compare different models on the same dataset (but not on different datasets).\n",
    "-   R2 tells us what fraction of the variance in the data is “explained” by the regression line.\n",
    "-   R2 is a measure relative to the “prediction by mean” baseline. (Note that if “prediction by mean” is already good, even a well fitting regression line will not have a high R2.)\n",
    "-   Prediction by mean is the same thing as prediction by a line with\n",
    "\n",
    "$$w_0 = \\overline{y}, w_1 = 0$$\n",
    "\n",
    "-   The greater the true $w_1$, the more “wrong” the $w_1 = 0$ “prediction” is."
   ],
   "id": "2afc2b4b-28b4-443e-b364-436fb41e1529"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual analysis"
   ],
   "id": "08aa27de-6fa2-43d4-b170-44d668999f1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"anscombe\")\n",
    "df.groupby('dataset').agg({'x': ['count','mean', 'std'], 'y': ['count','mean', 'std']})"
   ],
   "id": "d818c7ba-99a8-40e0-8411-e88cd4092d07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_i   = df[df['dataset'].eq('I')]\n",
    "data_ii  = df[df['dataset'].eq('II')]\n",
    "data_iii = df[df['dataset'].eq('III')]\n",
    "data_iv  = df[df['dataset'].eq('IV')]"
   ],
   "id": "5cdfe571-e023-4afa-a689-7bef210ff77e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_i   = LinearRegression().fit(data_i[['x']],   data_i['y'])\n",
    "reg_ii  = LinearRegression().fit(data_ii[['x']],  data_ii['y'])\n",
    "reg_iii = LinearRegression().fit(data_iii[['x']], data_iii['y'])\n",
    "reg_iv  = LinearRegression().fit(data_iv[['x']],  data_iv['y'])"
   ],
   "id": "e8e478f5-8577-49a5-8620-d2212a32ce0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset I:   \",   reg_i.coef_,   reg_i.intercept_)\n",
    "print(\"Dataset II:  \",  reg_ii.coef_,  reg_ii.intercept_)\n",
    "print(\"Dataset III: \", reg_iii.coef_, reg_iii.intercept_)\n",
    "print(\"Dataset IV:  \",  reg_iv.coef_,  reg_iv.intercept_)"
   ],
   "id": "1595c2df-1d4f-4d2b-b11f-966f89811a07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset I:   \", metrics.r2_score(data_i['y'],  reg_i.predict(data_i[['x']])))\n",
    "print(\"Dataset II:  \", metrics.r2_score(data_ii['y'], reg_ii.predict(data_ii[['x']])))\n",
    "print(\"Dataset III: \", metrics.r2_score(data_iii['y'],reg_iii.predict(data_iii[['x']])))\n",
    "print(\"Dataset IV:  \", metrics.r2_score(data_iv['y'], reg_iv.predict(data_iv[['x']])))"
   ],
   "id": "e3f38815-b9b8-4708-b917-9d2cd8ece908"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset I:   \", metrics.mean_squared_error(data_i['y'],  reg_i.predict(data_i[['x']])))\n",
    "print(\"Dataset II:  \", metrics.mean_squared_error(data_ii['y'], reg_ii.predict(data_ii[['x']])))\n",
    "print(\"Dataset III: \", metrics.mean_squared_error(data_iii['y'],reg_iii.predict(data_iii[['x']])))\n",
    "print(\"Dataset IV:  \", metrics.mean_squared_error(data_iv['y'], reg_iv.predict(data_iv[['x']])))"
   ],
   "id": "b864efa3-64b3-4267-aea1-399b074335b3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these models are equally “good” according to our scoring metrics… BUT"
   ],
   "id": "6f892a05-4f23-4f45-af4d-76364391192b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\", \n",
    "           data=df, col_wrap=2, ci=None, palette=\"muted\", height=4, \n",
    "           scatter_kws={\"s\": 50, \"alpha\": 1});"
   ],
   "id": "0313aed6-cfdc-4bd0-8e52-465f3fe3151c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the linear model fit well?\n",
    "\n",
    "-   the linear model is a good fit for Dataset I\n",
    "-   Dataset II is clearly non-linear\n",
    "-   Dataset III has an outlier\n",
    "-   Dataset IV has a high leverage point"
   ],
   "id": "e44c1d87-5b4a-407f-a71d-b8e773637334"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy to identify problems in 1D - what about in higher D?\n",
    "\n",
    "-   Plot $\\hat{y}$ against $y$\n",
    "-   Plot residuals against $\\hat{y}$\n",
    "-   Plot residuals against each $x$ (including any $x$ not in the model)\n",
    "-   Plot residuals against time (for time series data)\n",
    "\n",
    "What should each of these plots look like if the regression is “good”?"
   ],
   "id": "c14fad5c-e6aa-4c70-9d38-7c9810bd0e52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_i   = data_i.assign(   yhat = reg_i.predict(  data_i[['x']]) )\n",
    "data_ii  = data_ii.assign(  yhat = reg_ii.predict( data_ii[['x']]) )\n",
    "data_iii = data_iii.assign( yhat = reg_iii.predict( data_iii[['x']]) )\n",
    "data_iv  = data_iv.assign(  yhat = reg_iv.predict(  data_iv[['x']]) )\n",
    "\n",
    "data_i   = data_i.assign(   residual = data_i['y'] - data_i['yhat'] )\n",
    "data_ii  = data_ii.assign(  residual = data_ii['y'] - data_ii['yhat'] )\n",
    "data_iii = data_iii.assign( residual = data_iii['y'] - data_iii['yhat'] )\n",
    "data_iv  = data_iv.assign(  residual = data_iv['y'] - data_iv['yhat'] )\n",
    "\n",
    "data_all = pd.concat([data_i, data_ii, data_iii, data_iv])\n",
    "data_all.head()"
   ],
   "id": "f6785c13-a295-4a56-a79d-f0242d676385"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"x\", y=\"residual\", col=\"dataset\", hue=\"dataset\", \n",
    "           data=data_all, col_wrap=2, ci=None, palette=\"muted\", height=4, \n",
    "           scatter_kws={\"s\": 50, \"alpha\": 1}, fit_reg=False);"
   ],
   "id": "72c26299-32d4-4fbb-9842-a5bd9daf8368"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression"
   ],
   "id": "8d0687ab-0162-4e25-b8fa-9aa60a887ed9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ],
   "id": "c1b5eae3-7817-4d83-ae84-79cefedb625a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_regression_data(n=n_samples, d=2, coef=[5,5], intercept=intercept)\n",
    "x_test,  y_test  = generate_linear_regression_data(n=50, d=2, coef=[5,5], intercept=intercept)\n"
   ],
   "id": "ec518718-aa9a-4abf-93b1-794d54ed7749"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ],
   "id": "f6643670-c6e0-44ac-8446-2cb906f0f281"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ],
   "id": "22280951-65e1-47f5-bbe0-0a5a6b69e18e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.scatter(x_train[:,0],  y_train);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.subplot(1,2,2);\n",
    "plt.scatter(x_train[:,1],  y_train);\n",
    "plt.xlabel(\"x2\");\n",
    "plt.ylabel(\"y\");"
   ],
   "id": "646d010d-72ad-48cd-8a24-7270423643a5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that there is no stochastic noise in this data - so it fits a linear model perfectly. But it’s more difficult to see that linear relationship in higher dimensions."
   ],
   "id": "f83d6ad3-3575-4076-b22c-bb9deb7ca0ee"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression"
   ],
   "id": "755cd8e9-a50c-4773-bf5d-735539e66038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_multi = LinearRegression().fit(x_train, y_train)\n",
    "print(\"Coefficient list: \", reg_multi.coef_)\n",
    "print(\"Intercept: \" , reg_multi.intercept_)"
   ],
   "id": "6c843a47-13b2-4821-a42d-1973d097567c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot hyperplane"
   ],
   "id": "b107c47a-37a5-49f9-805b-3548a9ec19ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev=20, azim=-20, X=x_train, y=y_train):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "\n",
    "\n",
    "    X1 = np.arange(-4, 4, 0.2)\n",
    "    X2 = np.arange(-4, 4, 0.2)\n",
    "    X1, X2 = np.meshgrid(X1, X2)\n",
    "    Z = X1*reg_multi.coef_[0] + X2*reg_multi.coef_[1]\n",
    "\n",
    "    # Plot the surface.\n",
    "    ax.plot_surface(X1, X2, Z, alpha=0.1, color='gray',\n",
    "                          linewidth=0, antialiased=False)\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], y, s=50)\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('y')\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20), \n",
    "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
    "         X=fixed(x_train), y=fixed(y_train));"
   ],
   "id": "0f7bd0d7-851b-4683-8cb5-e73a76920025"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE contour"
   ],
   "id": "276233e1-f28d-4062-ad1b-d466bb012466"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(3.0, 7.0, 0.05)\n",
    "\n",
    "coef_grid = np.array(np.meshgrid(coefs, coefs)).reshape(1, 2, coefs.shape[0], coefs.shape[0])\n",
    "y_train_hat_c = (reg_multi.intercept_ + np.sum(coef_grid * x_train.reshape(x_train.shape[0], 2, 1, 1), axis=1) )\n",
    "mses_train = np.mean((y_train_hat_c- y_train.reshape(-1, 1, 1))**2, axis=0)"
   ],
   "id": "d4cd4c47-ff53-4c4d-b392-10a3b863a211"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "p = plt.scatter(x=reg_multi.coef_[0], y=reg_multi.coef_[1], c=sns.color_palette()[1])\n",
    "p = plt.contour(coef_grid[0, 0, :, :], coef_grid[0, 1, :, :], mses_train, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');"
   ],
   "id": "2bf9f7b8-d36d-4775-8292-8ec002fcf2ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(3.0, 7.0, 0.05)\n",
    "\n",
    "coef_grid = np.array(np.meshgrid(coefs, coefs)).reshape(1, 2, coefs.shape[0], coefs.shape[0])\n",
    "y_test_hat_c = (reg_multi.intercept_ + np.sum(coef_grid * x_test.reshape(x_test.shape[0], 2, 1, 1), axis=1) )\n",
    "mses_test = np.mean((y_test_hat_c- y_test.reshape(-1, 1, 1))**2, axis=0)"
   ],
   "id": "bf94a2b5-255a-4def-839e-b7b7bc19795d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "p = plt.scatter(x=reg_multi.coef_[0], y=reg_multi.coef_[1], c=sns.color_palette()[1])\n",
    "p = plt.contour(coef_grid[0, 0, :, :], coef_grid[0, 1, :, :], mses_test, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');"
   ],
   "id": "6084d406-5ee7-44e2-8b07-3e5ba25fd163"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple linear regression with noise"
   ],
   "id": "f62f10a2-f121-4b01-be83-5724556892fa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ],
   "id": "44dd071a-10ae-4715-824e-4e8f81bc7a0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_regression_data(n=n_samples, d=2, coef=[5,5], intercept=intercept, sigma=5)\n",
    "x_test,  y_test  = generate_linear_regression_data(n=50, d=2, coef=[5,5], intercept=intercept, sigma=5)"
   ],
   "id": "25b40423-dc3e-4fd0-923f-93a641d19190"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.scatter(x_train[:,0],  y_train);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.subplot(1,2,2);\n",
    "plt.scatter(x_train[:,1],  y_train);\n",
    "plt.xlabel(\"x2\");\n",
    "plt.ylabel(\"y\");"
   ],
   "id": "a306b73e-276a-41c9-8e3d-512bb4ca4305"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression"
   ],
   "id": "7eec5e3e-4e58-43f9-af77-c5aa1281096c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_multi_noisy = LinearRegression().fit(x_train, y_train)\n",
    "print(\"Coefficient list: \", reg_multi_noisy.coef_)\n",
    "print(\"Intercept: \" , reg_multi_noisy.intercept_)"
   ],
   "id": "f59018cb-6644-4f30-bba8-3ef26e6f682c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot hyperplane"
   ],
   "id": "cb9821ac-8e27-409c-9b75-936c3baf8dac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev=20, azim=-20, X=x_train, y=y_train):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "\n",
    "\n",
    "    X1 = np.arange(-4, 4, 0.2)\n",
    "    X2 = np.arange(-4, 4, 0.2)\n",
    "    X1, X2 = np.meshgrid(X1, X2)\n",
    "    Z = X1*reg_multi_noisy.coef_[0] + X2*reg_multi_noisy.coef_[1]\n",
    "\n",
    "    # Plot the surface.\n",
    "    ax.plot_surface(X1, X2, Z, alpha=0.1, color='gray',\n",
    "                          linewidth=0, antialiased=False)\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], y, s=50)\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('y')\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20), \n",
    "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
    "         X=fixed(x_train), y=fixed(y_train));"
   ],
   "id": "14e7c422-d77f-496d-9c33-5cc7945bbbbf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE contour"
   ],
   "id": "b54b0169-d1ac-4439-8b68-d5cdc5afcd38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(3.0, 7.0, 0.05)\n",
    "\n",
    "coef_grid = np.array(np.meshgrid(coefs, coefs)).reshape(1, 2, coefs.shape[0], coefs.shape[0])\n",
    "y_train_hat_c = (reg_multi_noisy.intercept_ + np.sum(coef_grid * x_train.reshape(x_train.shape[0], 2, 1, 1), axis=1) )\n",
    "mses_train = np.mean((y_train_hat_c- y_train.reshape(-1, 1, 1))**2, axis=0)"
   ],
   "id": "67578cc9-3c70-447c-afb2-41b71c293b41"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "p = plt.scatter(x=reg_multi_noisy.coef_[0], y=reg_multi_noisy.coef_[1], c=sns.color_palette()[1])\n",
    "p = plt.contour(coef_grid[0, 0, :, :], coef_grid[0, 1, :, :], mses_train, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');"
   ],
   "id": "34023a70-fee8-4427-a8f2-745da6d2c530"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(3.0, 7.0, 0.05)\n",
    "\n",
    "coef_grid = np.array(np.meshgrid(coefs, coefs)).reshape(1, 2, coefs.shape[0], coefs.shape[0])\n",
    "y_test_hat_c = (reg_multi_noisy.intercept_ + np.sum(coef_grid * x_test.reshape(x_test.shape[0], 2, 1, 1), axis=1) )\n",
    "mses_test = np.mean((y_test_hat_c- y_test.reshape(-1, 1, 1))**2, axis=0)"
   ],
   "id": "ae5a1eaa-8f69-4b31-91a3-071aec452b6d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "p = plt.scatter(x=reg_multi_noisy.coef_[0], y=reg_multi_noisy.coef_[1], c=sns.color_palette()[1])\n",
    "p = plt.contour(coef_grid[0, 0, :, :], coef_grid[0, 1, :, :], mses_test, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');"
   ],
   "id": "0804d114-faa5-46a4-906b-5fc2c97ad898"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear basis function regression"
   ],
   "id": "5327aded-9c87-47d9-9ac4-e4b301e6d57b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumptions of the linear model (that the target variable can be predicted as a linear combination of the features) can be restrictive. We can capture more complicated relationships using linear basis function regression.\n",
    "\n",
    "Fundamental idea: with a set of “basis” functions, we represent the “shape” of our data as a weighted sum of basis functions:\n",
    "\n",
    "$$ \\hat{y_i} =  \\sum_{j=0}^p w_p \\phi_p(\\mathbf{x_i}) $$\n",
    "\n",
    "(We’ll revisit this idea again later in the semester, when we talk about kernels; and again, when we talk about activation functions in neural networks.)"
   ],
   "id": "ebab0217-569e-411d-a55b-b8b6d1df44ad"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re going to look at some examples of basis functions (but not an exhaustive list…)\n",
    "\n",
    "(Note: it’s also possible to mix-and-match basis functions from different “families” in the same model! And, you can apply basis functions to multiple features, too.)"
   ],
   "id": "087642f9-5c6e-46cb-8511-5f2691bb0045"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear basis\n",
    "\n",
    "Transform a feature $x$ using $\\phi_0(x) = 1, \\phi_1(x) = x$, i.e.\n",
    "\n",
    "$$y \\approx w_0 + w_1 x $$"
   ],
   "id": "93c92310-6000-4028-b1ef-203859acc04a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_basis(x):\n",
    "  return np.hstack([np.ones(x.shape),x])\n",
    "\n",
    "x = np.arange(-1.5,1.5,step=0.01).reshape(-1,1)\n",
    "x_trans = linear_basis(x)\n",
    "\n",
    "@interact(w0 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w1 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          show_sum = False)\n",
    "def plot_linear(w0, w1, show_sum):\n",
    "  plt.figure(figsize=(10,5));\n",
    "  w = np.array([w0, w1])\n",
    "  l = ['1', 'x']\n",
    "  y = np.sum(w*x_trans, axis=1)\n",
    "  if show_sum:\n",
    "    sns.lineplot(x=x.squeeze(), y=y, label='sum', alpha=1, lw=2);\n",
    "  for i in range(2):\n",
    "    sns.lineplot(x=x.squeeze(), y=w[i]*x_trans[:,i], label='$' + l[i] + '$', alpha=0.5);\n",
    "  plt.ylim(-2, 2);\n",
    "  plt.title(\"Linear basis\");\n",
    "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0);\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('$\\phi(x)$')\n",
    "  plt.show()"
   ],
   "id": "8091ae37-eb86-40be-85b9-236fd83562e6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial basis\n",
    "\n",
    "Transform a feature $x$ using $\\phi_j(x) = x^j$, i.e.\n",
    "\n",
    "$$y \\approx w_0 x^0 + w_1 x^1 + \\ldots + w_p x^p $$\n",
    "\n",
    "Note that the model is linear in the parameters $\\mathbf{w}$, which is what makes it a linear model even though it is not linear in $x$."
   ],
   "id": "6646430d-f1fb-4bf4-8a23-1805be19268f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_basis(x, d):\n",
    "  return x**np.arange(d)\n",
    "\n",
    "x = np.arange(-1.5,1.5,step=0.01).reshape(-1,1)\n",
    "x_trans = polynomial_basis(x,5)\n",
    "\n",
    "@interact(w0 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w1 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w2 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w3 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w4 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          show_sum = False)\n",
    "def plot_poly(w0, w1, w2, w3, w4, show_sum):\n",
    "  plt.figure(figsize=(10,5));\n",
    "  w = np.array([w0, w1, w2, w3, w4])\n",
    "  y = np.sum(w*x_trans, axis=1)\n",
    "  if show_sum:\n",
    "    sns.lineplot(x=x.squeeze(), y=y, label='sum', alpha=1, lw=2);\n",
    "  for i in range(5):\n",
    "    sns.lineplot(x=x.squeeze(), y=w[i]*x_trans[:,i], label='$x^' + str(i) + '$', alpha=0.5);\n",
    "  plt.ylim(-2, 2);\n",
    "  plt.title(\"Polynomial basis\");\n",
    "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0);\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('$\\phi(x)$')\n",
    "  plt.show()"
   ],
   "id": "1a9ec87f-698d-4179-baef-8011de06236c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, polynomial functions are actually not very useful for modeling real data -\n",
    "\n",
    "-   polynomials are “global” functions - affect the entire range from $-\\infty$ to $\\infty$, even though different parts of the data might have different behavior\n",
    "-   they get weird at the boundaries of the data (Runge’s phenomenon) and *really* bad if you need to extrapolate past the range of the training data\n",
    "\n",
    "Instead of high-degree polynomials, we tend to prefer lower-degree *piecewise* functions, so we can fit *local* behavior."
   ],
   "id": "3b53b242-9aeb-488b-af39-77c02d177ea0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splines\n",
    "\n",
    "With splines, the feature axis is divided into breakpoints - we call each breakpoints a “knot” - and then we define basis functions that are a polynomial function of the feature between two knots.\n",
    "\n",
    "If we constrain the piecewise function to meet at the knots, we call these splines - basis splines or “B splines”. Here is how these functions are defined:\n",
    "\n",
    "First, for constant functions (degree 0) - given “knots” at positions $k_t, k_{t+1}$:\n",
    "\n",
    "$$\n",
    "\\phi_{t,0}({x}) = \n",
    "\\begin{cases}\n",
    "1, \\quad  k_t \\leq x < k_{t+1} \\\\\n",
    "0, \\quad  \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then for degree $p>0$, it is defined recursively:\n",
    "\n",
    "$$\n",
    "\\phi_{t, p}( x ) := \\dfrac{ x - k_t }{k_{t+p} - k_t} \\phi_{t,p-1}( x ) + \\dfrac{k_{t+p+1} - x }{k_{t+p+1} - k_{t+1}} \\phi_{t+1,p-1}( x )\n",
    "$$\n",
    "\n",
    "But, you won’t need to compute this yourself - you can use [`SplineTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.SplineTransformer.html) in `sklearn`."
   ],
   "id": "75628485-9119-45f6-9560-9d894f5d0dd7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import SplineTransformer\n",
    "\n",
    "def spline_basis(x, d):\n",
    "  return np.hstack([SplineTransformer(knots='uniform', n_knots = d, degree=i, extrapolation=\"constant\").fit_transform(x) for i in range(d)])\n",
    "\n",
    "x = np.arange(-1.5,1.5,step=0.01).reshape(-1,1)\n",
    "x_trans = spline_basis(x,3)\n",
    "\n",
    "@interact(w00 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w10 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w01 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w11 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w21 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w02 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w12 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w22 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w32 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          show_sum = False)\n",
    "def plot_spline(w00, w10, w01, w11, w21, w02, w12, w22, w32, show_sum):\n",
    "  plt.figure(figsize=(10,5));\n",
    "  labels = np.array([\"\\phi_{0,0}\", \"\\phi_{1,0}\", \n",
    "                     \"\\phi_{0,1}\", \"\\phi_{1,1}\", \"\\phi_{2,1}\",\n",
    "                     \"\\phi_{0,2}\", \"\\phi_{1,2}\", \"\\phi_{2,2}\", \"\\phi_{3,2}\"])\n",
    "  w = np.array([w00, w10, w01, w11, w21, w02, w12, w22, w32])\n",
    "  y = np.sum(w*x_trans, axis=1)\n",
    "  if show_sum:\n",
    "    sns.lineplot(x=x.squeeze(), y=y, label='sum', alpha=1, lw=2);\n",
    "  for i in range(9):\n",
    "    sns.lineplot(x=x.squeeze(), y=w[i]*x_trans[:,i], label='$'+labels[i]+'$', alpha=0.5);  \n",
    "  plt.ylim(-2, 2);\n",
    "  plt.title(\"Spline\");\n",
    "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0);\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('$\\phi(x)$')\n",
    "  plt.show()"
   ],
   "id": "f7e0f7e3-fe29-4ea3-a094-96dbecc901dc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial basis\n",
    "\n",
    "Transform a feature $x$ using $\\phi_j(x) = \\exp\\left(-\\frac{(x-\\mu_j)^2}{s^2}\\right)$, i.e.\n",
    "\n",
    "$$y \\approx w_0 \\exp\\left(-\\frac{(x-\\mu_0)^2}{s^2}\\right) + w_1 \\exp\\left(-\\frac{(x-\\mu_1)^2}{s^2}\\right) + \\ldots + w_p \\exp\\left(-\\frac{(x-\\mu_p)^2}{s^2}\\right)$$\n",
    "\n",
    "The model is linear in the parameters $\\mathbf{w}$, which is what makes it a linear model even though it is not linear in $x$. However, it is not linear in the basis function parameters $\\mu_j$ or $s$! (Those basis function parameters will not be “learned” - they are fixed by you.)\n",
    "\n",
    "Note that in contrast to the polynomials which had “global” effect, each radial basis function has “local” effect. (Think of it as a weighted sum of little “bumps”.)"
   ],
   "id": "39e41ee9-2580-49cd-a150-c8d60c1e9c63"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.1\n",
    "def radial_basis(x, mu_list):\n",
    "  return np.exp(-1*(x-mu_list)**2/s**2) \n",
    "  \n",
    "x = np.arange(-1.5,1.5,step=0.01).reshape(-1,1)\n",
    "x_trans = radial_basis(x, [-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "@interact(w0 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w1 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w2 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w3 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w4 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          show_sum = False)\n",
    "def plot_radial(w0, w1, w2, w3, w4, show_sum):\n",
    "  plt.figure(figsize=(10,5));\n",
    "  w = np.array([w0, w1, w2, w3, w4])\n",
    "  labels = ['$exp(-(x+1)^2)/(' + str(s) + '^2))$', '$exp(-(x+0.5)^2)/(' + str(s) + '^2))$',\n",
    "            '$exp(-(x)^2)/(' + str(s) + '^2))$', '$exp(-(x-0.5)^2)/(' + str(s) + '^2))$',\n",
    "            '$exp(-(x-1)^2)/(' + str(s) + '^2))$']\n",
    "  y = np.sum(w*x_trans, axis=1)\n",
    "  if show_sum:\n",
    "    sns.lineplot(x=x.squeeze(), y=y, label='sum', alpha=1, lw=2);\n",
    "  for i in range(5):\n",
    "    sns.lineplot(x=x.squeeze(), y=w[i]*x_trans[:,i], label=labels[i], alpha=0.5);\n",
    "  plt.ylim(-2, 2)\n",
    "  plt.title(\"Radial basis\");\n",
    "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0);\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('$\\phi(x)$')\n",
    "  plt.show()"
   ],
   "id": "d121eaee-8734-4f91-989a-c52688128b1e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(try changing `s` to see its effect!)"
   ],
   "id": "5d72e0f6-3ed0-4136-8c2e-baca2bcd579f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoidal basis\n",
    "\n",
    "Transform a feature $x$ using \\$\\_j(x) = ( ) \\$ where $\\sigma(a) = \\frac{1}{1+\\exp({-a})}$, i.e.\n",
    "\n",
    "$$y \\approx w_0 \\sigma \\left( \\frac{(x-\\mu_0)}{s}  \\right) + w_1 \\sigma \\left( \\frac{(x-\\mu_1)}{s}  \\right) + \\ldots + w_p \\sigma \\left( \\frac{(x-\\mu_p)}{s}  \\right)$$\n",
    "\n",
    "(Similar to the RBF but with “steps” instead of “bumps”…)"
   ],
   "id": "3d02e5d6-d72a-497f-81f5-49359964e9a4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.05\n",
    "def sigmoid_basis(x, mu_list):\n",
    "  return ((1+np.exp((-x+mu_list)/s))**-1)\n",
    "  \n",
    "x = np.arange(-1.5,1.5,step=0.01).reshape(-1,1)\n",
    "x_trans = sigmoid_basis(x, [-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "@interact(w0 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w1 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w2 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w3 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w4 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          show_sum = False)\n",
    "def plot_sigmoid(w0, w1, w2, w3, w4, show_sum):\n",
    "  plt.figure(figsize=(10,5));\n",
    "  w = np.array([w0, w1, w2, w3, w4])\n",
    "  labels = ['$\\sigma((x+1)/' + str(s) + ')$', '$\\sigma((x+0.5)/' + str(s) + ')$', \n",
    "            '$\\sigma((x)/' + str(s) + ')$', '$\\sigma((x-0.5)/' + str(s) + ')$', \n",
    "            '$\\sigma((x-1)/' + str(s) + ')$']\n",
    "  y = np.sum(w*x_trans, axis=1)\n",
    "  if show_sum:\n",
    "    sns.lineplot(x=x.squeeze(), y=y, label='sum', alpha=1, lw=2);\n",
    "  for i in range(5):\n",
    "    sns.lineplot(x=x.squeeze(), y=w[i]*x_trans[:,i], label=labels[i], alpha=0.5);\n",
    "  plt.ylim(-1.5, 2)\n",
    "  plt.title(\"Sigmoidal basis\");\n",
    "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0);\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('$\\phi(x)$')\n",
    "  plt.show()"
   ],
   "id": "9b2772c9-a4e8-41a9-be62-e7f740b1eeca"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(try changing `s` to see its effect!)"
   ],
   "id": "c60fddc9-a8a0-4adc-ba07-b38f48b07d42"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourier basis\n",
    "\n",
    "Transform a feature $x$ using $\\phi_j(x) = cos(\\pi j x) + sin(\\pi j x)$, i.e.\n",
    "\n",
    "$$y \\approx w_0 + w_1 \\sin(\\pi x) + w_2 \\cos(\\pi x) + w_3 \\sin(\\pi  2 x) + w_4 \\cos(\\pi  2 x) + \\ldots $$"
   ],
   "id": "44424ea5-eb7b-4d7d-a2af-56089d2f6529"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_basis(x, d):\n",
    "  sins = np.sin(np.pi*np.arange(1,d+1)*x)\n",
    "  coss = np.cos(np.pi*np.arange(1,d+1)*x)\n",
    "  return np.hstack([sins, coss])\n",
    "    \n",
    "x = np.arange(-1.5,1.5,step=0.01).reshape(-1,1)\n",
    "x_trans = fourier_basis(x, 2)\n",
    "\n",
    "@interact(w1 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w2 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w3 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          w4 = widgets.FloatSlider(min=-2, max=2, step=0.1, value=1),\n",
    "          show_sum = False)\n",
    "def plot_fourier(w1, w2, w3, w4, show_sum):\n",
    "  plt.figure(figsize=(10,5));\n",
    "  w = np.array([w1, w2, w3, w4])\n",
    "  labels = [\"sin(\\pi x)\", \"sin(\\pi 2 x)\", \"cos(\\pi x)\", \"cos(\\pi 2 x)\"]\n",
    "  y = np.sum(w*x_trans, axis=1)\n",
    "  if show_sum:\n",
    "    sns.lineplot(x=x.squeeze(), y=y, label='sum', alpha=1, lw=2);\n",
    "  for i in range(4):\n",
    "    sns.lineplot(x=x.squeeze(), y=w[i]*x_trans[:,i], label='$' + labels[i] + '$', alpha=0.5);\n",
    "  plt.ylim(-1.5, 2)\n",
    "  plt.title(\"Fourier basis\");\n",
    "  plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0);\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('$\\phi(x)$')\n",
    "  plt.show()"
   ],
   "id": "a407ca7f-dbc8-4db9-a694-7b3b5bc46866"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original vs transformed feature space\n",
    "\n",
    "When we use one of these (or another function) to transform our data before fitting a linear regression model, we take a problem that is not necessarily *linear in the original feature space*, and move it to a *transformed feature space where it is linear*.\n",
    "\n",
    "Take the following example: suppose data is generated as\n",
    "\n",
    "$$y_i = w_0 + w_1 \\log {x_i} + \\epsilon_i $$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$."
   ],
   "id": "9540657b-5245-4d61-8e9b-a18c67f68e9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "x = np.random.uniform(0.2, 4, size=n)\n",
    "y = 3*np.log(x) + 0.3 * np.random.randn(n)"
   ],
   "id": "3242a3ce-89f3-4e58-97ac-f15eda1ddb95"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot get a good fit for a linear model in the original feature space:"
   ],
   "id": "690c44ae-6814-4f6d-80f4-c9d84c730206"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "plt.title(\"Original feature space\")\n",
    "sns.regplot(x=x,y=y, ci=None)\n",
    "plt.xlabel(\"x\");\n",
    "plt.ylabel(\"y\");"
   ],
   "id": "2b4624ee-9ae2-416b-8af3-f32e1a317ea1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in the transformed feature space, where $\\log (x)$ is a feature, we can get a good fit:"
   ],
   "id": "9290baa5-328d-492f-b71a-9ae098ce6ff9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "plt.title(\"Transformed feature space\")\n",
    "sns.regplot(x=np.log(x),y=y, ci=None)\n",
    "plt.xlabel(\"log x\");\n",
    "plt.ylabel(\"y\");"
   ],
   "id": "bb1f1a7c-3760-43e4-a09e-d7ea38a6b251"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize this process in 2D. In the following example, we have\n",
    "\n",
    "$$y_i = w_0 + w_1 x_i + w_2 \\log {x_i} + \\epsilon_i $$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$."
   ],
   "id": "2772708a-3c05-48cb-bdc4-06f47ac2b1e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "x = np.random.uniform(0.2, 4, size=n)\n",
    "y = -1*x + 3*np.log(x)  + 7 +  0.5 * np.random.randn(n)"
   ],
   "id": "3928d311-66c4-4fba-b2c2-c2f4d4701b3a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev=20, azim=-20, X=x, y=y):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "\n",
    "\n",
    "    X1 = np.arange(0.2, 5, 0.2)\n",
    "    X2 = np.log(np.arange(0.2, 5, 0.2))\n",
    "    X1, X2 = np.meshgrid(X1, X2)\n",
    "    Z = X1*(-1) + X2*3\n",
    "\n",
    "    # Plot the surface.\n",
    "    ax.plot_surface(X1, X2, Z, alpha=0.1, color='gray',\n",
    "                          linewidth=0, antialiased=False)\n",
    "    ax.scatter3D(X, np.log(X), y, s=50)\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('log x')\n",
    "    ax.set_zlabel('y')\n",
    "    ax.set_title(\"Transformed feature space\")\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20), \n",
    "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
    "         X=fixed(x), y=fixed(y));"
   ],
   "id": "a28bf47d-4ac9-4bc4-a5c1-eecb61a40662"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   First, set the elevation to 0 and azimuth to 90 to see the view in the original feature space, with $x$ on the horizontal axis and $y$ on the vertical axis.\n",
    "-   When we add a second feature by “transforming” $x$, we move from a 1D feature space to a 2D feature space. Set azimuth to 30 to see the problem in 2D."
   ],
   "id": "8b426aa3-fd82-4886-a14e-cf71af878c4d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a transformation"
   ],
   "id": "5ecf07e5-182f-46c6-97a2-0ba9994602d3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you decide what transformation to apply to the data?\n",
    "\n",
    "-   domain knowledge\n",
    "-   exploratory data analysis\n",
    "-   residual analysis (after model fitting)\n",
    "-   trial and error"
   ],
   "id": "4a4ba15f-ea74-415d-9ef8-5cd2a7d272c7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ],
   "id": "bd9c1b80-996e-455d-a5ae-d71dc486f32f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s consider an example! Now suppose we have a process that generates data as\n",
    "\n",
    "$$y_i = w_0 + w_1 x_{i,1} + w_2 x_{i,2} + w_3 x_{i,1}^2 + w_4 x_{i,2}^2 + w_5 x_{i,1} x_{i,2} + \\epsilon_i $$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$. In other words:\n",
    "\n",
    "$$\\mathbf{\\phi} = [1, x_1, x_2, x_1^2, x_2^2, x_1 x_2] $$\n",
    "\n",
    "Note that the model is *linear* in $\\textbf{w}$.\n",
    "\n",
    "The $x_1 x_2$ term is called an *interaction* term, and reflects that the effect of $x_1$ on $y$ may depend on the value of $x_2$."
   ],
   "id": "5d075624-69a0-4541-80df-b541c6152efc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def generate_linear_basis_data(n=200, d=2, coef=[1,1,0.5,0.5,1], intercept=1, sigma=0):\n",
    "  x = np.random.randn(n,d)\n",
    "  x = np.column_stack((x, x**2 ))\n",
    "  for pair in list(itertools.combinations(range(d), 2)):\n",
    "    x = np.column_stack((x, x[:,pair[0]]*x[:,pair[1]]))\n",
    "  y = (np.dot(x, coef) + intercept).squeeze() + sigma * np.random.randn(n)\n",
    "  return x[:,:d], y"
   ],
   "id": "9975a0a7-719e-4ac1-9e0b-4b3e9ea016a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_basis_data(sigma=0.2)\n",
    "x_test,  y_test  = generate_linear_basis_data(n=50, sigma=0.2)"
   ],
   "id": "62f571b5-cd86-4dae-92e4-91cfc83f55e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ],
   "id": "c0af7500-e152-416f-82d3-8873589a043f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.scatter(x_train[:,0],  y_train);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.subplot(1,2,2);\n",
    "plt.scatter(x_train[:,1],  y_train);\n",
    "plt.xlabel(\"x2\");\n",
    "plt.ylabel(\"y\");"
   ],
   "id": "536273f8-c998-4e82-b7d2-900aa5dd2466"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev, azim, w0, w1, w2, w3, w4, w5, show_sum, show_basis, show_data, X, y):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "\n",
    "    X1 = np.arange(-4, 4, 0.2)\n",
    "    X2 = np.arange(-4, 4, 0.2)\n",
    "    X1, X2 = np.meshgrid(X1, X2)\n",
    "    Z0 = w0*np.ones(shape=(X1.shape[0], X2.shape[0]))\n",
    "    Z1 = w1*X1\n",
    "    Z2 = w2*X2\n",
    "    Z3 = w3*X1**2\n",
    "    Z4 = w4*X2**2\n",
    "    Z5 = w5*X1*X2\n",
    "\n",
    "    # Plot the surfaces.\n",
    "    if show_basis:\n",
    "      ax.plot_surface(X1, X2, Z0, alpha=0.1, color=sns.color_palette()[1], linewidth=0, antialiased=False)\n",
    "      ax.plot_surface(X1, X2, Z1, alpha=0.1, color=sns.color_palette()[2], linewidth=0, antialiased=False)\n",
    "      ax.plot_surface(X1, X2, Z2, alpha=0.1, color=sns.color_palette()[3], linewidth=0, antialiased=False)\n",
    "      ax.plot_surface(X1, X2, Z3, alpha=0.1, color=sns.color_palette()[4], linewidth=0, antialiased=False)\n",
    "      ax.plot_surface(X1, X2, Z4, alpha=0.1, color=sns.color_palette()[5], linewidth=0, antialiased=False)\n",
    "      ax.plot_surface(X1, X2, Z5, alpha=0.1, color=sns.color_palette()[6], linewidth=0, antialiased=False)\n",
    "    if show_sum:\n",
    "      ax.plot_surface(X1, X2, (Z0+Z1+Z2+Z3+Z4+Z5), alpha=0.5, color='white', linewidth=0, antialiased=False)\n",
    "    if show_data:\n",
    "      ax.scatter3D(X[:, 0], X[:, 1], y, s=50)\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('y')\n",
    "    ax.set_zlim(0, 25)\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20), \n",
    "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
    "          w0 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w1 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w2 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "          w3 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=0.5),\n",
    "          w4 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=0.5),\n",
    "          w5 = widgets.FloatSlider(min=-1, max=2, step=0.1, value=1),\n",
    "         show_sum = False, show_basis = False, show_data = False,\n",
    "         X=fixed(x_train), y=fixed(y_train));"
   ],
   "id": "195cbe09-0f3e-4682-9cfe-7d0bf8c2dd79"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression"
   ],
   "id": "687749d6-1881-475d-9259-f83e71b7a7f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lbf = LinearRegression().fit(x_train, y_train)\n",
    "print(\"Intercept: \" , reg_lbf.intercept_)\n",
    "print(\"Coefficient list: \", reg_lbf.coef_)"
   ],
   "id": "468e47b0-d4fb-4e6a-8f68-64c3014e9b83"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute MSE and R2"
   ],
   "id": "eb761c5f-ea06-4931-9f3b-e0f9dca8bc1f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = reg_lbf.predict(x_train)\n",
    "print(\"Training MSE: \", metrics.mean_squared_error(y_train, y_train_hat))\n",
    "print(\"Training R2:  \", metrics.r2_score(y_train, y_train_hat))"
   ],
   "id": "15eff7b4-3266-4514-844f-ab40053f9559"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot hyperplane"
   ],
   "id": "93b228eb-4fa5-4eb0-b849-47085d3071be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev=20, azim=-20, X=x_train, y=y_train):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "\n",
    "\n",
    "    X1 = np.arange(-4, 4, 0.2)\n",
    "    X2 = np.arange(-4, 4, 0.2)\n",
    "    X1, X2 = np.meshgrid(X1, X2)\n",
    "    Z = X1*reg_lbf.coef_[0] + X2*reg_lbf.coef_[1] + reg_lbf.intercept_\n",
    "\n",
    "    # Plot the surface.\n",
    "    ax.plot_surface(X1, X2, Z, alpha=0.1, color='gray',\n",
    "                          linewidth=0, antialiased=False)\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], y, s=50)\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('y')\n",
    "    plt.show()\n",
    "\n",
    "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20), \n",
    "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
    "         X=fixed(x_train), y=fixed(y_train));"
   ],
   "id": "40c29554-2d39-4b31-bf9d-91b383592bcf"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual analysis"
   ],
   "id": "f4ea357e-e2ad-4b6d-a08f-935710bb1a1b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_train = y_train - y_train_hat"
   ],
   "id": "7c8bfffb-a501-419e-aa7d-020988f4d045"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_hi = np.max(np.concatenate([y_train, y_train_hat]))\n",
    "lim_lo = np.min(np.concatenate([y_train, y_train_hat]))\n",
    "sns.scatterplot(x=y_train, y=y_train_hat);\n",
    "plt.xlabel('y');\n",
    "plt.ylabel('y_hat');\n",
    "plt.xlim(lim_lo, lim_hi);\n",
    "plt.ylim(lim_lo, lim_hi);"
   ],
   "id": "830fc8f0-ea83-49af-bf65-da593f444c12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the error random? Or does it look systematic?"
   ],
   "id": "db9e2e81-8143-4524-a02f-5a2ddf56842a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_train_hat, y=residual_train);\n",
    "plt.xlabel('y_hat');\n",
    "plt.ylabel('Residual');"
   ],
   "id": "c756cc7a-f0ba-41c8-825b-1512afb74eaa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.scatter(x_train[:,0],  residual_train);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"Residual\");\n",
    "plt.subplot(1,2,2);\n",
    "plt.scatter(x_train[:,1],  residual_train);\n",
    "plt.xlabel(\"x2\");\n",
    "plt.ylabel(\"Residual\");"
   ],
   "id": "e69a480f-4a69-4eb7-a2ed-bdbc5cd2da81"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is clearly some non-linearity, we can try to fit a model to a non-linear transformation of the features."
   ],
   "id": "5b6c5535-2a43-4445-b84e-92e23d8016ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_trans = np.column_stack((x_train, x_train**2))\n",
    "\n",
    "reg_lbf_trans = LinearRegression().fit(x_train_trans, y_train)\n",
    "print(\"Intercept: \" , reg_lbf_trans.intercept_)\n",
    "print(\"Coefficient list: \", reg_lbf_trans.coef_)\n",
    "\n",
    "y_train_trans_hat = reg_lbf_trans.predict(x_train_trans)\n",
    "print(\"Training MSE: \", metrics.mean_squared_error(y_train, y_train_trans_hat))\n",
    "print(\"Training R2:  \", metrics.r2_score(y_train, y_train_trans_hat))\n",
    "\n",
    "residual_train_trans = y_train - y_train_trans_hat"
   ],
   "id": "ed706d0f-aec6-4d54-bd13-16b53dd563c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_hi = np.max(np.concatenate([y_train, y_train_trans_hat]))\n",
    "lim_lo = np.min(np.concatenate([y_train, y_train_trans_hat]))\n",
    "sns.scatterplot(x=y_train, y=y_train_trans_hat);\n",
    "plt.xlabel('y');\n",
    "plt.ylabel('y_hat');\n",
    "plt.xlim(lim_lo, lim_hi);\n",
    "plt.ylim(lim_lo, lim_hi);"
   ],
   "id": "f0781ca1-e0c0-4234-bbbf-d5733940c738"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_train, y=residual_train_trans);\n",
    "plt.xlabel('y');\n",
    "plt.ylabel('Residual');"
   ],
   "id": "65b37137-a373-4f88-ac09-69c30ac214a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.scatter(x_train[:,0],  residual_train_trans);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"Residual\");\n",
    "plt.subplot(1,2,2);\n",
    "plt.scatter(x_train[:,1],  residual_train_trans);\n",
    "plt.xlabel(\"x2\");\n",
    "plt.ylabel(\"Residual\");"
   ],
   "id": "4bd01499-72f9-4e49-a3b7-03c8b6c12072"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_inter = np.column_stack((x_train_trans, x_train[:,0]*x_train[:,1]))\n",
    "\n",
    "reg_lbf_inter = LinearRegression().fit(x_train_inter, y_train)\n",
    "print(\"Intercept: \" , reg_lbf_inter.intercept_)\n",
    "print(\"Coefficient list: \", reg_lbf_inter.coef_)\n",
    "\n",
    "y_train_inter_hat = reg_lbf_inter.predict(x_train_inter)\n",
    "print(\"Training MSE: \", metrics.mean_squared_error(y_train, y_train_inter_hat))\n",
    "print(\"Training R2:  \", metrics.r2_score(y_train, y_train_inter_hat))\n",
    "\n",
    "residual_train_inter = y_train - y_train_inter_hat"
   ],
   "id": "f055f4df-a73c-40b7-94ab-1e6b4522bda7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim_hi = np.max(np.concatenate([y_train, y_train_inter_hat]))\n",
    "lim_lo = np.min(np.concatenate([y_train, y_train_inter_hat]))\n",
    "sns.scatterplot(x=y_train, y=y_train_inter_hat);\n",
    "plt.xlabel('y');\n",
    "plt.ylabel('y_hat');\n",
    "plt.xlim(lim_lo, lim_hi);\n",
    "plt.ylim(lim_lo, lim_hi);"
   ],
   "id": "6701eedc-f2ca-4490-87f2-895360d4a01b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=y_train, y=residual_train_inter);\n",
    "plt.xlabel('y');\n",
    "plt.ylabel('Residual');"
   ],
   "id": "4a119b9e-296e-4bc2-90a6-9aecf562146e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.scatter(x_train[:,0],  residual_train_inter);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"Residual\");\n",
    "plt.subplot(1,2,2);\n",
    "plt.scatter(x_train[:,1],  residual_train_inter);\n",
    "plt.xlabel(\"x2\");\n",
    "plt.ylabel(\"Residual\");"
   ],
   "id": "84dd7f2b-6619-4995-919b-ab5d0b0244fe"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test set"
   ],
   "id": "aaa8a512-4012-4444-8e0e-106c91886d1c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = reg_lbf_inter.predict(x_train_inter)\n",
    "print(\"Training MSE: \", metrics.mean_squared_error(y_train, y_train_hat))\n",
    "print(\"Training R2:  \", metrics.r2_score(y_train, y_train_hat))"
   ],
   "id": "950abf82-187a-4200-8d33-3a6459107d1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_inter = np.column_stack((x_test, x_test**2))\n",
    "x_test_inter = np.column_stack((x_test_inter, x_test[:,0]*x_test[:,1]))\n",
    "\n",
    "y_test_hat = reg_lbf_inter.predict(x_test_inter)\n",
    "print(\"Test MSE: \", metrics.mean_squared_error(y_test, y_test_hat))\n",
    "print(\"Test R2:  \", metrics.r2_score(y_test, y_test_hat))"
   ],
   "id": "86fa7c34-16f7-4596-8670-d071292cb9c6"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
