{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo: Logistic regression for classification of handwritten digits\n",
    "=================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will explore the use of logistic regression for\n",
    "classification of handwritten digits. In other words, given an image of\n",
    "a handwritten digit, we want to classify it as a 0, 1, 2, 3, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the digits dataset\n",
    "\n",
    "For this demo, we will use a dataset known as\n",
    "[MNIST](https://en.wikipedia.org/wiki/MNIST_database). It contains\n",
    "70,000 samples of handwritten digits, size-normalized and centered in a\n",
    "fixed-size image. Each sample is represented as a 28x28 pixel array, so\n",
    "there are 784 features per samples.\n",
    "\n",
    "We will start by loading the dataset using the `fetch_openml` function.\n",
    "This function allows us to retrieve a dataset by name from\n",
    "[OpenML](https://www.openml.org/), a public repository for machine\n",
    "learning data and experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the data has 784 features and we have 70,000 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variables is a label for each digit: 0, 1, 2, 3, 4, 5, 6, 7,\n",
    "8, 9. There are 6000-8000 samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['0', '1', '2','3', '4','5', '6', '7', '8', '9']\n",
    "nclasses = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each “feature” represents a pixel in the image, and each pixel can take\n",
    "on any integer value from 0 to 255. A large value for a pixel means that\n",
    "there is writing in that part of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a few examples, by plotting the 784 features as a 28x28 grid.\n",
    "In these images, white pixels indicate high values in the feature\n",
    "matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_class = 5\n",
    "figure = plt.figure(figsize=(nclasses*2,(1+samples_per_class*2)));\n",
    "\n",
    "for idx_cls, cls in enumerate(classes):\n",
    "  idxs = np.flatnonzero(y == cls)\n",
    "  idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "  for i, idx in enumerate(idxs):\n",
    "    plt_idx = i * nclasses + idx_cls + 1\n",
    "    p = plt.subplot(samples_per_class, nclasses, plt_idx);\n",
    "    p = sns.heatmap(np.reshape(X[idx], (28,28)), cmap=plt.cm.gray, \n",
    "             xticklabels=False, yticklabels=False, cbar=False);\n",
    "    p = plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "\n",
    "Next, we will split our data into a test and training set.\n",
    "\n",
    "We can use `train_test_split` from `sklearn.model_selection` to split\n",
    "the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset is very large, it can take a long time to train a\n",
    "classifier on it. We just want to use it to demonstrate some useful\n",
    "concepts, so we will work with a smaller subset of the dataset. When we\n",
    "split the data using the `train_test_split` function, we will specify\n",
    "that we want 7,500 samples in the training set and 2,500 samples in the\n",
    "test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=9,\n",
    "                                     train_size=7500, test_size=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’l also scale the data so that each feature takes on a value between 0\n",
    "and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train/255.0\n",
    "X_test_scaled = X_test/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a classifier using logistic regression\n",
    "\n",
    "Finally, we are ready to train a classifier. We will use `sklearn`'s\n",
    "[LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "Unlike the linear regression, there is no closed form solution to the\n",
    "least squares parameter estimate in logistic regression. Therefore, we\n",
    "need to use a “solver” which finds a numerical solution. Several solvers\n",
    "are available for use with `sklearn`'s `LogisticRegression`, but they\n",
    "don’t all support all varieties of logistic regression.\n",
    "\n",
    "We will use the `saga` solver, which\n",
    "\n",
    "-   works well when there is a large number of samples,\n",
    "-   supports logistic regression with no regularization penalty, L1\n",
    "    penalty, L2 penalty, or ElasticNet (which uses both penalties),\n",
    "-   and also supports multinomial regression with multiple classes,\n",
    "    using the softmax function.\n",
    "\n",
    "In addition to specifying which solver we want to use, we also specify a\n",
    "tolerance, which gives stopping criteria for the solver. A higher\n",
    "tolerance will finish faster, but may not find the optimal solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='none', \n",
    "                         tol=0.1, solver='saga',\n",
    "                         multi_class='multinomial').fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the classifier has been trained (fitted), we can get the\n",
    "coefficient values.\n",
    "\n",
    "We had 784 features - one for each pixel - so we will have 784\n",
    "coefficients. Furthermore, we have 10 classes, so we will have a vector\n",
    "of 784 coefficients for each of the 10 classes.\n",
    "\n",
    "Therefore, our coefficient matrix has 10 rows and 784 columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coef_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the coefficients of the logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit of logistic regression is its interpretability - we can use\n",
    "the coefficient values to understand what features (i.e. which pixels)\n",
    "are important in determining what class a sample belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot shows the coefficient vector for each class, with\n",
    "positive coefficients in blue and negative coefficients in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = np.max(np.abs(clf.coef_))\n",
    "\n",
    "p = plt.figure(figsize=(25, 2.5));\n",
    "\n",
    "for i in range(nclasses):\n",
    "    p = plt.subplot(1, nclasses, i + 1)\n",
    "    p = plt.imshow(clf.coef_[i].reshape(28, 28),\n",
    "                  cmap=plt.cm.RdBu, vmin=-scale, vmax=scale);\n",
    "    p = plt.axis('off')\n",
    "    p = plt.title('Class %i' % i);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see which pixels are positively associated with belonging to the\n",
    "class, and which pixels are negatively associated with belonging to the\n",
    "class.\n",
    "\n",
    "For example, consider Class 0. If a sample has large values in the\n",
    "pixels shown in blue (the 0 shape around the center of the image), the\n",
    "probability of that sample being a 0 digit increases. If the sample has\n",
    "large values in the pixels in the center of the image, the probability\n",
    "of the sample being a 0 digit decreases.\n",
    "\n",
    "Many pixels have coefficients whose magnitude are very small. These are\n",
    "shown in white, and they are not very important for this classification\n",
    "task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a fitted logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the coefficient matrix, we can get the per-class probability for\n",
    "any sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that for logistic regression with the softmax function, the\n",
    "conditional probability of a sample belonging to class $k$ is given by:\n",
    "\n",
    "$$P(y=k | \\mathbf{x}) = \\frac{e^{z_k}}{\\sum_{\\ell=1}^K e^{z_\\ell}}$$\n",
    "\n",
    "where $z_{k} = w_k x$.\n",
    "\n",
    "($w_k$ is the weight vector for class $k$, and $x$ includes a 1s column\n",
    "so that the intercept can be included in the weight matrix.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let’s look at a specific test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test_scaled[sample_idx].reshape(28,28), cmap='gray');\n",
    "plt.title('Label: %s\\n' % y_test[sample_idx]);\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’l compute $z_k$ for each class $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [ clf.intercept_[k] + np.dot(clf.coef_[k], X_test_scaled[sample_idx]) for k in range(10) ]\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can compute the conditional probability for each class, for\n",
    "this sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = [np.exp(z[k]) for k in range(10)]\n",
    "exps_sum = np.sum(exps)\n",
    "probs = exps/exps_sum\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the first entry is the probability of belonging to class 0 (i.e.\n",
    "having the label '0'), the second entry is the probability of belonging\n",
    "to class 1, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=np.arange(0,10), y=probs);\n",
    "plt.ylabel(\"Probability\");\n",
    "plt.xlabel(\"Class\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, to get the predicted *label*, we can find the class with the\n",
    "highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_cls = np.argmax(probs)\n",
    "classes[idx_cls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If* this matches the actual label for the first test sample, then our\n",
    "prediction is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[sample_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LogisticRegression` implementation in `sklearn` includes functions\n",
    "to compute both the per-class probability, and the most likely label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `predict_proba` function on the logistic regression to\n",
    "get these probabilities. For each sample, it returns 10 probabilities -\n",
    "one for each of the ten classes (i.e. each value of $k$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = clf.predict_proba(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at our example test point, and compare to our own\n",
    "computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob[sample_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `predict` function to predict a label for each sample in the\n",
    "test set. This will return the class label with the highest probability.\n",
    "\n",
    "For our test sample, the prediction is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[sample_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the true value is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[sample_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate classifier performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first important metric is the accuracy - what percent of predicted\n",
    "labels are the same as the true labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few ways to compute this value -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy =  np.mean(y_test == y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = clf.score(X_test_scaled, y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about other important metrics?\n",
    "\n",
    "For a binary classifier, we also care about\n",
    "\n",
    "-   The number of true positive (TP) outputs - samples from the positive\n",
    "    class that are predicted as positive\n",
    "-   The number of true negative (TN) outputs - samples from the negative\n",
    "    class that are predicted as negative\n",
    "-   The number of false positive (FP) outputs - samples from the\n",
    "    negative class that are predicted as positive, and\n",
    "-   The number of false negative (FN) outputs - samples from the\n",
    "    positive class that are predicted as negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are often presented together in a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a multi-class problem, we can extend the confusion matrix to have\n",
    "more rows and columns. The diagonal of the multi-class confusion matrix\n",
    "shows the number of correct classifications for each class, and other\n",
    "entries show instances where a sample from one class was mistakenly\n",
    "assigned a different class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a confusion matrix using the `pandas` library's `crosstab`\n",
    "function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.crosstab(y_test, y_pred, \n",
    "                               rownames=['Actual'], colnames=['Predicted'])\n",
    "p = plt.figure(figsize=(10,10));\n",
    "p = sns.heatmap(cm, annot=True, fmt=\"d\", cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a version that is slightly easier to interpret - we have\n",
    "normalized the confusion matrix by row, so that the entries on the\n",
    "diagonal show the accuracy per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.crosstab(y_test, y_pred, \n",
    "                               rownames=['Actual'], colnames=['Predicted'], normalize='index')\n",
    "p = plt.figure(figsize=(10,10));\n",
    "p = sns.heatmap(cm, annot=True, fmt=\".2f\", cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the digits 0, 1, 4 are easiest for the logistic\n",
    "regression to classify, while the digits 8, 5, 2, and 3 are more\n",
    "difficult (because the classification accuracay was less for these\n",
    "digits).\n",
    "\n",
    "We can also see which digits are easily confused with one another. For\n",
    "example, we can see that some 8s are misclassified as 1s, and some 5s\n",
    "are misclassified as 8s."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "colab": {
   "name": "4-logistic-regression-digits.ipynb"
  }
 }
}
