{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case study: COMPAS and classifier fairness\n",
    "------------------------------------------\n",
    "\n",
    "![ProPublica\n",
    "headline](https://static.propublica.org/projects/algorithmic-bias/assets/img/generated/opener-b-crop-2400*1350-00796e.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About COMPAS\n",
    "------------\n",
    "\n",
    "COMPAS is a tool used in many jurisdictions around the U.S. to predict\n",
    "*recidivism* risk - the risk that a criminal defendant will reoffend.\n",
    "\n",
    "-   COMPAS assigns scores from 1 (lowest risk) to 10 (highest risk).\n",
    "-   It also assigns a class: each sample is labeled as high risk of\n",
    "    recidivism, medium risk of recidivism, or low risk of recidivism.\n",
    "    For this analysis, we turn it into a binary classification problem\n",
    "    by re-labeling as medium or high risk of recidivism vs. low risk of\n",
    "    recidivism.\n",
    "-   As input, the model uses 137 factors, including age, gender, and\n",
    "    criminal history of the defendant.\n",
    "-   Race is *not* an explicit feature considered by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using COMPAS\n",
    "\n",
    "-   Judges can see the defendant’s COMPAS score when deciding whether to\n",
    "    detain the defendant prior to trial and/or when sentencing.\n",
    "-   Defendants who are classified medium or high risk (scores of 5-10),\n",
    "    are more likely to be held in prison while awaiting trial than those\n",
    "    classified as low risk (scores of 1-4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProPublica claims (1)\n",
    "\n",
    "> Prediction Fails Differently for Black Defendants\n",
    "\n",
    "|                                           | WHITE | AFRICAN AMERICAN |\n",
    "|-------------------------------------------|:------|:-----------------|\n",
    "| Labeled Higher Risk, But Didn’t Re-Offend | 23.5% | 44.9%            |\n",
    "| Labeled Lower Risk, Yet Did Re-Offend     | 47.7% | 28.0%            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProPublica claims (2)\n",
    "\n",
    "> Overall, Northpointe’s assessment tool correctly predicts recidivism\n",
    "> 61 percent of the time. But blacks are almost twice as likely as\n",
    "> whites to be labeled a higher risk but not actually re-offend. It\n",
    "> makes the opposite mistake among whites: They are much more likely\n",
    "> than blacks to be labeled lower risk but go on to commit other crimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replicating ProPublica analysis\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform into a binary classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s make this a binary classification problem. We will add a\n",
    "new column that translates the risk score (`decile_score`) into a binary\n",
    "label.\n",
    "\n",
    "Any score 5 or higher (Medium or High risk) means that a defendant is\n",
    "treated as a likely recividist, and a score of 4 or lower (Low risk)\n",
    "means that a defendant is treated as unlikely to reoffend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_med_or_high_risk']  = (df['decile_score']>=5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using this data as a case study in evaluating the performance of\n",
    "a binary classifier.\n",
    "\n",
    "-   Which column in this data frame represents $\\hat{y}$, the prediction\n",
    "    of a binary classifier?\n",
    "-   Which column in this data frame represents $y$, the actual outcome\n",
    "    that the classifier is trying to predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of the model, we will compare the model’s\n",
    "predictions to the “truth”:\n",
    "\n",
    "-   The risk score prediction of the COMPAS system is in the\n",
    "    `decile_score` column,\n",
    "-   The classification of COMPAS as medium/high risk or low risk is in\n",
    "    the `is_med_or_high_risk` column\n",
    "-   The “true” recidivism value (whether or not the defendant committed\n",
    "    another crime in the next two years) is in the `two_year_recid`\n",
    "    column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start by computing the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df['is_med_or_high_risk']==df['two_year_recid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to “prediction by mode”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(df['two_year_recid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This, itself, might already be considered problematic...\n",
    "\n",
    "-   it’s not at all obvious that pretrial release and sentencing\n",
    "    decisions benefit from a risk assessment that is just a bit better\n",
    "    than a random classifier, and\n",
    "-   it’s not obvious that the people using these risk assessment scores\n",
    "    (for example, judges) are aware that the accuracy is so low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score includes both kinds of errors:\n",
    "\n",
    "-   false positives (defendant is predicted as medium/high risk but does\n",
    "    not reoffend)\n",
    "-   false negatives (defendant is predicted as low risk, but does\n",
    "    reoffend)\n",
    "\n",
    "but these errors have different costs.\n",
    "\n",
    "(Which has a higher “cost”: giving an overly harsh sentence to someone\n",
    "who will not reoffend, or giving a too-lenient sentence to someone who\n",
    "will reoffend?)\n",
    "\n",
    "It can be useful to pull out the different types of errors separately,\n",
    "to see the rate of different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we create a confusion matrix, we can use it to derive a whole set of\n",
    "classifier metrics:\n",
    "\n",
    "-   True Positive Rate (TPR) also called recall or sensitivity\n",
    "-   True Negative Rate (TNR) also called specificity\n",
    "-   Positive Predictive Value (PPV) also called precision\n",
    "-   Negative Predictive Value (NPV)\n",
    "-   False Positive Rate (FPR)\n",
    "-   False Discovery Rate (FDR)\n",
    "-   False Negative Rate (FNR)\n",
    "-   False Omission Rate (FOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ffund.github.io/intro-ml-tss21/images/ConfusionMatrix.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.crosstab(df['is_med_or_high_risk'], df['two_year_recid'], \n",
    "                               rownames=['Predicted'], colnames=['Actual'])\n",
    "p = plt.figure(figsize=(5,5));\n",
    "p = sns.heatmap(cm, annot=True, fmt=\"d\", cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `sklearn`'s `confusion_matrix` to pull out these values\n",
    "and compute any metrics of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[tn , fp],[fn , tp]]  = confusion_matrix(df['two_year_recid'], df['is_med_or_high_risk'])\n",
    "print(\"True negatives:  \", tn)\n",
    "print(\"False positives: \", fp)\n",
    "print(\"False negatives: \", fn)\n",
    "print(\"True positives:  \", tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can compute them directly using `crosstab` -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we normalize by row - show the PPV, FDR, FOR, NPV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.crosstab(df['is_med_or_high_risk'], df['two_year_recid'], \n",
    "                               rownames=['Predicted'], colnames=['Actual'], normalize='index')\n",
    "p = plt.figure(figsize=(5,5));\n",
    "p = sns.heatmap(cm, annot=True, fmt=\".2f\", cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we normalize by colum - show the TPR, FPR, FNR, TNR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.crosstab(df['is_med_or_high_risk'], df['two_year_recid'], \n",
    "                               rownames=['Predicted'], colnames=['Actual'], normalize='columns')\n",
    "p = plt.figure(figsize=(5,5));\n",
    "p = sns.heatmap(cm, annot=True, fmt=\".2f\", cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we see that a defendant has a similar likelihood of being\n",
    "wrongly labeled a likely recidivist and of being wrongly labeled as\n",
    "unlikely to reoffend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = fp/(fp+tn)\n",
    "tpr = tp/(tp+fn)\n",
    "fnr = fn/(fn+tp)\n",
    "tnr = tn/(tn+fp)\n",
    "\n",
    "print(\"False positive rate (overall): \", fpr)\n",
    "print(\"False negative rate (overall): \", fnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also directly evaluate the risk score, instead of just the\n",
    "labels. The risk score is meant to indicate the probability that a\n",
    "defendant will reoffend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = df.groupby('decile_score').agg({'two_year_recid': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=d);\n",
    "plt.ylim(0,1);\n",
    "plt.ylabel('Recidivism rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defendants with a higher COMPAS score indeed had higher rates of\n",
    "recidivism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can look at the ROC curve and AUC, which tells us how to\n",
    "work with the FPR-TPR tradeoff by setting the threshold for “medium or\n",
    "high risk” at different decile score levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "fpr, tpr, thresholds = roc_curve(df['two_year_recid'], df['decile_score'])\n",
    "auc = roc_auc_score(df['two_year_recid'], df['decile_score'])\n",
    "sns.lineplot(x=fpr, y=tpr, color='gray', alpha=0.5);\n",
    "sns.scatterplot(x=fpr, y=tpr, hue=pd.Categorical(thresholds), legend='full');\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--');\n",
    "plt.title('AUC: %s' % ('{0:.4f}'.format(auc)));\n",
    "plt.xlabel(\"False positive rate\");\n",
    "plt.ylabel(\"True positive rate\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness\n",
    "\n",
    "A useful reference for the fairness definitions in this notebook:\n",
    "[Fairness Definitions\n",
    "Explained](https://fairware.cs.umass.edu/papers/Verma.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMPAS has been under scrutiny for issues related for fairness with\n",
    "respect to race of the defendant.\n",
    "\n",
    "Race is not an explicit input to COMPAS, but some of the questions that\n",
    "*are* used as input may have strong correlations with race.\n",
    "\n",
    "First, we will find out how frequently each race is represented in the\n",
    "data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['race'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will focus specifically on African-American or Caucasian defendants,\n",
    "since they are the subject of the ProPublica claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.race.isin([\"African-American\",\"Caucasian\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s compare the accuracy for the two groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['two_year_recid']==df['is_med_or_high_risk']).astype(int).groupby(df['race']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It isn’t exactly the same, but it’s similar - within a few points. This\n",
    "is a type of fairness known as **overall accuracy equality**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s see whether a defendant who is classified as medium/high\n",
    "risk has the same probability of recidivism for the two groups.\n",
    "\n",
    "In other words, we will compute the PPV for each group:\n",
    "\n",
    "$$PPV = \\frac{TP}{TP+FP} = P(y=1 | \\hat{y} = 1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['is_med_or_high_risk']==1]['two_year_recid'].groupby(df['race']).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, similar (within a few points). This is a type of fairness known\n",
    "as **predictive parity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend this idea, to check whether a defendant with a given score\n",
    "has the same probability of recidivism for the two groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(df.groupby(['decile_score','race']).agg({'two_year_recid': 'mean'}))\n",
    "d = d.reset_index()\n",
    "im = sns.scatterplot(data=d, x='decile_score', y='two_year_recid', hue='race');\n",
    "im.set(ylim=(0,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that for both African-American and Caucasian defendants, for\n",
    "any given COMPAS score, recidivism rates are similar. This is a type of\n",
    "fairness known as **calibration**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will look at the frequency with which defendants of each race\n",
    "are assigned each COMPAS score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"race\", margin_titles=True);\n",
    "g.map(plt.hist, \"decile_score\", bins=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that Caucasian defendants in this sample are more likely to\n",
    "be assigned a low risk score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, to evaluate whether this is *unfair*, we need to know the true\n",
    "prevalence - whether the rates of recividism are the same in both\n",
    "populations, according to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('race').agg({'two_year_recid': 'mean',  \n",
    "                        'is_med_or_high_risk': 'mean', \n",
    "                        'decile_score': 'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions of the model are fairly close to the actual prevalence\n",
    "in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, our analysis suggests that COMPAS is fair with respect to race:\n",
    "\n",
    "-   The overall accuracy of the COMPAS label is the same, regardless of\n",
    "    race (**overall accuracy equality**)\n",
    "-   The likelihood of recividism among defendants labeled as medium or\n",
    "    high risk is similar, regardless of race (**predictive parity**)\n",
    "-   For any given COMPAS score, the risk of recidivism is similar,\n",
    "    regardless of race - the “meaning” of the score is consistent across\n",
    "    race (**calibration**)\n",
    "\n",
    "We do not have **statistical parity** (a type of fairness corresponding\n",
    "to equal probability of positive classification), but we don’t\n",
    "necessarily expect to when the prevalance of actual positive is\n",
    "different between groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisiting the ProPublica claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProPublica made a specific claim:\n",
    "\n",
    "> 23.5% of Caucasian defendants, 44.9% of African-American defendants\n",
    "> were “Labeled Higher Risk, But Didn’t Re-Offend”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What metric should we check to evaluate whether this claim is correct?\n",
    "\n",
    "$$FDR   = \\frac{FP}{FP + TP} = P(y = 0 | \\hat{y} = 1)$$\n",
    "\n",
    "$$FPR = \\frac{FP}{FP + TN} = P(\\hat{y} = 1 | y=0)$$\n",
    "\n",
    "$$FOR   = \\frac{FN}{FN + TN} = P(y = 1 |\\hat{y} = 0)$$\n",
    "\n",
    "$$FNR = \\frac{FN}{TP + FN} = P(\\hat{y} = 0 | y=1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is “Labeled Higher Risk, But Didn’t Re-Offend” the same thing as \"Didn’t\n",
    "Re-Offend, But Labeled Higher Risk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following image, the top row shows the confusion matrix\n",
    "normalized by row - the PPV, FDR, FOR, NPV - by race. The bottom row\n",
    "shows the confusion matrix normalized by column - TPR, FPR, FNR, TNR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(9,9));\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "for i, race in enumerate(['Caucasian', 'African-American']):\n",
    "  cm = pd.crosstab(df[df.race.eq(race)]['is_med_or_high_risk'], df[df.race.eq(race)]['two_year_recid'], \n",
    "                               rownames=['Predicted'], colnames=['Actual'], normalize='index')\n",
    "  p = plt.subplot(2,2,i+1)\n",
    "  p = sns.heatmap(cm, annot=True, fmt=\".2f\", cbar=False, vmin=0, vmax=1)\n",
    "  p = plt.title(\"PPV, FDR, FOR, NPV\\nfor %s defendants\" % race)\n",
    "\n",
    "for i, race in enumerate(['Caucasian', 'African-American']):\n",
    "  cm = pd.crosstab(df[df.race.eq(race)]['is_med_or_high_risk'], df[df.race.eq(race)]['two_year_recid'], \n",
    "                               rownames=['Predicted'], colnames=['Actual'], normalize='columns')\n",
    "  p = plt.subplot(2,2,i+3)\n",
    "  p = sns.heatmap(cm, annot=True, fmt=\".2f\", cbar=False, vmin=0, vmax=1)\n",
    "  p = plt.title(\"TPR, FPR, FNR, TNR\\nfor %s defendants\" % race)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we fix it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we adjust the thresholds separately for each group, to try and\n",
    "equalize the error rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {'Caucasian': 4, 'African-American': 6}\n",
    "df['threshold'] = df['race'].map(thresholds)\n",
    "df['is_med_or_high_risk']  = (df['decile_score']>=df['threshold']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(9,9));\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "for i, race in enumerate(['Caucasian', 'African-American']):\n",
    "  cm = pd.crosstab(df[df.race.eq(race)]['is_med_or_high_risk'], df[df.race.eq(race)]['two_year_recid'], \n",
    "                               rownames=['Predicted'], colnames=['Actual'], normalize='index')\n",
    "  p = plt.subplot(2,2,i+1)\n",
    "  p = sns.heatmap(cm, annot=True, fmt=\".2f\", cbar=False, vmin=0, vmax=1)\n",
    "  p = plt.title(\"PPV, FDR, FOR, NPV\\nfor %s defendants\" % race)\n",
    "\n",
    "for i, race in enumerate(['Caucasian', 'African-American']):\n",
    "  cm = pd.crosstab(df[df.race.eq(race)]['is_med_or_high_risk'], df[df.race.eq(race)]['two_year_recid'], \n",
    "                               rownames=['Predicted'], colnames=['Actual'], normalize='columns')\n",
    "  p = plt.subplot(2,2,i+3)\n",
    "  p = sns.heatmap(cm, annot=True, fmt=\".2f\", cbar=False, vmin=0, vmax=1)\n",
    "  p = plt.title(\"TPR, FPR, FNR, TNR\\nfor %s defendants\" % race)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it so tricky to satisfy multiple types of fairness at once? This\n",
    "is due to a proven *impossibility result*.\n",
    "\n",
    "Any time\n",
    "\n",
    "-   the *base rate* (prevalence of the positive condition) is different\n",
    "    in the two groups, and\n",
    "-   we do not have a perfect classifier\n",
    "\n",
    "Then we cannot simultaneously satisfy:\n",
    "\n",
    "-   Equal PPV and NPV for both groups (known as **conditional use\n",
    "    accuracy equality**), and\n",
    "-   Equal FPR and FNR for both groups (known as **equalized odds** or\n",
    "    **conditional procedure accuracy equality**)\n",
    "\n",
    "The proof is in: [Inherent Trade-Offs in the Fair Determination of Risk\n",
    "Scores](https://arxiv.org/pdf/1609.05807.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interactive online demos on this:\n",
    "\n",
    "-   Google's People + AI + Research (PAIR) group explainer: [Measuring\n",
    "    fairness](https://pair.withgoogle.com/explorables/measuring-fairness/)\n",
    "-   Another Google Explainer: [Attacking discrimination with smarter\n",
    "    machine\n",
    "    learning](https://research.google.com/bigpicture/attacking-discrimination-in-ml/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are human decision-makers more fair?\n",
    "------------------------------------\n",
    "\n",
    "From [The accuracy, fairness, and limits of predicting\n",
    "recidivism](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5777393/)\n",
    "(Julia Dressel and Hany Farid, January 2018):\n",
    "\n",
    "![Human vs. COMPAS\n",
    "predictions](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5777393/bin/aao5580-F1.jpg)\n",
    "\n",
    "> Participants saw a short description of a defendant that included the\n",
    "> defendant’s sex, age, and previous criminal history, but not their\n",
    "> race (see Materials and Methods). Participants predicted whether this\n",
    "> person would recidivate within 2 years of their most recent crime.… We\n",
    "> compare these results with the performance of COMPAS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we learned\n",
    "\n",
    "-   A model can be biased with respect to age, race, gender, even if\n",
    "    those features are not used as input to the model. (“Fairness\n",
    "    through unawareness” is often not very helpful!)\n",
    "-   Human biases and unfairness in society leak into the data used to\n",
    "    train machine learning models. For example, if Black defendants are\n",
    "    subject to closer monitoring than white defendants, they might have\n",
    "    higher rights of *measured* recidivism even if the underlying rates\n",
    "    of offense were the same.\n",
    "-   There are many measures of fairness, it may be impossible to satisfy\n",
    "    some combination of these simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What we can do\n",
    "\n",
    "What can we do to improve fairness of our machine learning models?\n",
    "\n",
    "-   Are there policy/legal actions that might help?\n",
    "    -   Justice in Forensic Algorithms Act [proposed by Rep. Mark\n",
    "        Takano](https://medium.com/@repmarktakano/opening-the-black-box-of-forensic-algorithms-6194493b9960)\n",
    "        would give defendants access to source code - would that help\n",
    "        defendants facing unfair machine learning models?\n",
    "-   What can we do, as machine learning engineers, that might help?\n",
    "    -   Exploratory data analysis - look for possible underlying bias in\n",
    "        the data.\n",
    "    -   Avoid using sensitive group as a feature (or a proxy for a\n",
    "        sensitive group), but this doesn’t necessarily help. (It didn’t\n",
    "        help in this example.) Sometimes, using the sensitive group to\n",
    "        explicitly *add* fairness might be better.\n",
    "    -   Make sure different groups are well represented in the data.\n",
    "    -   End users must be made aware of what the model output means -\n",
    "        for example, judges should understand that a “high risk” label\n",
    "        only means a 65% chance of reoffending.\n",
    "    -   Evaluate final model for bias with respect to sensitive groups.\n",
    "    -   May not be possible to satisfy multiple fairness metrics\n",
    "        simultaneously, work with end users to decide which fairness\n",
    "        metrics to prioritize, and to create a model that is fair w.r.t.\n",
    "        that metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details on the COMPAS analysis\n",
    "-----------------------------------\n",
    "\n",
    "-   Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner, May\n",
    "    2016, [Machine\n",
    "    Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\n",
    "-   Jeff Larson, Surya Mattu, Lauren Kirchner and Julia Angwin, May\n",
    "    2016, [How We Analyzed the COMPAS Recidivism\n",
    "    Algorithm](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)\n",
    "-   William Dieterich, Christina Mendoza, and Tim Brennan, July 2016,\n",
    "    [COMPAS Risk Scales: Demonstrating Accuracy Equity and Predictive\n",
    "    Parity](http://go.volarisgroup.com/rs/430-MBX-989/images/ProPublica_Commentary_Final_070616.pdf)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "colab": {
   "name": "4-compas-case-study.ipynb"
  }
 }
}
