{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization: in depth\n",
    "========================\n",
    "\n",
    "*Fraida Fund*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background: regularization\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization helps us control the bias-variance tradeoff of a machine\n",
    "learning model by giving us a parameter with which we can tune model\n",
    "“complexity”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of minimizing the MSE, we define a new loss function that\n",
    "combines the MSE and a penalty function. Then, we minimize this new loss\n",
    "function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, with Ridge regularization, the penalty term is the squared\n",
    "L2 norm of the coefficients, so our new loss function is:\n",
    "\n",
    "$$J(w) = \\|y - X w\\|^2_2 + \\alpha \\|w\\|^2_2$$\n",
    "\n",
    "The complexity parameter $\\alpha$ controls the level of regularization:\n",
    "the larger the value of $\\alpha$, the more the penalty term “counts”,\n",
    "and the “simpler” the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With LASSO regularization, the penalty term is the L1 norm of the\n",
    "coefficients, so our new loss function is:\n",
    "\n",
    "$$J(w) = \\|y - X w\\|^2_2 + \\alpha \\|w\\|_1$$\n",
    "\n",
    "Again, the complexity parameter $\\alpha$ controls the level of\n",
    "regularization: the larger the value of $\\alpha$, the more the penalty\n",
    "term “counts”, and the “simpler” the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common regularizer is ElasticNet, which combines both L1 and L2\n",
    "regularization:\n",
    "\n",
    "$$J(w) \\frac{1}{2n} ||X w - y||_2 ^ 2 + \\alpha \\left( \\rho ||w||_1 +\n",
    "\\frac{(1-\\rho)}{2} ||w||_2 ^ 2 \\right)$$\n",
    "\n",
    "Here, we control the regularization with two parameters:\n",
    "\n",
    "-   $\\alpha$ controls the overall regularization level. Greater $\\alpha$\n",
    "    corresponds to a simpler model\n",
    "-   $\\rho$ is the L1 ratio, and is a value between 0 and 1 that controls\n",
    "    the weight of the L1 penalty relative to the L2 penalty. Greater\n",
    "    $\\rho$ means more L1 regularization and less L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to convince you that both L1 and L2\n",
    "regularization can reduce the variance of a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# for 3d interactive plots\n",
    "from ipywidgets import interact, fixed\n",
    "import ipywidgets as widgets\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split, validation_curve\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression (L2 penalty)\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Ridge regularization, the penalty term is the squared L2 norm of\n",
    "the coefficients, so our loss function is:\n",
    "\n",
    "$$J(w) = \\|y - X w\\|^2_2 + \\alpha \\|w\\|^2_2$$\n",
    "\n",
    "(If the data is not zero-mean, the intercept $w_0$ is not included in\n",
    "the penalty term.)\n",
    "\n",
    "The complexity parameter $\\alpha$ controls the level of regularization:\n",
    "the larger the value of $\\alpha$, the more the penalty term “counts”,\n",
    "and the “simpler” the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Textbook reference on collinearity in linear regression**: For more\n",
    "background reading on this topic, please refer to the discussion on\n",
    "collinearity starting on Page 99 of Introduction to Statistical Learning\n",
    "with Applications in R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression reduces variance for many problems, but it is\n",
    "especially helpful in reducing the variance of models trained on\n",
    "*collinear features*.\n",
    "\n",
    "You may recall that we saw collinear features before, in our case study\n",
    "on linear regression. There, the individual attractiveness ratings of\n",
    "six “rankers” were highly correlated with one another, as well as with\n",
    "the average attractiveness ranking. We saw that a very large positive\n",
    "coefficient for one variable can be “canceld” by a similarly large\n",
    "negative coefficient on a correlated variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore this further, we’l reintroduce a familiar function, our\n",
    "`generate_linear_regression_data` function, with the ability to generate\n",
    "partly collinear features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_regression_data(n=100, d=1, coef=[5], intercept=1, sigma=0, uninformative=0, collinear=0, collinear_sigma = 0):\n",
    "  x = np.random.uniform(-1,1,size=(n,d-uninformative-collinear))\n",
    "  y = (np.dot(x, coef) + intercept).squeeze() + sigma * np.random.randn(n)\n",
    "  if collinear:\n",
    "    # select random columns from x\n",
    "    collinear_cols = np.random.choice(d-uninformative-collinear,size=collinear, replace=True)\n",
    "    # generate some random coefficients for the new collinear columns\n",
    "    collinear_coefs = np.random.uniform(1, 10, collinear)\n",
    "    # generate noise so new columns are not exactly collinear\n",
    "    collinear_noise = collinear_sigma*np.random.normal(size=(n,collinear))\n",
    "    # value of new collinear columns\n",
    "    collinear_features = collinear_coefs * x[:, collinear_cols] + collinear_noise\n",
    "    x = np.column_stack((x, collinear_features))\n",
    "  if uninformative:\n",
    "    x = np.column_stack((x, sigma*np.random.randn(n,uninformative)))\n",
    "\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see how this data generating function works. We will generate some\n",
    "data with five features. The first three features are independent of one\n",
    "another, but the last two features will be highly collinear with a\n",
    "randomly selected column from the first three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_linear_regression_data(n=50, d=5, coef=[2, 6, 5], collinear=2, collinear_sigma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(np.hstack((x,y.reshape(-1,1))),\n",
    "                          columns = ['x0', 'x1', 'x2', 'x3', 'x4', 'y']));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the collinearity in the pairplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we’l compute the matrix of correlation coefficients for the five\n",
    "features. An element of this matrix that is large (close to 1) indicates\n",
    "a pair of highly correlated variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x, rowvar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An aside - our function creates new columns that are each a linear\n",
    "function of one other column. In this case, we can see the collinearity\n",
    "by looking at the correlation coefficient, as we have done here.\n",
    "\n",
    "More generally, a new column may be a linear combination of *multiple*\n",
    "columns. This is known as *multicollinearity* and it wouldn’t\n",
    "necessarily be obvious from the correlation coefficient *or* from the\n",
    "pairplot.\n",
    "\n",
    "To identify multicollinearity, we should check the Variance Inflation\n",
    "Factor (VIF). A VIF of 1 means no collinearity; a large VIF indicates\n",
    "multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = [variance_inflation_factor(x, i) for i in range(x.shape[1])]\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE contours with collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models trained on collinear data have high variance. This is because\n",
    "there can be many *very different* combinations of coefficients that\n",
    "have a similar MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, suppose you have a “true” function $f(x_1)$:\n",
    "\n",
    "$$ y = 2x_1 $$\n",
    "\n",
    "Now suppose you also have an exactly collinear feature, $x_2 = 4 x_1$.\n",
    "\n",
    "There are now many coefficients that can exactly predict $y$ using\n",
    "different weights of $x_1$ and $x_2$. We can use:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y &= 2 x_1  \\\\\n",
    "y &= \\frac{1}{2} x_2  \\\\\n",
    "y &= x_1 + \\frac{1}{4} x_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The toy example above features exactly collinear features, but even\n",
    "partly collinear features are problematic, for the same reason. And with\n",
    "partly collinear features, we can’t just disregard one of the features.\n",
    "A partly collinear feature may capture information that isn’t included\n",
    "in any other feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further explore the effect of collinearity on a linear regression,\n",
    "consider the following demo. Here, we compare the MSE contours for two\n",
    "linear regressions: one with two linearly independent features, and one\n",
    "with two collinear features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a, y_a = generate_linear_regression_data(n=1000, d=2, coef=[6], sigma=0.1, intercept=intercept, collinear=1, collinear_sigma=0.5)\n",
    "x_b, y_b = generate_linear_regression_data(n=1000, d=2, coef=[6, -6], sigma=0.1, intercept=intercept, collinear=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(-30, 30, 0.1)\n",
    "mses_a = np.zeros((len(coefs), len(coefs)))\n",
    "mses_b = np.zeros((len(coefs), len(coefs)))\n",
    "\n",
    "for idx_1, c_1 in enumerate(coefs):\n",
    "  for idx_2, c_2 in enumerate(coefs):\n",
    "    y_a_coef = (intercept + np.dot(x_a,[c_1, c_2])).squeeze()\n",
    "    mses_a[idx_1,idx_2] =  1.0/(len(y_a_coef)) * np.sum((y_a - y_a_coef)**2)\n",
    "    y_b_coef = (intercept + np.dot(x_b,[c_1, c_2])).squeeze()\n",
    "    mses_b[idx_1,idx_2] =  1.0/(len(y_b_coef)) * np.sum((y_b - y_b_coef)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6));\n",
    "\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_a, levels=np.arange(0, 1000, 20), cmap='Spectral');\n",
    "plt.scatter(0,2, marker='*', color='white', s=100);\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');\n",
    "plt.title(\"Two collinear features\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_b, levels=np.arange(0, 1000, 20), cmap='Spectral');\n",
    "plt.scatter(-2,2, marker='*', color='white', s=100);\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');\n",
    "plt.title(\"Two independent features\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On both contour plots, the dark red region shows the combination of\n",
    "coefficients where MSE is smallest, and the star marks the “true”\n",
    "coefficients.\n",
    "\n",
    "-   On the right, with two independent features, all of the combinations\n",
    "    of coefficients that have small MSE are fairly similar.\n",
    "-   On the left, with collinear features, we can see that the region\n",
    "    with small MSE spans a broad range of coefficient values. The region\n",
    "    appears as a long, narrow “valley” in the MSE contour. (If you would\n",
    "    plot a “higher is better” metric, such as R2, instead of a “lower is\n",
    "    better” metric like MSE, you would see a long, narrow “ridge”\n",
    "    (Ridge!!!) in the contour plot!)\n",
    "\n",
    "We can see that with collinear features, a small change in the training\n",
    "data could cause the pair of coefficient values that have the smallest\n",
    "MSE - the least squares estimate - to move anywhere along the “valley”.\n",
    "The model will have much more variance due to the much larger range of\n",
    "likely coefficient estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation: collinear features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further emphasize this point, here's a simulation that compares\n",
    "\n",
    "-   Model A with one feature\n",
    "-   Model B with two collinear features\n",
    "\n",
    "This simulation is similar to the ones we saw in a previous discussion\n",
    "on bias and variance. We will generate test data, then generate many\n",
    "independent training sets from the same distribution. For each\n",
    "independent training set, we train each model, and then use the fitted\n",
    "models to make predictions on the test data.\n",
    "\n",
    "Afterwards, we will look at:\n",
    "\n",
    "-   The difference between the average prediction of all the fitted\n",
    "    models and the true function (bias)\n",
    "-   The difference between the average prediction of all the fitted\n",
    "    models and the predictions of individual models (variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 200\n",
    "n_test = 50\n",
    "n_train = 50\n",
    "sigma = 0.5\n",
    "collinear_sigma = 0.2\n",
    "\n",
    "# generate test data once\n",
    "x_test, y_test = generate_linear_regression_data(n=n_test, d=2, coef=[2], \n",
    "                intercept=1, sigma=sigma, collinear=1, collinear_sigma=collinear_sigma)\n",
    "y_test_no_noise =  np.dot(x_test[:,0:1], [2]) + 1\n",
    "\n",
    "\n",
    "# prepare some matrices for storing simulation results\n",
    "y_predict = np.zeros((n_test, n_repeat, 2))\n",
    "\n",
    "y_test_err = np.zeros((n_test, n_repeat, 2))\n",
    "y_test_bias = np.zeros((n_test, 2))\n",
    "\n",
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in range(n_repeat):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  x_train, y_train = generate_linear_regression_data(n=n_train, d=2, coef=[2], \n",
    "                intercept=1, sigma=sigma, collinear=1, collinear_sigma=collinear_sigma)\n",
    "\n",
    "  # model A: use only the first column\n",
    "  y_predict[:, i, 0] = LinearRegression().fit(x_train[:, 0:1], y_train).predict(x_test[:,0:1])\n",
    "  # model B: use all columns\n",
    "  y_predict[:, i, 1] = LinearRegression().fit(x_train, y_train).predict(x_test)\n",
    "\n",
    "  # overall squared error is due to difference between f_t(x) + epsilon, model prediction f(x) \n",
    "  y_test_err[:, i, 0] = (y_test - y_predict[:, i, 0])**2\n",
    "  y_test_err[:, i, 1] = (y_test - y_predict[:, i, 1])**2\n",
    "\n",
    "# bias is due to difference between f_t(x) and mean of model prediction f(x) \n",
    "y_test_bias[:, 0] = (y_test_no_noise - y_predict[:, :, 0].mean(axis=1))**2\n",
    "y_test_bias[:, 1] = (y_test_no_noise - y_predict[:, :, 1].mean(axis=1))**2\n",
    "\n",
    "# irreducible error is due to the difference between f_t(x) and f_t(x) + epsilon\n",
    "y_test_noise_var = (y_test_no_noise - y_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(10,10));\n",
    "\n",
    "models = ['Model A', 'Model B']\n",
    "\n",
    "for midx, model in enumerate(models):\n",
    "  p = plt.subplot(2, 2, midx+1);\n",
    "  for i in range(n_repeat):\n",
    "    p = sns.lineplot(x=x_test[:,0], y=y_predict[:, i, midx], alpha=0.05, color='orange');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_no_noise, color='black', label='True function');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_predict[:, :, midx].mean(axis=1), color='red', label='Mean of models');\n",
    "  p = sns.scatterplot(x=x_test[:,0], y=y_test, label='Test data');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('y');\n",
    "  p = plt.title(model);\n",
    "  p = plt.ylim(y_test.min()*1.1, y_test.max()*1.1)\n",
    " \n",
    "  p = plt.subplot(2, 2, midx+3);\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_err[:, :, midx].mean(axis=1), color='red', alpha=0.5, label='Total squared error', markers=True);\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_bias[:, midx], color='cyan', alpha=0.5, label='Bias^2');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_predict[:, :, midx].var(axis=1), color='green', alpha=0.5, label='Variance');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_noise_var, color='purple', alpha=0.2, label='Irreducibile error');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('Error');\n",
    "  p = plt.title(\"MSE: %f \\n Irreducible noise: %f \\n Bias^2: %f \\n Variance: %f \" % \n",
    "                (y_test_err[:,:,midx].mean(axis=(0,1)), \n",
    "                 y_test_noise_var.mean(), \n",
    "                 (y_test_bias[:, midx]).mean(),\n",
    "                  y_predict[:, :, midx].var(axis=1).mean() ) )\n",
    "  p = plt.ylim(0, 1.1* y_test_err[:, :, :].mean(axis=(1)).max() )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual orange lines in the two top subplots show the predictions\n",
    "of the model when it is trained on each of the many independent training\n",
    "sets.\n",
    "\n",
    "Note that for Model A, on the left, all the orange lines are similar -\n",
    "the predictions on the test set are similar for all the different\n",
    "training sets (low variance).\n",
    "\n",
    "For Model B, on the right, the different training sets lead to very\n",
    "different coefficient estimates, and therefore, high variance in the\n",
    "model estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this:\n",
    "\n",
    "-   What happens to the variance when you change `collinear_sigma`,\n",
    "    which determines *how* collinear the columns are?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE contours with L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does Ridge regularization reduce variance? Recall our MSE contour\n",
    "plots from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6));\n",
    "\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_a, levels=np.arange(0, 1000, 20), cmap='Spectral');\n",
    "plt.scatter(0,2, marker='*', color='white', s=100);\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');\n",
    "plt.title(\"Two collinear features\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_b, levels=np.arange(0, 1000, 20), cmap='Spectral');\n",
    "plt.scatter(-2,2, marker='*', color='white', s=100);\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');\n",
    "plt.title(\"Two independent features\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are collinear features, there are very different coefficient\n",
    "estimates that all have similar MSE. The coefficient estimate is very\n",
    "unstable, so the model has high variance.\n",
    "\n",
    "To reduce the variance, we want to make sure that the training algorithm\n",
    "selects similar coefficients every time, even when there are slight\n",
    "differences in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when we introduce the L2 penalty term on top of our MSE\n",
    "contours? In the following plot, we show L2 penalty contours for the\n",
    "coefficients overlaid on top of MSE contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(-30, 30, 0.1)\n",
    "l2_penalty_coefs = np.zeros((len(coefs), len(coefs)))\n",
    "for idx_1, c_1 in enumerate(coefs):\n",
    "  for idx_2, c_2 in enumerate(coefs):\n",
    "        l2_penalty_coefs[idx_1,idx_2] =  np.linalg.norm( [c_1, c_2], ord=2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6));\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_a, levels=np.arange(0, 1000, 20), cmap='Spectral');\n",
    "p = plt.contour(X1, X2, l2_penalty_coefs, levels=[0, 25, 50, 100, 200], cmap='binary');\n",
    "plt.scatter(0,2, marker='*', color='white', s=100);\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');\n",
    "plt.title(\"Two collinear features\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_b, levels=np.arange(0, 1000, 20), cmap='Spectral');\n",
    "p = plt.contour(X1, X2, l2_penalty_coefs, levels=[0, 25, 50, 100, 200], cmap='binary');\n",
    "plt.scatter(-2,2, marker='*', color='white', s=100);\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');\n",
    "plt.title(\"Two independent features\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE term in the loss function tends to push our coefficient\n",
    "estimates towards the “true” coefficients, in the center of the dark red\n",
    "region. At the same time, the L2 penalty term tends to push our\n",
    "coefficient estimate towards the origin. The $\\alpha$ parameter\n",
    "determines the relative strength of each effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the case with collinear features. When we used MSE alone as our\n",
    "loss function, there was a wide range of coefficients with almost\n",
    "equivalent MSE. Now, with the addition of the L2 penalty, there is a\n",
    "much smaller range of coefficients with small MSE *and* small L2\n",
    "penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this more clearly if we plot the contour of the Ridge loss\n",
    "function\n",
    "\n",
    "$$J(w) = \\|y - X w\\|^2_2 + \\alpha \\|w\\|^2_2$$\n",
    "\n",
    "with $\\alpha=1$, instead of the MSE contour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6));\n",
    "alpha = 1\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_a+alpha*l2_penalty_coefs, levels=np.arange(0, 1000, 20), cmap='Spectral');\n",
    "plt.scatter(0,2, marker='*', color='white', s=100);\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');\n",
    "plt.title(\"Two collinear features\");\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_b+alpha*l2_penalty_coefs, levels=np.arange(0, 1000, 20), cmap='Spectral');\n",
    "plt.scatter(-2,2, marker='*', color='white', s=100);\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');\n",
    "plt.title(\"Two independent features\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, the regularized loss function shrinks the region of\n",
    "coefficients for which the loss function is very small - the region of\n",
    "likely coefficient estimates.\n",
    "\n",
    "In the case with collinear features, though, the *range* of coefficients\n",
    "with small loss shrinks dramatically due to regularization. The L2\n",
    "penalty term “pulls up” on the parts of the “ridge” that are far from\n",
    "the origin, increasing the value of the loss function there. The long,\n",
    "narrow “ridge” becomes a small peak, and the coefficient estimates are\n",
    "now much more stable.\n",
    "\n",
    "However, the “true” coefficients may no longer be included in the region\n",
    "where the value of the loss function is smallest - the L2 penalty\n",
    "introduces *bias*, moving the coefficient estimate away from the true\n",
    "value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation: L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the effect of the L2 penalty in simulation. Here, we\n",
    "compare:\n",
    "\n",
    "-   Model A, which is a least squares linear regression trained on two\n",
    "    collinear features.\n",
    "-   Model B, which is a linear regression with an L2 penalty, also\n",
    "    trained on two collinear features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 1000\n",
    "n_test = 50\n",
    "n_train = 50\n",
    "sigma = 0.5\n",
    "\n",
    "# generate test data once\n",
    "x_test, y_test = generate_linear_regression_data(n=n_test, d=2, coef=[2], intercept=1, sigma=sigma, collinear=1, collinear_sigma=0.1)\n",
    "y_test_no_noise =  np.dot(x_test[:,0:1], [2]) + 1\n",
    "\n",
    "# prepare some matrices for storing simulation results\n",
    "y_predict = np.zeros((n_test, n_repeat, 2))\n",
    "\n",
    "y_test_err = np.zeros((n_test, n_repeat, 2))\n",
    "y_test_bias = np.zeros((n_test, 2))\n",
    "\n",
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in range(n_repeat):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  x_train, y_train = generate_linear_regression_data(n=n_train, d=2, coef=[2], sigma=sigma, collinear=1, collinear_sigma=0.1)\n",
    "\n",
    "  # model A: use only the first column\n",
    "  y_predict[:, i, 0] = LinearRegression().fit(x_train, y_train).predict(x_test)\n",
    "  # model B: use all columns\n",
    "  y_predict[:, i, 1] = Ridge(alpha=1).fit(x_train, y_train).predict(x_test)\n",
    "\n",
    "  # overall squared error is due to difference between f_t(x) + epsilon, model prediction f(x) \n",
    "  y_test_err[:, i, 0] = (y_test - y_predict[:, i, 0])**2\n",
    "  y_test_err[:, i, 1] = (y_test - y_predict[:, i, 1])**2\n",
    "\n",
    "# bias is due to difference between f_t(x) and mean of model prediction f(x) \n",
    "y_test_bias[:, 0] = (y_test_no_noise - y_predict[:, :, 0].mean(axis=1))**2\n",
    "y_test_bias[:, 1] = (y_test_no_noise - y_predict[:, :, 1].mean(axis=1))**2\n",
    "\n",
    "# irreducible error is due to the difference between f_t(x) and f_t(x) + epsilon\n",
    "y_test_noise_var = (y_test_no_noise - y_test)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(10,10));\n",
    "\n",
    "models = ['Model A', 'Model B']\n",
    "\n",
    "for midx, model in enumerate(models):\n",
    "  p = plt.subplot(2, 2, midx+1);\n",
    "  for i in range(n_repeat):\n",
    "    p = sns.lineplot(x=x_test[:,0], y=y_predict[:, i, midx], alpha=0.05, color='orange');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_no_noise, color='black', label='True function');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_predict[:, :, midx].mean(axis=1), color='red', label='Mean of models');\n",
    "  p = sns.scatterplot(x=x_test[:,0], y=y_test, label='Test data');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('y');\n",
    "  p = plt.title(model);\n",
    "  p = plt.ylim(y_test.min()*1.1, y_test.max()*1.1)\n",
    " \n",
    "  p = plt.subplot(2, 2, midx+3);\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_err[:, :, midx].mean(axis=1), color='red', alpha=0.5, label='Total squared error', markers=True);\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_bias[:, midx], color='cyan', alpha=0.5, label='Bias^2');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_predict[:, :, midx].var(axis=1), color='green', alpha=0.5, label='Variance');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_noise_var, color='purple', alpha=0.2, label='Irreducibile error');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('Error');\n",
    "  p = plt.title(\"MSE: %f \\n Irreducible error: %f \\n Bias^2: %f \\n Variance: %f \" % \n",
    "                (y_test_err[:,:,midx].mean(axis=(0,1)), \n",
    "                 y_test_noise_var.mean(), \n",
    "                 (y_test_bias[:, midx]).mean(),\n",
    "                  y_predict[:, :, midx].var(axis=1).mean() ) )\n",
    "  p = plt.ylim(0, 1.1* y_test_err[:, :, :].mean(axis=(1)).max() )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model B, which uses an L2 penalty term in the loss function, has much\n",
    "lower variance than Model A.\n",
    "\n",
    "However, Model B also has higher bias.\n",
    "\n",
    "While there is some estimation error in individual coefficient estimates\n",
    "of a least squares linear regression, it will estimate the “true”\n",
    "coefficients *on average*, so as long as there is no under-modeling, it\n",
    "will be unbiased. We can see that the mean prediction for this test set\n",
    "(red line) is almost exactly the same as the true function (black line)\n",
    "in the top left plot.\n",
    "\n",
    "The Ridge regression, on the other hand, does *not* estimate the “true”\n",
    "coefficient on average, since the L2 penalty term pushes it to estimate\n",
    "coefficients with slightly smaller magnitude than the true values.\n",
    "Therefore, its estimate is biased even if there is no under-modeling. We\n",
    "can see that the mean prediction for this test set (red line) is *not*\n",
    "the same as the true function (black line) in the top right plot. The\n",
    "mean prediction has a coefficient with a slightly smaller magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $\\alpha$ increases the weight of the L2 penalty term, the variance of\n",
    "the model decreases, but the bias increases. The optimal $\\alpha$ for a\n",
    "given problem will depend on whether bias outweighs variance in the MSE\n",
    "breakdown. In a model with high bias, regularization will make the\n",
    "overall MSE worse. In a model with high variance, regularization will\n",
    "increase bias but will still improve MSE.\n",
    "\n",
    "How should we choose $\\alpha$, if it increases bias but decreases\n",
    "variance? The choice of $\\alpha$ is essentially a model selection\n",
    "problem. We can use cross validation to select the $\\alpha$ for which\n",
    "the validation MSE is the lowest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing inputs\n",
    "\n",
    "One important thing to be aware of: the L2 penalty term\n",
    "disproportionately “shrinks” large coefficients compared to small\n",
    "coefficients.\n",
    "\n",
    "You can think of the L2 penalty as a rubber band connecting each\n",
    "coefficient value and the origin. The rubber band “pulls” the\n",
    "coefficient towards the origin, but it pull more strongly on large\n",
    "coefficients, for which the rubber band is stretched more tightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be especially problematic when we have features with different\n",
    "scales, which in turn leads to coefficients on different scales. In\n",
    "least squares linear regression, scaling any variable wouldn’t affect\n",
    "the MSE. In Ridge regression, scaling a variable changes the way that\n",
    "the L2 penalty acts on its coefficient.\n",
    "\n",
    "To prevent this, and make sure Ridge penalizes all coefficients\n",
    "“fairly”, we will usually “standardize” inputs before applying Ridge\n",
    "regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore this further, we will generate linear data in which two\n",
    "features $x_1$ and $x_2$ are equally important for determining $y$, but\n",
    "the scale is different by a factor of 10, so the coefficients are also\n",
    "different by a factor of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a, y_a = generate_linear_regression_data(n=500, d=1, coef=[2])\n",
    "x_b, y_b = generate_linear_regression_data(n=500, d=1, coef=[2])\n",
    "x_b_scaled = x_b*10\n",
    "\n",
    "coefs = [2, 2/10.0]\n",
    "intercept=2\n",
    "\n",
    "y = y_a + y_b\n",
    "x = np.hstack((x_a, x_b_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what happens to the coefficients as we apply Ridge regression\n",
    "with increasing $\\alpha$, we are going to create a plot of the\n",
    "“regularization path”. This plot shows how the values of the\n",
    "coefficients change as the weight of the regularization penalty in the\n",
    "loss function changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "coefs_alpha = np.zeros((len(alpha_list), 2))\n",
    "\n",
    "for idx, alpha in enumerate(alpha_list):\n",
    "    regr_ridge = Ridge(alpha=alpha).fit(x,y)\n",
    "    coefs_alpha[idx] = regr_ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=alpha_list, y=coefs_alpha[:, 0], label='w1', marker='o');\n",
    "sns.lineplot(x=alpha_list, y=coefs_alpha[:, 1], label='w2', marker='o');\n",
    "plt.xscale('log')\n",
    "plt.ylabel(\"Coefficient value\");\n",
    "plt.xlabel(\" α\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the L2 penalty, it is much more effective to reduce large\n",
    "coefficients than small coefficients. The larger coefficient is\n",
    "therefore “pulled” much more strongly to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this (and for other reasons beyond the scope of this course),\n",
    "we typically scale or standardize our data before applying Ridge\n",
    "regression. The most common approach is to *standardize* - for each\n",
    "sample, we compute\n",
    "\n",
    "$$x_{i,d}' = (x_{i,d} - \\bar{x}_d) / \\sigma_{x_d}$$\n",
    "\n",
    "for each sample $i$ and feature $d$, where $\\bar{x}_d$ is the mean of\n",
    "the training samples for feature $d$ and $\\sigma_{x_d}$ is the standard\n",
    "deviation of the training samples for feature $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sklearn` library includes a `StandardScaler` that can be used for\n",
    "this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(x)\n",
    "x_scaled = scaler.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "coefs_alpha = np.zeros((len(alpha_list), 2))\n",
    "\n",
    "for idx, alpha in enumerate(alpha_list):\n",
    "    regr_ridge = Ridge(alpha=alpha).fit(x_scaled,y)\n",
    "    coefs_alpha[idx] = regr_ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=alpha_list, y=coefs_alpha[:, 0], label='w1', marker='o');\n",
    "sns.lineplot(x=alpha_list, y=coefs_alpha[:, 1], label='w2', marker='o');\n",
    "plt.xscale('log')\n",
    "plt.ylabel(\"Coefficient value\");\n",
    "plt.xlabel(\" α\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that the Ridge regularizer exerts the same “pull” on both\n",
    "coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note: When you use a `StandardScaler` to transform your\n",
    "training data before fitting your model, you must use the same\n",
    "`StandardScaler` object to transform your test data before using your\n",
    "fitted model for predictions!\n",
    "\n",
    "Don’t fit the `StandardScaler` again, just use the scaler that was\n",
    "*already fitted using your training data* on your test data, like\n",
    "\n",
    "    x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO regression (L1 penalty)\n",
    "-----------------------------\n",
    "\n",
    "With LASSO regularization, the penalty term is the L1 norm of the\n",
    "coefficients, so our loss function is:\n",
    "\n",
    "$$J(w) = \\|y - X w\\|^2_2 + \\alpha \\|w\\|_1$$\n",
    "\n",
    "(If the data is not zero-mean, the intercept $w_0$ is not included in\n",
    "the penalty term.)\n",
    "\n",
    "The complexity parameter $\\alpha$ controls the level of regularization:\n",
    "the larger the value of $\\alpha$, the more the penalty term “counts”,\n",
    "and the “simpler” the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "LASSO regression reduces variance in general, but it is especially\n",
    "helpful as an automated feature selection method.\n",
    "\n",
    "In a previous notebook, we saw that a model trained on a large number of\n",
    "features - especially so-called “uninformative” features - has very high\n",
    "variance. Ideally, we would want to use only the most informative\n",
    "features to train our model, and exclude the less informative features.\n",
    "This model selection problem is known as feature selection.\n",
    "\n",
    "LASSO “zeros out” some coefficients, so the features associated with\n",
    "them are not used for prediction. It is essentially an automated feature\n",
    "selection method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation: uninformative features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us remind ourselves of the effect of uninformative features, using a\n",
    "simulation. Here, we compare:\n",
    "\n",
    "-   Model A, which is a least squares linear regression trained on one\n",
    "    informative feature.\n",
    "\n",
    "$$t(x) = w_{0,t} + w_{1,t} x_1, \\quad f(x,w) = w_{0} + w_{1} x_1$$\n",
    "\n",
    "-   Model B, which is a least squares linear regression trained on one\n",
    "    informative feature and five uninformative features.\n",
    "\n",
    "$$t(x) = w_{0,t} + w_{1,t} x_1, \\quad  f(x,w) = w_{0} + w_{1} x_1 + w_2 x_2 + w_3 x_3 + w_4 x_4 + w_5 x_5 + w_6 x_6$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 1000\n",
    "n_test = 50\n",
    "n_train = 50\n",
    "sigma = 1\n",
    "\n",
    "\n",
    "# generate test data once\n",
    "x_test, y_test = generate_linear_regression_data(n=n_test, d=6, coef=[2], intercept=1, sigma=sigma, uninformative=5)\n",
    "y_test_no_noise =  np.dot(x_test[:,0:1], [2]) + 1\n",
    "\n",
    "\n",
    "# prepare some matrices for storing simulation results\n",
    "y_predict = np.zeros((n_test, n_repeat, 2))\n",
    "\n",
    "y_test_err = np.zeros((n_test, n_repeat, 2))\n",
    "y_test_bias = np.zeros((n_test, 2))\n",
    "\n",
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in range(n_repeat):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  x_train, y_train = generate_linear_regression_data(n=n_test, d=6, coef=[2], intercept=1, sigma=sigma, uninformative=5)\n",
    "\n",
    "  # model A: use only the first column\n",
    "  y_predict[:, i, 0] = LinearRegression().fit(x_train[:, 0:1], y_train).predict(x_test[:,0:1])\n",
    "  # model B: use all columns\n",
    "  y_predict[:, i, 1] = LinearRegression().fit(x_train, y_train).predict(x_test)\n",
    "\n",
    "  # overall squared error is due to difference between f_t(x) + epsilon, model prediction f(x) \n",
    "  y_test_err[:, i, 0] = (y_test - y_predict[:, i, 0])**2\n",
    "  y_test_err[:, i, 1] = (y_test - y_predict[:, i, 1])**2\n",
    "\n",
    "# bias is due to difference between f_t(x) and mean of model prediction f(x) \n",
    "y_test_bias[:, 0] = (y_test_no_noise - y_predict[:, :, 0].mean(axis=1))**2\n",
    "y_test_bias[:, 1] = (y_test_no_noise - y_predict[:, :, 1].mean(axis=1))**2\n",
    "\n",
    "# irreducible error is due to the difference between f_t(x) and f_t(x) + epsilon\n",
    "y_test_noise_var = (y_test_no_noise - y_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(10,10));\n",
    "\n",
    "models = ['Model A', 'Model B']\n",
    "\n",
    "for midx, model in enumerate(models):\n",
    "  p = plt.subplot(2, 2, midx+1);\n",
    "  for i in range(n_repeat):\n",
    "    p = sns.lineplot(x=x_test[:,0], y=y_predict[:, i, midx], alpha=0.05, color='orange');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_no_noise, color='black', label='True function');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_predict[:, :, midx].mean(axis=1), color='red', label='Mean of models');\n",
    "  p = sns.scatterplot(x=x_test[:,0], y=y_test, label='Test data');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('y');\n",
    "  p = plt.title(model);\n",
    "  p = plt.ylim(y_test.min()*1.1, y_test.max()*1.1)\n",
    " \n",
    "  p = plt.subplot(2, 2, midx+3);\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_err[:, :, midx].mean(axis=1), color='red', alpha=0.5, label='Total squared error', markers=True);\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_bias[:, midx], color='cyan', alpha=0.5, label='Bias^2');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_predict[:, :, midx].var(axis=1), color='green', alpha=0.5, label='Variance');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_noise_var, color='purple', alpha=0.2, label='Irreducibile error');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('Error');\n",
    "  p = plt.title(\"MSE: %f \\n Irreducible error: %f \\n Bias^2: %f \\n Variance: %f \" % \n",
    "                (y_test_err[:,:,midx].mean(axis=(0,1)), \n",
    "                 y_test_noise_var.mean(), \n",
    "                 (y_test_bias[:, midx]).mean(),\n",
    "                  y_predict[:, :, midx].var(axis=1).mean() ) )\n",
    "  p = plt.ylim(0, 1.1* y_test_err[:, :, :].mean(axis=(1)).max() )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the model trained with additional uninformative features\n",
    "has higher variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation: L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the simulation above change when we add an L1 penalty term? We\n",
    "will try it and see! Here, we compare:\n",
    "\n",
    "-   Model A, which is a least squares linear regression trained on one\n",
    "    informative feature and five uninformative features.\n",
    "-   Model B, which is a least squares linear regression trained on one\n",
    "    informative feature and five uninformative features, with an L1\n",
    "    penalty term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeat = 1000\n",
    "n_test = 50\n",
    "n_train = 50\n",
    "sigma = 1\n",
    "alpha = 0.1\n",
    "\n",
    "# generate test data once\n",
    "x_test, y_test = generate_linear_regression_data(n=n_test, d=6, coef=[2], intercept=1, sigma=sigma, uninformative=5)\n",
    "y_test_no_noise =  np.dot(x_test[:,0:1], [2]) + 1\n",
    "\n",
    "\n",
    "# prepare some matrices for storing simulation results\n",
    "y_predict = np.zeros((n_test, n_repeat, 2))\n",
    "\n",
    "y_test_err = np.zeros((n_test, n_repeat, 2))\n",
    "y_test_bias = np.zeros((n_test, 2))\n",
    "\n",
    "# now simulate training the model many times, on different training data every time\n",
    "# and evaluate using the test data\n",
    "for i in range(n_repeat):\n",
    "\n",
    "  # train both models on newly generated training data\n",
    "  x_train, y_train = generate_linear_regression_data(n=n_test, d=6, coef=[2], intercept=1, sigma=sigma, uninformative=5)\n",
    "\n",
    "  # model A: use linear regression on all columns\n",
    "  y_predict[:, i, 0] = LinearRegression().fit(x_train, y_train).predict(x_test)\n",
    "  # model B: use LASSO on all columns\n",
    "  y_predict[:, i, 1] = Lasso(alpha=alpha).fit(x_train, y_train).predict(x_test)\n",
    "\n",
    "  # overall squared error is due to difference between f_t(x) + epsilon, model prediction f(x) \n",
    "  y_test_err[:, i, 0] = (y_test - y_predict[:, i, 0])**2\n",
    "  y_test_err[:, i, 1] = (y_test - y_predict[:, i, 1])**2\n",
    "\n",
    "# bias is due to difference between f_t(x) and mean of model prediction f(x) \n",
    "y_test_bias[:, 0] = (y_test_no_noise - y_predict[:, :, 0].mean(axis=1))**2\n",
    "y_test_bias[:, 1] = (y_test_no_noise - y_predict[:, :, 1].mean(axis=1))**2\n",
    "\n",
    "# irreducible error is due to the difference between f_t(x) and f_t(x) + epsilon\n",
    "y_test_noise_var = (y_test_no_noise - y_test)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.figure(figsize=(10,10));\n",
    "\n",
    "models = ['Model A', 'Model B']\n",
    "\n",
    "for midx, model in enumerate(models):\n",
    "  p = plt.subplot(2, 2, midx+1);\n",
    "  for i in range(n_repeat):\n",
    "    p = sns.lineplot(x=x_test[:,0], y=y_predict[:, i, midx], alpha=0.05, color='orange');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_no_noise, color='black', label='True function');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_predict[:, :, midx].mean(axis=1), color='red', label='Mean of models');\n",
    "  p = sns.scatterplot(x=x_test[:,0], y=y_test, label='Test data');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('y');\n",
    "  p = plt.title(model);\n",
    "  p = plt.ylim(y_test.min()*1.1, y_test.max()*1.1)\n",
    " \n",
    "  p = plt.subplot(2, 2, midx+3);\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_err[:, :, midx].mean(axis=1), color='red', alpha=0.5, label='Total squared error', markers=True);\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_bias[:, midx], color='cyan', alpha=0.5, label='Bias^2');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_predict[:, :, midx].var(axis=1), color='green', alpha=0.5, label='Variance');\n",
    "  p = sns.lineplot(x=x_test[:,0], y=y_test_noise_var, color='purple', alpha=0.2, label='Irreducibile error');\n",
    "  p = plt.xlabel('x');\n",
    "  p = plt.ylabel('Error');\n",
    "  p = plt.title(\"MSE: %f \\n Irreducible error: %f \\n Bias^2: %f \\n Variance: %f \" % \n",
    "                (y_test_err[:,:,midx].mean(axis=(0,1)), \n",
    "                 y_test_noise_var.mean(), \n",
    "                 (y_test_bias[:, midx]).mean(),\n",
    "                  y_predict[:, :, midx].var(axis=1).mean() ) )\n",
    "  p = plt.ylim(0, 1.1* y_test_err[:, :, :].mean(axis=(1)).max() )\n",
    "\n",
    "plt.subplots_adjust(hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO reduces variance, but introduces some extra bias. Because the\n",
    "coefficient values are pulled away from their “true” values, towards\n",
    "zero, the mean prediction (red line) is different from the true function\n",
    "(black line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Ridge regression, we can use cross validation to select the\n",
    "value of $\\alpha$ that achieves the lowest MSE (the optimal\n",
    "bias-variance tradeoff) on a validation data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE contours with L1 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see why the L1 penalty “zeros” out some coefficients, we will look at\n",
    "the MSE contour plots of a couple of linear regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept=1\n",
    "coefs_a = [5,9]\n",
    "coefs_b = [2, -8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a, y_a = generate_linear_regression_data(n=1000, d=2, coef=coefs_a, intercept=intercept, sigma=2)\n",
    "x_b, y_b = generate_linear_regression_data(n=1000, d=2, coef=coefs_b, intercept=intercept, sigma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(-10, 10, 0.1)\n",
    "mses_a = np.zeros((len(coefs), len(coefs)))\n",
    "mses_b = np.zeros((len(coefs), len(coefs)))\n",
    "l1_penalty_coefs = np.zeros((len(coefs), len(coefs)))\n",
    "\n",
    "for idx_1, c_1 in enumerate(coefs):\n",
    "  for idx_2, c_2 in enumerate(coefs):\n",
    "    y_a_coef = (intercept + np.dot(x_a,[c_1, c_2])).squeeze()\n",
    "    mses_a[idx_1,idx_2] =  1.0/(len(y_a_coef)) * np.sum((y_a - y_a_coef)**2)\n",
    "    y_b_coef = (intercept + np.dot(x_b,[c_1, c_2])).squeeze()\n",
    "    mses_b[idx_1,idx_2] =  1.0/(len(y_b_coef)) * np.sum((y_b - y_b_coef)**2)\n",
    "    l1_penalty_coefs[idx_1,idx_2] =  np.linalg.norm( [c_1, c_2], ord=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6));\n",
    "\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_a, levels=50, cmap='Spectral');\n",
    "plt.scatter(coefs_a[1], coefs_a[0], marker='*', color='white', s=100)\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_b, levels=50, cmap='Spectral');\n",
    "plt.scatter(coefs_b[1], coefs_b[0], marker='*', color='white', s=100)\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can overlay the L1 penalty term on top of the MSE contours, to see\n",
    "the “pull” it exerts on the coefficient estimates.\n",
    "\n",
    "The L1 penalty term pulls the coefficient estimate toward the origin,\n",
    "like the L2 penalty term. However, the shape of the penalty is\n",
    "different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6));\n",
    "\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_a, levels=50, cmap='Spectral');\n",
    "p = plt.contour(X1, X2, l1_penalty_coefs, cmap='binary');\n",
    "plt.scatter(coefs_a[1], coefs_a[0], marker='*', color='white', s=100)\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_b, levels=50, cmap='Spectral');\n",
    "p = plt.contour(X1, X2, l1_penalty_coefs, cmap='binary');\n",
    "plt.scatter(coefs_b[1], coefs_b[0], marker='*', color='white', s=100)\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will see how the solution to the regularized loss function\n",
    "changes as $\\alpha$ varies.\n",
    "\n",
    "We will plot the contours of the regularized loss function along with\n",
    "the “path” of the optimization for different values of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = np.arange(0.001, 5, 0.05)\n",
    "n_alphas = len(alpha_list)\n",
    "coefs_lasso = np.zeros((n_alphas, 2, 2))\n",
    "cost_lasso  = np.zeros((n_alphas, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, alpha in enumerate(alpha_list):\n",
    "  regr_lasso_a = Lasso(alpha=alpha).fit(x_a, y_a)\n",
    "  coefs_lasso[idx,:,0] = regr_lasso_a.coef_\n",
    "  cost_lasso[idx,:,0]  = mean_squared_error(y_a, regr_lasso_a.predict(x_a))\n",
    "\n",
    "  regr_lasso_b = Lasso(alpha=alpha).fit(x_b, y_b)\n",
    "  coefs_lasso[idx,:,1] = regr_lasso_b.coef_\n",
    "  cost_lasso[idx,:,1]  = mean_squared_error(y_b, regr_lasso_b.predict(x_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6));\n",
    "\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_a, levels=50, cmap='Spectral');\n",
    "p = plt.contour(X1, X2, l1_penalty_coefs, cmap='binary');\n",
    "plt.plot(coefs_lasso[:,1,0], coefs_lasso[:,0,0], marker='.', color='black', markersize=10, alpha=0.2)\n",
    "plt.scatter(coefs_a[1], coefs_a[0], marker='*', color='white', s=100)\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_b, levels=50, cmap='Spectral');\n",
    "p = plt.contour(X1, X2, l1_penalty_coefs, cmap='binary');\n",
    "plt.plot(coefs_lasso[:,1,1], coefs_lasso[:,0,1], marker='.', color='black', markersize=10, alpha=0.2)\n",
    "plt.scatter(coefs_b[1], coefs_b[0], marker='*', color='white', s=100)\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how LASSO shrinks the coefficients, pulling them away from the\n",
    "“true” coefficients and toward the origin:\n",
    "\n",
    "-   First, LASS shrinks both coefficient estimates equally as $\\alpha$\n",
    "    increases.\n",
    "-   Eventually, one estimate - the estimate of the coefficient that has\n",
    "    the smallest magnitude - reaches 0. From then on, as we continue to\n",
    "    increase $\\alpha$, this coefficient will remain at 0, and only the\n",
    "    other coefficient will shrink.\n",
    "-   This process continues as we increase $\\alpha$ further, until all\n",
    "    coefficients reach 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this more clearly if we plot the regularization path on top\n",
    "of the LASSO loss function contour:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6));\n",
    "\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_a + 10*l1_penalty_coefs, levels=50, cmap='Spectral');\n",
    "plt.scatter(coefs_a[1], coefs_a[0], marker='*', color='white', s=100)\n",
    "plt.plot(coefs_lasso[:,1,0], coefs_lasso[:,0,0], marker='.', color='black', markersize=15, alpha=0.2)\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contourf(X1, X2, mses_b + 10*l1_penalty_coefs, levels=50, cmap='Spectral');\n",
    "plt.scatter(coefs_b[1], coefs_b[0], marker='*', color='white', s=100)\n",
    "plt.plot(coefs_lasso[:,1,1], coefs_lasso[:,0,1], marker='.', color='black', markersize=15, alpha=0.2)\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D visualization of L1 and L2 regularization {\\#3d-visualization-of-l1-and-l2-regularization}\n",
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_true = [2,-4]\n",
    "intercept = 0\n",
    "\n",
    "coefs = np.arange(-10, 10, 0.1)\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "mses = np.zeros((len(coefs), len(coefs)))\n",
    "penalty_coefs = np.zeros((len(coefs), len(coefs), 2))\n",
    "\n",
    "alpha_list = np.arange(0.001, 5, 0.05)\n",
    "n_alphas = len(alpha_list)\n",
    "\n",
    "x, y = generate_linear_regression_data(n=1000, d=2, \n",
    "                                coef=coefs_true, intercept=intercept, sigma=2)\n",
    "\n",
    "for idx_1, c_1 in enumerate(coefs):\n",
    "  for idx_2, c_2 in enumerate(coefs):\n",
    "    y_coef = (intercept + np.dot(x,[c_1, c_2])).squeeze()\n",
    "    mses[idx_1,idx_2] =  mean_squared_error(y, y_coef)\n",
    "    penalty_coefs[idx_1,idx_2,0] =  np.linalg.norm( [c_1, c_2], ord=1)\n",
    "    penalty_coefs[idx_1,idx_2,1] =  np.linalg.norm( [c_1, c_2], ord=2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev=20, azim=20, a=0.001, reg='L2',\n",
    "            show_cost = True, show_mse=False, show_pen=False,\n",
    "            X1=X1, X2=X2, mses=mses, penalty_coefs=penalty_coefs):\n",
    "  \n",
    "    if reg=='L1':\n",
    "      r_idx = 0\n",
    "    if reg=='L2':\n",
    "      r_idx = 1\n",
    "\n",
    "    alpha_list = np.arange(0.001, 5.05, 0.05)\n",
    "    a_idx = np.where(alpha_list>a)[0][0]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "\n",
    "    cost = mses+a*penalty_coefs[:,:,r_idx]\n",
    "\n",
    "    if show_cost:\n",
    "      ax.plot_surface(X1, X2, mses+a*penalty_coefs[:,:,r_idx], cmap='plasma',\n",
    "                    linewidth=0, antialiased=False, alpha=0.4);\n",
    "      # put star at minimum cost\n",
    "      idx_min = np.unravel_index(cost.argmin(), cost.shape)\n",
    "      ax.scatter3D(X1[idx_min], X2[idx_min], cost[idx_min]+5, \n",
    "                   marker='*', color='black', s=100)\n",
    "\n",
    "    if show_mse:\n",
    "      ax.plot_surface(X1, X2, mses, alpha=0.2, cmap='gist_gray',\n",
    "                          linewidth=0, antialiased=False)\n",
    "    if show_pen:\n",
    "      ax.plot_surface(X1, X2, a*penalty_coefs[:,:,r_idx], alpha=0.2, cmap='gist_gray',\n",
    "                    linewidth=0, antialiased=False);\n",
    "\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('w2')\n",
    "    ax.set_ylabel('w1')\n",
    "    ax.set_zlabel('Cost')\n",
    "    ax.set_zlim(0, 200)\n",
    "    ax.set_xlim(np.min(X2), np.max(X2))\n",
    "    ax.set_ylim(np.min(X1), np.max(X1))\n",
    "\n",
    "\n",
    "interact(plot_3D, elev=widgets.FloatSlider(min=-90,max=90,step=1, value=20), \n",
    "         azim=widgets.FloatSlider(min=-90,max=90,step=1, value=20), \n",
    "         a=widgets.FloatSlider(min=0.001, max=5, step=0.05, value=0.001, description='alpha'),\n",
    "         reg = widgets.RadioButtons(options=['L1', 'L2'], description='Penalty type'),\n",
    "         show_cost = widgets.Checkbox(value=True, description='Show cost surface'),\n",
    "         show_mse = widgets.Checkbox(value=False, description='Show MSE surface'),\n",
    "         show_pen = widgets.Checkbox(value=False, description='Show regularization penalty surface'),\n",
    "         X1=fixed(X1), X2=fixed(X2), \n",
    "         mses=fixed(mses), penalty_coefs=fixed(penalty_coefs));\n"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "colab": {
   "name": "3-regularization-deep-dive-with-3d.ipynb",
   "toc_visible": "true"
  }
 }
}
