{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis\n",
    "=========================\n",
    "\n",
    "*Fraida Fund*\n",
    "\n",
    "In this notebook\n",
    "----------------\n",
    "\n",
    "In this notebook:\n",
    "\n",
    "-   We practice using `pandas` to read in and manipulate a data set\n",
    "-   We learn a basic “recipe” for exploratory data analysis and apply it\n",
    "    to an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in applying machine learning to a real problem is\n",
    "*finding* or *creating* an appropriate data set with which to train your\n",
    "model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes data “good”?\n",
    "\n",
    "What makes a good data set?\n",
    "\n",
    "-   **Size**: the more *samples* are in the data set, the more examples\n",
    "    your machine learning model will be able to learn from. Often, a\n",
    "    simple machine learning model trained on a large data set will\n",
    "    outperform a “fancy” models on a small data set.\n",
    "-   **Quality**: Are there *predictive* features in the data? Are no\n",
    "    values (or very few values) missing, noisy, or incorrect? Is the\n",
    "    scenario in which the data collected similar to the scenario in\n",
    "    which your model will be used? These are some examples that we might\n",
    "    ask to evaluate the quality of a data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of exploratory data analysis\n",
    "\n",
    "Once we have identified one or more candidate data sets for a particular\n",
    "problem, we perform some *exploratory data analysis*. This process helps\n",
    "us\n",
    "\n",
    "-   detect and possibly correct mistakes in the data\n",
    "-   check our assumptions about the data\n",
    "-   determine relationships between features\n",
    "-   assess the direction and rough size of relationships between\n",
    "    features and the target variable\n",
    "\n",
    "Exploratory data analysis is important for understanding whether this\n",
    "data set is appropriate for the machine learning task at hand, and if\n",
    "any extra cleaning or processing steps are required before we use the\n",
    "data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Recipe” for exploratory data analysis\n",
    "--------------------------------------\n",
    "\n",
    "We will practice using a basic “recipe” for exploratory data analysis.\n",
    "\n",
    "1.  Learn about your data\n",
    "2.  Load data and check that it is loaded correctly\n",
    "3.  Visually inspect the data\n",
    "4.  Compute summary statistics\n",
    "5.  Explore the data further and look for potential issues\n",
    "\n",
    "Every exploratory data analysis is different, as specific\n",
    "characteristics of the data may lead you to explore different things in\n",
    "depth. However, this “recipe” can be a helpful starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Brooklyn Bridge pedestrian data set\n",
    "--------------------------------------------\n",
    "\n",
    "Support you are developing a machine learning model to predict the\n",
    "volume of pedestrian traffic on the Brooklyn Bridge. There is a dataset\n",
    "available that you think may be useful as training data: [Brooklyn\n",
    "Bridge Automated Pedestrian Counts\n",
    "dataset](https://www1.nyc.gov/html/dot/html/about/datafeeds.shtml#Pedestrians),\n",
    "from the NYC Department of Transportation.\n",
    "\n",
    "We will practice applying the “recipe” for exploratory data analysis to\n",
    "this data.\n",
    "\n",
    "We will use the `pandas` library in Python, which includes many powerful\n",
    "utilities for managing data. You can refer to the [`pandas`\n",
    "reference](https://pandas.pydata.org/pandas-docs/stable/reference/index.html)\n",
    "for more details on the `pandas` functions used in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn about your data\n",
    "\n",
    "The first step is to learn more about the data:\n",
    "\n",
    "-   Read about *methodology* and *data codebook*\n",
    "-   How many rows and columns are in the data?\n",
    "-   What does each variable mean? What units are data recorded in?\n",
    "-   How was data collected? Identify sampling issues, timeliness issues,\n",
    "    fairness issues, etc.\n",
    "\n",
    "For the Brooklyn Bridge dataset, you can review the associated\n",
    "documentation on the NYC Data website:\n",
    "\n",
    "-   [NYC Data\n",
    "    Website](https://data.cityofnewyork.us/Transportation/Brooklyn-Bridge-Automated-Pedestrian-Counts-Demons/6fi9-q3ta)\n",
    "-   [Data\n",
    "    dictionary](https://data.cityofnewyork.us/api/views/6fi9-q3ta/files/845905ea-21d4-4ec7-958a-a1a09214513d?download=true&filename=Brooklyn_Bridge_Automated_Pedestrian_Counts_Demonstration_Project_data_dictionary.xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and check that it is loaded directly\n",
    "\n",
    "The next step is to load the data in preparation for your exploratory\n",
    "data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import some useful libraries:\n",
    "\n",
    "-   In Python - libraries add powerful functionality\n",
    "-   You can import an entire library (`import foo`) or part\n",
    "    (`from foo import bar`)\n",
    "-   You can define a nickname, which you will use to call functions of\n",
    "    these libraries (many libraries have “conventional” nicknames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also set up notebook to:\n",
    "\n",
    "-   show plots “inline” - don’t make me explicitly ask for them to be\n",
    "    shown\n",
    "-   show output of all commands in a cell, not just the last one\n",
    "\n",
    "(depending on whether you are running this on Colab or in your own\n",
    "Jupyter notebook installation, which have different default settings,\n",
    "these steps may not be strictly necessary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set everything up, we are ready to read in our data!\n",
    "\n",
    "In most cases, we will use the `read_csv` function in `pandas` to read\n",
    "in our data. Function documentation: [pandas\n",
    "reference](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)\n",
    "\n",
    "``` python\n",
    "pandas.read_csv(filepath_or_buffer, \n",
    "    sep=',', header='infer', \n",
    "    names=None,\n",
    "    ...)\n",
    "```\n",
    "\n",
    "`read_csv` is for “flat” files. Other pandas functions exist for loading\n",
    "other kinds of data (read from database, Excel file, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data.cityofnewyork.us/api/views/6fi9-q3ta/rows.csv?accessType=DOWNLOAD'\n",
    "df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to verify that the data was loaded correctly. For *tabular*\n",
    "data, we can start by looking at a few rows of data with the `head`\n",
    "function. (For data that is not tabular, such as image, text, or audio\n",
    "data, we might start by looking at a few random samples instead.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to look for in the output above, that is easily missed: verify\n",
    "that column names and row names are loaded correctly, and that the first\n",
    "row of real data is actually data, and not column labels.\n",
    "\n",
    "We should also check the shape of the data frame - the number of rows\n",
    "and columns. This, too, should be checked against our assumptions about\n",
    "the data from the NYC Data website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the names of the columns and their data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a quick summary with `info()`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` infers the data type of each column automatically from the\n",
    "contents of the data.\n",
    "\n",
    "If the data type of a column is not what you expect it to be, this can\n",
    "often be a signal that the data needs cleaning. For example, if you\n",
    "expect a column to be numeric and it is read in as non-numeric, this\n",
    "indicates that there are probably some samples that include a\n",
    "non-numeric value in that column. (The [NYC Data\n",
    "website](https://data.cityofnewyork.us/Transportation/Brooklyn-Bridge-Automated-Pedestrian-Counts-Demons/6fi9-q3ta)\n",
    "indicates what type of data *should* be in each column, so you should\n",
    "reference that when checking this output. )\n",
    "\n",
    "We have a date/time column that was read in as a string, so we can\n",
    "correct that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour_beginning'] = pd.to_datetime(df['hour_beginning'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once we have done that, we can order the data frame by time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by='hour_beginning')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the `hour_beginning` variable includes the full date\n",
    "and time in one field. For our analysis, it would be more useful to have\n",
    "separate fields for the date, month, day of the week, and hour.\n",
    "\n",
    "We can create these additional fields by assigning the desired value to\n",
    "them directly - then, observe the effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hour'] = df['hour_beginning'].dt.hour\n",
    "df['month'] = df['hour_beginning'].dt.month\n",
    "df['date'] = df['hour_beginning'].dt.date\n",
    "df['day_name'] = df['hour_beginning'].dt.day_name()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data that is recorded at regular time intervals, it is also\n",
    "important to know whether the data is complete, or whether there are\n",
    "gaps in time. We will use some helpful `pandas` functions:\n",
    "\n",
    "-   [`pd.to_datetime`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html)\n",
    "-   [`pd.date_range`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html)\n",
    "\n",
    "First, we will use `date_range` to get the list of hour intervals that\n",
    "we expect to find in the dataset. Then, we will find the difference\n",
    "between this list and the actual list of hour intervals in the dataset -\n",
    "these are missing intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get beginning and end of date range\n",
    "min_dt = df.hour_beginning.min()\n",
    "max_dt = df.hour_beginning.max()\n",
    "print(min_dt)\n",
    "print(max_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then identify the missing hours\n",
    "expected_range = pd.date_range(start = min_dt, end = max_dt, freq='H' )\n",
    "missing_hours = expected_range.difference(df['hour_beginning'])\n",
    "print(missing_hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had the expected number of rows (the output of `shape` matched the\n",
    "description of the data on the NYC Data website), but the data seems to\n",
    "be missing samples from August 2018 through December 2018, which is\n",
    "worth keeping in mind if we decide to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(missing_hours.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also a good time to look for rows that are missing data in some\n",
    "columns (“NA” values), that may need to be cleaned.\n",
    "\n",
    "We can see the number of NAs in each column by summing up all the\n",
    "instances where the `isnull` function returns a True value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some rows of data that are missing weather, temperature, and\n",
    "precipitation data. We can see these rows with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['temperature'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas includes routines to fill in missing data using the `fillna`\n",
    "function\n",
    "([reference](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)).\n",
    "We will fill these using the “forward fill” method, which caries the\n",
    "last valid observation forward to fill in NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['temperature'] = df['temperature'].fillna(method=\"ffill\")\n",
    "df['precipitation'] = df['precipitation'].fillna(method=\"ffill\")\n",
    "df['weather_summary'] = df['weather_summary'].fillna(method=\"ffill\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can count the NAs again and find that there are only missing\n",
    "values in the `events` column. This is the expected result, since there\n",
    "are many days with no event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually inspect data\n",
    "\n",
    "Now we are ready to visually inspect the data.\n",
    "\n",
    "For tabular data, and especially tabular data with many numeric\n",
    "features, it is often useful to create a *pairplot*. A pairplot shows\n",
    "pairwise relationships between all numerical variables. It is a useful\n",
    "way to identify:\n",
    "\n",
    "-   features that are predictive - if there is any noticeable\n",
    "    relationship between the target variable and any other variable.\n",
    "-   features that are correlated - if two features are highly\n",
    "    correlated, we may be able to achieve equally good results just\n",
    "    using one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a “default” pairplot with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, each pane shows one numerical variable on the x-axis and another\n",
    "numerical variable on the y-axis, so that we can see if a relationship\n",
    "exists between them. The panes along the diagonal shows the empirical\n",
    "distribution of values for each feature in this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, it is difficult to see anything useful because there is so much\n",
    "going on in this plot. We can improve things somewhat by:\n",
    "\n",
    "-   specifying only the variables we want to include, and exluding\n",
    "    variables that don’t contain useful information, such as `lat` and\n",
    "    `long`, and\n",
    "-   making the points on the plot smaller and partially transparent, to\n",
    "    help with the overplotting.\n",
    "\n",
    "We’ll also change the histograms on the diagonal, which show the\n",
    "frequency of values for each variable, into a density plot which shows\n",
    "the same information in a more useful format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, \n",
    "             vars=['Pedestrians', 'temperature', 'precipitation', 'hour', 'month'],\n",
    "             diag_kind = 'kde',\n",
    "             plot_kws={'alpha':0.5, 'size': 0.1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are mainly interested in the top row of the plot, which shows how the\n",
    "target variable (`Pedestrians`) varies with the temperature,\n",
    "precipitation levels, and hour. However, it is also useful to note\n",
    "relationships between features. For example, there is a natural\n",
    "relationship between the time of data and the temperature, and between\n",
    "the month and the temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics\n",
    "\n",
    "Now, we are ready to explore summary statistics. The “five number\n",
    "summary” - extremes (min and max), median, and quartiles -can help us\n",
    "gain a better understanding of the data. We can use the `describe`\n",
    "function in `pandas` to compute this summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are especially interested in `Pedestrians`, the target variable, so\n",
    "we can describe that one separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Pedestrians'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical variables, we can use `groupby` to get frequency and\n",
    "other useful summary statistics.\n",
    "\n",
    "For example, we may be interested in the summary statistics for\n",
    "`Pedestrians` for different weather conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('weather_summary')['Pedestrians'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make special note of the `count` column, which shows us the prevalence\n",
    "of different weather conditions in this dataset. There are some weather\n",
    "conditions for which we have very few examples.\n",
    "\n",
    "Another categorical variable is `events`, which indicates whether the\n",
    "day is a holiday, and which holiday. Holidays have very different\n",
    "pedestrian traffic characteristics from other days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('events')['Pedestrians'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to get the total pedestrian count for the day of a\n",
    "holiday, rather than the summary statistics for the hour-long intervals.\n",
    "We can use the `agg` function to compute key statistics, including\n",
    "summing over all the samples in the group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('events').agg({'Pedestrians': 'sum'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore relationships and look for issues\n",
    "\n",
    "Finally, let’s further explore relationships between likely predictors\n",
    "and our target variable. We can group by `day_name`, then call the\n",
    "`describe` function on the `Pedestrians` column to see the effect of day\n",
    "of the week on traffic volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('day_name')['Pedestrians'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can see the effect of temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('temperature')['Pedestrians'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the effect of precipitation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('precipitation')['Pedestrians'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even plot it separately, by saving it in a new data frame and\n",
    "plotting *that* data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_precip = df.groupby('precipitation')['Pedestrians'].describe()\n",
    "df_precip = df_precip.reset_index()\n",
    "sns.scatterplot(data=df_precip, x='precipitation', y='50%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that certain weather conditions (very high temperature, heavy\n",
    "precipitation, fog) are extremely underrepresented in the dataset. This\n",
    "would be something to consider if, for example, we wanted to use this\n",
    "dataset to predict the effect of extreme weather on pedestrian traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for the live session\n",
    "----------------------------\n",
    "\n",
    "In our live recitation section this week, we will review the Brooklyn\n",
    "Bridge pedestrian example. Then, we’ll practice applying our “recipe”\n",
    "for exploratory data analysis to a few other tabular, text, and image\n",
    "data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for the live meeting, consider the following machine\n",
    "learning tasks, and candidate data sets. Try applying the first step of\n",
    "our “recipe” - learn about the data - to these examples. Do these data\n",
    "sets seem appropriate for the task? Without an in-depth analysis, can\n",
    "you identify any important limitations of the datasets, or problems that\n",
    "need addressing before we use them to train a machine learning model?\n",
    "\n",
    "#### Satirical headline classification:\n",
    "\n",
    "You are hired by a major social media platform to develop a machine\n",
    "learning model that will be used to clearly mark *satirical news\n",
    "articles* when they are shared on social media. You consider using this\n",
    "dataset of 9,000 headlines from [The Onion](https://www.theonion.com/)\n",
    "and 15,000 headlines from [Not The Onion on\n",
    "Reddit](https://www.reddit.com/r/nottheonion/). [Link to OnionOrNot\n",
    "data](https://github.com/lukefeilberg/onion)\n",
    "\n",
    "#### Chest X-ray classification:\n",
    "\n",
    "You are working for a large hospital system to develop a machine\n",
    "learning model that, given a chest X-ray, should identify those that\n",
    "likely have COVID-19 so that they can take proper precautions against\n",
    "the spread of infection within the hospital. You consider using a two\n",
    "datasets together: one with several hundred images of chest X-rays of\n",
    "likely COVID-19 patients, and a pre-COVID dataset of chest X-ray images.\n",
    "[Link to COVID-19 chest X-ray\n",
    "data](https://github.com/ieee8023/covid-chestxray-dataset), [Link to\n",
    "pre-COVID chest X-ray\n",
    "data](https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/overview)\n",
    "\n",
    "#### Taxi tip prediction:\n",
    "\n",
    "You are developing an app for NYC taxi drivers that will predict what\n",
    "the typical tip would be for a given fare. You consider using data\n",
    "collected by the NYC Taxi and Limousine Commission on taxi trips. The\n",
    "links are for 2019 data, but previous years are also available. [Data\n",
    "link for yellow (Manhattan) taxi\n",
    "trips](https://data.cityofnewyork.us/Transportation/2019-Yellow-Taxi-Trip-Data/2upf-qytp)\n",
    "and [data link for green (non-Manhattan) taxi\n",
    "trips](https://data.cityofnewyork.us/Transportation/2019-Green-Taxi-Trip-Data/q5mz-t52e)\n",
    "\n",
    "#### Highway traffic prediction:\n",
    "\n",
    "You are working for the state of New York to develop a traffic\n",
    "prediction model for the NYS Thruway. The following Thruway data is\n",
    "available: Number and types of vehicles that entered from each entry\n",
    "point on the Thruway, along with their exit points, at 15 minute\n",
    "intervals. The link points to the most recent week’s worth of available\n",
    "data, but this data is available through 2014. [Link to NYS Thruway\n",
    "data](https://data.ny.gov/Transportation/NYS-Thruway-Origin-and-Destination-Points-for-All-/4dbf-24u2)\n",
    "\n",
    "#### Offensive post classification:\n",
    "\n",
    "The social media platform was so impressed with your work on detection\n",
    "of satirical headlines, that they asked you to work on a model to\n",
    "identify posts using offensive language. As training data, they hand you\n",
    "80,000 tweets, labeled as either “hateful”, “abusive”, “spam”, or\n",
    "“none”, by majority vote of five people. [Link to abusive tweets\n",
    "data](https://dataverse.mpi-sws.org/dataset.xhtml?persistentId=doi:10.5072/FK2/ZDTEMN)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
