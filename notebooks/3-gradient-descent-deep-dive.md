::: {.cell .markdown}

# Gradient descent

_Fraida Fund_

:::

::: {.cell .code}
```python
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
sns.set()


%matplotlib inline
from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"
```
:::

::: {.cell .markdown}

## Data generated by a linear function

:::

::: {.cell .code}
```python
def generate_linear_regression_data(n=100, d=1, coef=[5], intercept=1, sigma=0):
  x = np.random.randn(n,d)
  y = (np.dot(x, coef) + intercept).squeeze() + sigma * np.random.randn(n)
  return x, y
```
:::

::: {.cell .markdown}

## Gradient descent for simple linear regression

:::

::: {.cell .markdown}

### Generate data

:::

::: {.cell .code}
```python
w_true = np.array([2, 3])
# y = 2 + 3x
```
:::

::: {.cell .code}
```python
intercept = w_true[0]
coef = w_true[1:]
print(intercept, coef)
```
:::

::: {.cell .code}
```python
n_samples = 100
```
:::

::: {.cell .code}
```python
x, y = generate_linear_regression_data(n=n_samples, d=1, coef=coef, intercept=intercept)
```
:::

::: {.cell .code}
```python
x.shape
```
:::

::: {.cell .code}
```python
y.shape
```
:::

::: {.cell .code}
```python
X = np.hstack((np.ones((n_samples, 1)), x))
X.shape
```
:::

::: {.cell .code}
```python
print(X[:5])
```
:::

::: {.cell .markdown}
### Define a descent step
:::

::: {.cell .markdown}
In each gradient descent step, we will compute

\begin{aligned} w_{k+1} &= w_k + \alpha_k X^T (y - X w_k) \\ 
                        &= w_k + \alpha\_k \sum_{i=1}^n (y_k - \langle w_k,x_i \rangle) x_i
\end{aligned}

:::

::: {.cell .code}
```python
def gd_step(w, X, y, lr):
  # use current parameters to get y_hat
  y_hat = np.dot(X,w)
  # compute gradient for this y_hat
  grad = np.matmul(X.T, y_hat-y)
  # update weights
  w_new = w - lr*grad

  # we don't have to actually compute MSE
  # but I want to, for visualization 
  mse = 1.0/len(y)*np.sum(y_hat - y)**2

  return (w_new, mse, grad)
```
:::

::: {.cell .markdown}
### Perform gradient descent
:::

::: {.cell .code}
```python
itr = 50
lr = 0.001
w_init = np.random.randn(len(w_true))
print(w_init)
```
:::

::: {.cell .code}
```python
w_steps = np.zeros((itr, len(w_init)))
mse_steps = np.zeros(itr)
grad_steps = np.zeros((itr, len(w_init)))

w_star = w_init
for i in range(itr):
  w_star, mse, gradient = gd_step(w_star, X, y, lr)
  w_steps[i] = w_star
  mse_steps[i] = mse
  grad_steps[i] = gradient
```
:::

::: {.cell .code}
```python
print(w_star)
```
:::

::: {.cell .markdown}
### Visualize
:::

::: {.cell .code}
```python
colors = sns.color_palette("hls", len(w_true))

plt.figure(figsize=(18,5))

plt.subplot(1,3,1);

for n in range(len(w_true)):
  plt.axhline(y=w_true[n], linestyle='--', color=colors[n]);
  sns.lineplot(np.arange(itr), w_steps[:,n], color=colors[n]);

plt.xlabel("Iteration");
plt.ylabel("Coefficient Value");

plt.subplot(1,3, 2);
sns.lineplot(np.arange(itr), mse_steps);
#plt.yscale("log")
plt.xlabel("Iteration");
plt.ylabel("Mean Squared Error");


plt.subplot(1, 3, 3);
for n in range(len(coef)+1):
  sns.lineplot(np.arange(itr), grad_steps[:,n], color=colors[n]);
plt.xlabel("Iteration");
plt.ylabel("Gradient");
```
:::

::: {.cell .markdown}
### Other things to try

-   What happens if we increase the learning rate?
-   What happens if we decrease the learning rate?
:::

::: {.cell .markdown}

## Descent path

:::

::: {.cell .markdown}

### Generate data

We will revisit our multiple linear regression.

:::

::: {.cell .code}
```python
w_true = [2, 6, 5]
intercept = w_true[0]
coef = w_true[1:]
print(intercept, coef)
```
:::

::: {.cell .code}
```python
n_samples = 100
```
:::

::: {.cell .code}
```python
x, y = generate_linear_regression_data(n=n_samples, d=2, coef=coef, intercept=intercept)
```
:::

::: {.cell .markdown}
### MSE contour
:::

::: {.cell .code}
```python
coefs = np.arange(2, 8, 0.05)
mses_coefs = np.zeros((len(coefs), len(coefs)))

for idx_1, c_1 in enumerate(coefs):
  for idx_2, c_2 in enumerate(coefs):
    y_coef = (intercept + np.dot(x,[c_1, c_2])).squeeze()
    mses_coefs[idx_1,idx_2] =  1.0/(len(y_coef)) * np.sum((y - y_coef)**2)
```
:::

::: {.cell .code}
```python
plt.figure(figsize=(5,5));
X1, X2 = np.meshgrid(coefs, coefs)
p = plt.contour(X1, X2, mses_coefs, levels=5);
plt.clabel(p, inline=1, fontsize=10);
plt.xlabel('w2');
plt.ylabel('w1');
```
:::

::: {.cell .markdown}
### Perform gradient descent 

:::

::: {.cell .code}
```python
X = np.hstack((np.ones((n_samples, 1)), x))
X.shape
```
:::

::: {.cell .code}
```python
itr = 50
lr = 0.001
w_init = [intercept, 2, 8]
```
:::

::: {.cell .code}
```python
w_steps = np.zeros((itr, len(w_init)))
mse_steps = np.zeros(itr)
grad_steps = np.zeros((itr, len(w_init)))

w_star = w_init
for i in range(itr):
  w_star, mse, gradient = gd_step(w_star, X, y, lr)
  w_steps[i] = w_star
  mse_steps[i] = mse
  grad_steps[i] = gradient
```
:::

::: {.cell .markdown}
### Visualize 
:::

::: {.cell .code}
```python
colors = sns.color_palette("hls", len(w_true))

for n in range(len(w_true)):
  plt.axhline(y=w_true[n], linestyle='--', color=colors[n]);
  sns.lineplot(np.arange(itr), w_steps[:,n], color=colors[n]);

plt.xlabel("Iteration");
plt.ylabel("Coefficient Value");
```
:::

::: {.cell .code}
```python
plt.figure(figsize=(5,5));
X1, X2 = np.meshgrid(coefs, coefs);
p = plt.contour(X1, X2, mses_coefs, levels=5);
plt.clabel(p, inline=1, fontsize=10);
plt.xlabel('w2');
plt.ylabel('w1');
sns.lineplot(w_steps[:,2], w_steps[:,1], color='black', alpha=0.5);
sns.scatterplot(w_steps[:,2], w_steps[:,1], hue=np.arange(itr), edgecolor=None);
```
:::

::: {.cell .code}
```python
w_star
```
:::

::: {.cell .markdown }
### Other things to try

-   What happens if we generate noisy data?
:::

::: {.cell .markdown}
### Stochastic gradient descent

For stochastic gradient descent, we will compute the gradient and update the weights using one sample (or a subset of the samples) in each step.

**A note on sampling**: In practice, the samples are often sampled without replacement, but the statistical guarantee of convergence is for sampling with replacement. In this example, we sample with replacement. You can read more about different varieties of gradient descent and stochastic gradient descent in [How is stochastic gradient descent implemented in the context of machine learning and deep learning](https://sebastianraschka.com/faq/docs/sgd-methods.html).

:::

::: {.cell .markdown}
### Define a descent step
:::

::: {.cell .code}
```python
def sgd_step(w, X, y, lr, n):

  idx_sample = np.random.choice(X.shape[0], n, replace=True)

  X_sample = X[idx_sample, :]
  y_sample = y[idx_sample]

  # use current parameters to get y_hat
  y_hat = np.dot(X_sample,w)
  # compute gradient for this y_hat
  grad = np.matmul(X_sample.T, y_hat-y_sample)
  # update weights
  w_new = w - lr*grad

  return w_new
```
:::

::: {.cell .markdown}
### Perform gradient descent 
:::

::: {.cell .code}
```python
itr = 50
lr = 0.001
n = 1
w_init = [intercept, 2, 8]
```
:::

::: {.cell .code}
```python
w_steps = np.zeros((itr, len(w_init)))

w_star = w_init
for i in range(itr):
  w_star = sgd_step(w_star, X, y, lr, n)
  w_steps[i] = w_star
```
:::

::: {.cell .code}
```python
w_star
```
:::

::: {.cell .markdown}
### Visualize
:::

::: {.cell .code}
```python
colors = sns.color_palette("hls", len(coef) + 1)

plt.axhline(y=intercept, linestyle='--', color=colors[0]);
sns.lineplot(np.arange(itr), w_steps[:,0], color=colors[0]);

for n in range(len(coef)):
  plt.axhline(y=coef[n], linestyle='--', color=colors[n+1]);
  sns.lineplot(np.arange(itr), w_steps[:,n+1], color=colors[n+1]);

plt.xlabel("Iteration");
plt.ylabel("Coefficient Value");
```
:::

::: {.cell .code}
```python
plt.figure(figsize=(5,5));
X1, X2 = np.meshgrid(coefs, coefs);
p = plt.contour(X1, X2, mses_coefs, levels=5);
plt.clabel(p, inline=1, fontsize=10);
plt.xlabel('w2');
plt.ylabel('w1');
sns.lineplot(w_steps[:,2], w_steps[:,1], color='black', alpha=0.5);
sns.scatterplot(w_steps[:,2], w_steps[:,1], hue=np.arange(itr), edgecolor=None);
```
:::

::: {.cell .markdown}
### Other things to try 

-   Increase learning rate?
-   Decrease learning rate?
-   Use decaying learning rate $\alpha_k = \frac{C}{k}$?
-   Increase number of samples used in each iteration?
:::



::: {.cell .markdown}

## Gradient descent with noise

:::


### Generate data

This time, we will use the `sigma` argument in our `generate_linear_regression_data` function to generate data that does not 
perfectly fit a linear model. 

:::

::: {.cell .code}
```python
w_true = [2, 6, 5]
intercept = w_true[0]
coef = w_true[1:]
print(intercept, coef)
```
:::

::: {.cell .code}
```python
n_samples = 1000
```
:::

::: {.cell .code}
```python
x, y = generate_linear_regression_data(n=n_samples, d=2, coef=coef, intercept=intercept, sigma=1)
```
:::


::: {.cell .markdown}
### MSE contour
:::

::: {.cell .code}
```python
coefs = np.arange(2, 8, 0.05)
mses_coefs = np.zeros((len(coefs), len(coefs)))

for idx_1, c_1 in enumerate(coefs):
  for idx_2, c_2 in enumerate(coefs):
    y_coef = (intercept + np.dot(x,[c_1, c_2])).squeeze()
    mses_coefs[idx_1,idx_2] =  1.0/(len(y_coef)) * np.sum((y - y_coef)**2)
```
:::

::: {.cell .code}
```python
plt.figure(figsize=(5,5));
X1, X2 = np.meshgrid(coefs, coefs)
p = plt.contour(X1, X2, mses_coefs, levels=5);
plt.clabel(p, inline=1, fontsize=10);
plt.xlabel('w2');
plt.ylabel('w1');
```
:::

::: {.cell .markdown}
### Perform gradient descent 

This time, the gradient descent may not necessarily arrive at the "true" coefficient values. That's not because it does not find the coefficients with minimum MSE; it's because the coefficients with minimum MSE on the noisy training data are not necessarily the "true" coefficients.

:::

::: {.cell .code}
```python
X = np.hstack((np.ones((n_samples, 1)), x))
X.shape
```
:::

::: {.cell .code}
```python
itr = 500
lr = 0.0002
w_init = [intercept, 2, 8]
```
:::

::: {.cell .code}
```python
w_steps = np.zeros((itr, len(w_init)))

w_star = w_init
for i in range(itr):
  w_star, mse, gradient = gd_step(w_star, X, y, lr)
  w_steps[i] = w_star
```
:::

::: {.cell .markdown}
### Visualize gradient descent
:::

::: {.cell .code}
```python
colors = sns.color_palette("hls", len(w_true))

for n in range(len(w_true)):
  plt.axhline(y=w_true[n], linestyle='--', color=colors[n]);
  sns.lineplot(np.arange(itr), w_steps[:,n], color=colors[n]);

plt.xlabel("Iteration");
plt.ylabel("Coefficient Value");
```
:::

::: {.cell .code}
```python
plt.figure(figsize=(5,5));
X1, X2 = np.meshgrid(coefs, coefs);
p = plt.contour(X1, X2, mses_coefs, levels=5);
plt.clabel(p, inline=1, fontsize=10);
plt.xlabel('w2');
plt.ylabel('w1');
sns.lineplot(w_steps[:,2], w_steps[:,1], color='black', alpha=0.5);
sns.scatterplot(w_steps[:,2], w_steps[:,1], hue=np.arange(itr), edgecolor=None);
```
:::

::: {.cell .code}
```python
w_star
```
:::



::: {.cell .markdown}
### Perform stochastic gradient descent 

With data that does not perfectly fit the linear model, the stochastic gradient descent converges to a "noise ball" around the optimal solution.

:::

::: {.cell .code}
```python
itr = 500
lr = 0.2
w_init = [intercept, 2, 8]
```
:::

::: {.cell .code}
```python
w_steps = np.zeros((itr, len(w_init)))

w_star = w_init
for i in range(itr):
  w_star = sgd_step(w_star, X, y, lr, n) 
  w_steps[i] = w_star
```
:::

::: {.cell .markdown}
### Visualize stochastic gradient descent
:::

::: {.cell .code}
```python
colors = sns.color_palette("hls", len(w_true))

for n in range(len(w_true)):
  plt.axhline(y=w_true[n], linestyle='--', color=colors[n]);
  sns.lineplot(np.arange(itr), w_steps[:,n], color=colors[n]);

plt.xlabel("Iteration");
plt.ylabel("Coefficient Value");
```
:::

::: {.cell .code}
```python
plt.figure(figsize=(5,5));
X1, X2 = np.meshgrid(coefs, coefs);
p = plt.contour(X1, X2, mses_coefs, levels=5);
plt.clabel(p, inline=1, fontsize=10);
plt.xlabel('w2');
plt.ylabel('w1');
sns.lineplot(w_steps[:,2], w_steps[:,1], color='black', alpha=0.5);
sns.scatterplot(w_steps[:,2], w_steps[:,1], hue=np.arange(itr), edgecolor=None);
```
:::

::: {.cell .code}
```python
w_star
```
:::
