{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generated by a linear function\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_regression_data(n=100, d=1, coef=[5], intercept=1, sigma=0):\n",
    "  x = np.random.randn(n,d)\n",
    "  y = (np.dot(x, coef) + intercept).squeeze() + sigma * np.random.randn(n)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent for simple linear regression\n",
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_true = np.array([2, 3])\n",
    "# y = 2 + 3x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = w_true[0]\n",
    "coef = w_true[1:]\n",
    "print(intercept, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_linear_regression_data(n=n_samples, d=1, coef=coef, intercept=intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((n_samples, 1)), x))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each gradient descent step, we will compute\n",
    "\n",
    "\\\\begin{aligned} w\\_{k+1} &= w\\_k + *k X^T (y - X w\\_k) \\\\ &= w\\_k + \\_k\n",
    "*{i=1}^n (y\\_k - w\\_k,x\\_i ) x\\_i \\\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_step(w, X, y, lr):\n",
    "  # use current parameters to get y_hat\n",
    "  y_hat = np.dot(X,w)\n",
    "  # compute gradient for this y_hat\n",
    "  grad = np.matmul(X.T, y_hat-y)\n",
    "  # update weights\n",
    "  w_new = w - lr*grad\n",
    "\n",
    "  # we don't have to actually compute MSE\n",
    "  # but I want to, for visualization \n",
    "  mse = 1.0/len(y)*np.sum(y_hat - y)**2\n",
    "\n",
    "  return (w_new, mse, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = 50\n",
    "lr = 0.001\n",
    "w_init = np.random.randn(len(w_true))\n",
    "print(w_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_steps = np.zeros((itr, len(w_init)))\n",
    "mse_steps = np.zeros(itr)\n",
    "grad_steps = np.zeros((itr, len(w_init)))\n",
    "\n",
    "w_star = w_init\n",
    "for i in range(itr):\n",
    "  w_star, mse, gradient = gd_step(w_star, X, y, lr)\n",
    "  w_steps[i] = w_star\n",
    "  mse_steps[i] = mse\n",
    "  grad_steps[i] = gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"hls\", len(w_true))\n",
    "\n",
    "plt.figure(figsize=(18,5))\n",
    "\n",
    "plt.subplot(1,3,1);\n",
    "\n",
    "for n in range(len(w_true)):\n",
    "  plt.axhline(y=w_true[n], linestyle='--', color=colors[n]);\n",
    "  sns.lineplot(np.arange(itr), w_steps[:,n], color=colors[n]);\n",
    "\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Coefficient Value\");\n",
    "\n",
    "plt.subplot(1,3, 2);\n",
    "sns.lineplot(np.arange(itr), mse_steps);\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Mean Squared Error\");\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 3);\n",
    "for n in range(len(coef)+1):\n",
    "  sns.lineplot(np.arange(itr), grad_steps[:,n], color=colors[n]);\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Gradient\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other things to try\n",
    "\n",
    "-   What happens if we increase the learning rate?\n",
    "-   What happens if we decrease the learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descent path\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data\n",
    "\n",
    "We will revisit our multiple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_true = [2, 6, 5]\n",
    "intercept = w_true[0]\n",
    "coef = w_true[1:]\n",
    "print(intercept, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_linear_regression_data(n=n_samples, d=2, coef=coef, intercept=intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(2, 8, 0.05)\n",
    "mses_coefs = np.zeros((len(coefs), len(coefs)))\n",
    "\n",
    "for idx_1, c_1 in enumerate(coefs):\n",
    "  for idx_2, c_2 in enumerate(coefs):\n",
    "    y_coef = (intercept + np.dot(x,[c_1, c_2])).squeeze()\n",
    "    mses_coefs[idx_1,idx_2] =  1.0/(len(y_coef)) * np.sum((y - y_coef)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contour(X1, X2, mses_coefs, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((np.ones((n_samples, 1)), x))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = 50\n",
    "lr = 0.001\n",
    "w_init = [intercept, 2, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_steps = np.zeros((itr, len(w_init)))\n",
    "mse_steps = np.zeros(itr)\n",
    "grad_steps = np.zeros((itr, len(w_init)))\n",
    "\n",
    "w_star = w_init\n",
    "for i in range(itr):\n",
    "  w_star, mse, gradient = gd_step(w_star, X, y, lr)\n",
    "  w_steps[i] = w_star\n",
    "  mse_steps[i] = mse\n",
    "  grad_steps[i] = gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"hls\", len(w_true))\n",
    "\n",
    "for n in range(len(w_true)):\n",
    "  plt.axhline(y=w_true[n], linestyle='--', color=colors[n]);\n",
    "  sns.lineplot(np.arange(itr), w_steps[:,n], color=colors[n]);\n",
    "\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Coefficient Value\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "X1, X2 = np.meshgrid(coefs, coefs);\n",
    "p = plt.contour(X1, X2, mses_coefs, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');\n",
    "sns.lineplot(w_steps[:,2], w_steps[:,1], color='black', alpha=0.5);\n",
    "sns.scatterplot(w_steps[:,2], w_steps[:,1], hue=np.arange(itr), edgecolor=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other things to try\n",
    "\n",
    "-   What happens if we generate noisy data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a descent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_step(w, X, y, lr, n):\n",
    "\n",
    "  idx_sample = np.random.choice(X.shape[0], n, replace=True)\n",
    "\n",
    "  X_sample = X[idx_sample, :]\n",
    "  y_sample = y[idx_sample]\n",
    "\n",
    "  # use current parameters to get y_hat\n",
    "  y_hat = np.dot(X_sample,w)\n",
    "  # compute gradient for this y_hat\n",
    "  grad = np.matmul(X_sample.T, y_hat-y_sample)\n",
    "  # update weights\n",
    "  w_new = w - lr*grad\n",
    "\n",
    "  return w_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = 50\n",
    "lr = 0.001\n",
    "n = 1\n",
    "w_init = [intercept, 2, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_steps = np.zeros((itr, len(w_init)))\n",
    "\n",
    "w_star = w_init\n",
    "for i in range(itr):\n",
    "  w_star = sgd_step(w_star, X, y, lr, n)\n",
    "  w_steps[i] = w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette(\"hls\", len(coef) + 1)\n",
    "\n",
    "plt.axhline(y=intercept, linestyle='--', color=colors[0]);\n",
    "sns.lineplot(np.arange(itr), w_steps[:,0], color=colors[0]);\n",
    "\n",
    "for n in range(len(coef)):\n",
    "  plt.axhline(y=coef[n], linestyle='--', color=colors[n+1]);\n",
    "  sns.lineplot(np.arange(itr), w_steps[:,n+1], color=colors[n+1]);\n",
    "\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Coefficient Value\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "X1, X2 = np.meshgrid(coefs, coefs);\n",
    "p = plt.contour(X1, X2, mses_coefs, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');\n",
    "sns.lineplot(w_steps[:,2], w_steps[:,1], color='black', alpha=0.5);\n",
    "sns.scatterplot(w_steps[:,2], w_steps[:,1], hue=np.arange(itr), edgecolor=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other things to try\n",
    "\n",
    "-   Increase learning rate?\n",
    "-   Decrease learning rate?\n",
    "-   Use decaying learning rate $\\alpha_k = \\frac{C}{k}$?\n",
    "-   Increase number of samples used in each iteration?"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
