{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent in depth\n",
    "=========================\n",
    "\n",
    "*Fraida Fund*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "colors = sns.color_palette(\"hls\", 4)\n",
    "\n",
    "# for 3d interactive plots\n",
    "from ipywidgets import interact, fixed, widgets\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent for simple linear regression\n",
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_regression_data(n=100, d=1, coef=[5], intercept=1, sigma=0):\n",
    "  x = np.random.randn(n,d)\n",
    "  y = (np.dot(x, coef) + intercept).squeeze() + sigma * np.random.randn(n)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = 2 + 3x\n",
    "w_true = np.array([2, 3])\n",
    "n_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_linear_regression_data(n=n_samples, d=1, coef=w_true[1:], intercept= w_true[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the \"design matrix\" with a ones column at beginning\n",
    "X = np.hstack((np.ones((n_samples, 1)), x))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each gradient descent step, we will compute\n",
    "\n",
    "$$ \n",
    "w^{t+1} = w^t - \\alpha^t \\nabla L(w^t)  \n",
    "$$\n",
    "\n",
    "With a mean squared error loss function\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "L(w) &= \\frac{1}{n} \\sum_{i=1}^n (y_i - \\langle w,x_i \\rangle)^2 \\\\\n",
    "     &= \\frac{1}{n} \\|y - Xw\\|^2 \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "we will compute the weights at each step as\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \n",
    "w^{t+1} &= w^t + \\frac{\\alpha^t}{n} \\sum_{i=1}^n (y_i - \\langle w^t,x_i \\rangle) x_i \\\\\n",
    "        &= w^t + \\frac{\\alpha^t}{n} X^T (y - X w^t)                  \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_step(w, X, y, lr):\n",
    "  # use current parameters to get y_hat\n",
    "  y_hat = np.dot(X,w)\n",
    "  error = y_hat-y\n",
    "  # compute gradient for this y_hat\n",
    "  grad = np.matmul(X.T, error)\n",
    "  # update weights\n",
    "  w_new = w - (lr/X.shape[0])*grad\n",
    "\n",
    "  # we don't have to actually compute MSE\n",
    "  # but I want to, for visualization \n",
    "  mse = np.mean(error**2, axis=0)\n",
    "\n",
    "  return (w_new, mse, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent settings: number of iterations, learning rate, starting point\n",
    "itr = 50\n",
    "lr = 0.1\n",
    "w_init = np.random.uniform(3,7,len(w_true))\n",
    "print(w_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_steps = np.zeros((itr, len(w_init)))\n",
    "mse_steps = np.zeros(itr)\n",
    "grad_steps = np.zeros((itr, len(w_init)))\n",
    "\n",
    "w_star = w_init\n",
    "for i in range(itr):\n",
    "  w_star, mse, gradient = gd_step(w_star, X, y, lr)\n",
    "  w_steps[i] = w_star\n",
    "  mse_steps[i] = mse\n",
    "  grad_steps[i] = gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "plt.subplot(1,3,1);\n",
    "\n",
    "for n, w in enumerate(w_true):\n",
    "  plt.axhline(y=w, linestyle='--', color=colors[n]);\n",
    "  sns.lineplot(x=np.arange(itr), y=w_steps[:,n], color=colors[n]);\n",
    "\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Coefficient Value\");\n",
    "\n",
    "plt.subplot(1,3, 2);\n",
    "sns.lineplot(x=np.arange(itr), y=mse_steps);\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Training MSE\");\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 3);\n",
    "for n, w in enumerate(w_true):\n",
    "  sns.lineplot(x=np.arange(itr), y=grad_steps[:,n], color=colors[n]);\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Gradient\");\n",
    "\n",
    "plt.suptitle(\"Estimate after %d iterations with rate %s: %s\" % \n",
    "          (itr, \"{0:0.4f}\".format(lr), [\"{0:0.4f}\".format(w) for w in w_star]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other things to try\n",
    "\n",
    "-   What happens if we increase the learning rate?\n",
    "-   What happens if we decrease the learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descent path on MSE contour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating data for a multiple regression (with two features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_true = [2, 6, 5]\n",
    "n_samples = 100\n",
    "x, y = generate_linear_regression_data(n=n_samples, d=2, coef=w_true[1:], intercept=w_true[0])\n",
    "X = np.hstack((np.ones((n_samples, 1)), x))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent settings: number of iterations, learning rate, starting point\n",
    "itr = 50\n",
    "lr = 0.1\n",
    "w_init = np.random.uniform(3, 7, len(w_true))\n",
    "print(w_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_steps = np.zeros((itr, len(w_init)))\n",
    "mse_steps = np.zeros(itr)\n",
    "grad_steps = np.zeros((itr, len(w_init)))\n",
    "\n",
    "w_star = w_init\n",
    "for i in range(itr):\n",
    "  w_star, mse, gradient = gd_step(w_star, X, y, lr)\n",
    "  w_steps[i] = w_star\n",
    "  mse_steps[i] = mse\n",
    "  grad_steps[i] = gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(2, 8, 0.05)\n",
    "\n",
    "coef_grid = np.array(np.meshgrid(coefs, coefs)).reshape(1, 2, coefs.shape[0], coefs.shape[0])\n",
    "y_hat_c = (w_true[0] + np.sum(coef_grid * x.reshape(x.shape[0], 2, 1, 1), axis=1) )\n",
    "mses_coefs = np.mean((y.reshape(-1, 1, 1)- y_hat_c)**2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "X1, X2 = np.meshgrid(coefs, coefs);\n",
    "p = plt.contour(X1, X2, mses_coefs, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');\n",
    "sns.lineplot(x=w_steps[:,1], y=w_steps[:,2], color='black', sort=False, alpha=0.5);\n",
    "sns.scatterplot(x=w_steps[:,1], y=w_steps[:,2], hue=np.arange(itr), edgecolor=None);\n",
    "plt.scatter(w_true[1], w_true[2], c='black', marker='*');\n",
    "plt.title(\"Estimate after %d iterations with rate %s: %s\" % (itr, \"{0:0.4f}\".format(lr), [\"{0:0.4f}\".format(w) for w in w_star]));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev=20, azim=-20, X1=X1, X2=X2, mses_coefs=mses_coefs, \n",
    "            w_steps=w_steps, mse_steps=mse_steps):\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "\n",
    "    # Plot the surface.\n",
    "    ax.plot_surface(X1, X2, mses_coefs, alpha=0.5, cmap=cm.coolwarm,\n",
    "                          linewidth=0, antialiased=False)\n",
    "    ax.scatter3D(w_steps[:, 1], w_steps[:, 2], mse_steps, s=5, color='black')\n",
    "    ax.plot(w_steps[:, 1], w_steps[:, 2], mse_steps, color='gray')\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('w1')\n",
    "    ax.set_ylabel('w2')\n",
    "    ax.set_zlabel('MSE')\n",
    "\n",
    "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20), \n",
    "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
    "         X1=fixed(X1), X2=fixed(X2), mses_coefs=fixed(mses_coefs),\n",
    "         w_steps=fixed(w_steps), mse_steps=fixed(mse_steps));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent\n",
    "---------------------------\n",
    "\n",
    "For stochastic gradient descent, we will compute the gradient and update\n",
    "the weights using one sample (or a mini-batch of samples) in each step.\n",
    "\n",
    "**A note on sampling**: In practice, the samples are often sampled\n",
    "without replacement, but the statistical guarantee of convergence is for\n",
    "sampling with replacement. In this example, we sample with replacement.\n",
    "You can read more about different varieties of gradient descent and\n",
    "stochastic gradient descent in [How is stochastic gradient descent\n",
    "implemented in the context of machine learning and deep\n",
    "learning](https://sebastianraschka.com/faq/docs/sgd-methods.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a stochastic descent step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_step(w, X, y, lr, n):\n",
    "\n",
    "  idx_sample = np.random.choice(X.shape[0], n, replace=True)\n",
    "\n",
    "  X_sample = X[idx_sample, :]\n",
    "  y_sample = y[idx_sample]\n",
    "\n",
    "  # use current parameters to get y_hat\n",
    "  y_hat = np.dot(X_sample,w)\n",
    "  error = y_hat-y_sample\n",
    "  # compute gradient for this y_hat\n",
    "  grad = np.matmul(X_sample.T, error)\n",
    "  # update weights\n",
    "  w_new = w - (lr/n)*grad\n",
    "\n",
    "  # we don't have to actually compute MSE\n",
    "  # but I want to, for visualization \n",
    "  # note: MSE is computed on entire data, not sample\n",
    "  mse = np.mean((y-np.dot(X, w))**2, axis=0)\n",
    "\n",
    "  return (w_new, mse, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have another gradient descent option: how many samples in a \"batch\" \n",
    "itr = 50\n",
    "lr = 0.1\n",
    "n_batch = 1\n",
    "w_init = [w_true[0], 2, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_steps = np.zeros((itr, len(w_init)))\n",
    "mse_steps = np.zeros(itr)\n",
    "\n",
    "w_star = w_init\n",
    "for i in range(itr):\n",
    "  w_star, mse, grad = sgd_step(w_star, X, y, lr, n_batch)\n",
    "  w_steps[i] = w_star\n",
    "  mse_steps[i] = mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "plt.subplot(1,3,1);\n",
    "\n",
    "for n, w in enumerate(w_true):\n",
    "  plt.axhline(y=w, linestyle='--', color=colors[n]);\n",
    "  sns.lineplot(x=np.arange(itr), y=w_steps[:,n], color=colors[n], label='w' + str(n));\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Coefficient Value\");\n",
    "\n",
    "plt.subplot(1, 3, 2);\n",
    "sns.lineplot(x=np.arange(itr), y=mse_steps);\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Training MSE\");\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 3);\n",
    "X1, X2 = np.meshgrid(coefs, coefs);\n",
    "p = plt.contour(X1, X2, mses_coefs, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');\n",
    "sns.lineplot(x=w_steps[:,1], y=w_steps[:,2], color='black', sort=False, alpha=0.5);\n",
    "sns.scatterplot(x=w_steps[:,1], y=w_steps[:,2], hue=np.arange(itr), edgecolor=None);\n",
    "plt.scatter(w_true[1], w_true[2], c='black', marker='*');\n",
    "\n",
    "plt.suptitle(\"Estimate after %d iterations with rate %s and batch size %d: %s\" % \n",
    "            (itr, \"{0:0.4f}\".format(lr), n_batch, [\"{0:0.4f}\".format(w) for w in w_star]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev=20, azim=-20, X1=X1, X2=X2, mses_coefs=mses_coefs, \n",
    "            w_steps=w_steps, mse_steps=mse_steps):\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "\n",
    "\n",
    "    # Plot the surface.\n",
    "    ax.plot_surface(X1, X2, mses_coefs, alpha=0.5, cmap=cm.coolwarm,\n",
    "                          linewidth=0, antialiased=False)\n",
    "    ax.scatter3D(w_steps[:, 1], w_steps[:, 2], mse_steps, s=5, color='black')\n",
    "    ax.plot(w_steps[:, 1], w_steps[:, 2], mse_steps, color='gray')\n",
    "\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('w1')\n",
    "    ax.set_ylabel('w2')\n",
    "    ax.set_zlabel('MSE')\n",
    "\n",
    "interact(plot_3D, elev=widgets.IntSlider(min=-90, max=90, step=10, value=20), \n",
    "          azim=widgets.IntSlider(min=-90, max=90, step=10, value=20),\n",
    "         X1=fixed(X1), X2=fixed(X2), mses_coefs=fixed(mses_coefs),\n",
    "         w_steps=fixed(w_steps), mse_steps=fixed(mse_steps));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other things to try\n",
    "\n",
    "-   Increase number of samples used in each iteration?\n",
    "-   Increase learning rate?\n",
    "-   Decrease learning rate?\n",
    "-   Use decaying learning rate $\\alpha^t = \\frac{\\alpha_0}{1 + kt}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent with noise\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate noisy data\n",
    "\n",
    "This time, we will use the `sigma` argument in our\n",
    "`generate_linear_regression_data` function to generate data that does\n",
    "not perfectly fit a linear model. (Using the same coefficients as the\n",
    "previous example.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_linear_regression_data(n=n_samples, d=2, coef=w_true[1:], intercept=w_true[0], sigma=3)\n",
    "X = np.column_stack((np.ones((n_samples, 1)), x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform gradient descent on noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = 50\n",
    "lr = 0.1\n",
    "w_init = [w_true[0], 2, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_steps = np.zeros((itr, len(w_init)))\n",
    "mse_steps = np.zeros(itr)\n",
    "\n",
    "w_star = w_init\n",
    "for i in range(itr):\n",
    "  w_star, mse, gradient = gd_step(w_star, X, y, lr)\n",
    "  w_steps[i] = w_star\n",
    "  mse_steps[i] = mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize gradient descent on noisy data\n",
    "\n",
    "This time, the gradient descent may not necessarily arrive at the “true”\n",
    "coefficient values. That’s not because it does not find the coefficients\n",
    "with minimum MSE; it’s because the coefficients with minimum MSE on the\n",
    "noisy training data are not necessarily the “true” coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "plt.subplot(1,3,1);\n",
    "\n",
    "for n, w in enumerate(w_true):\n",
    "  plt.axhline(y=w, linestyle='--', color=colors[n]);\n",
    "  sns.lineplot(x=np.arange(itr), y=w_steps[:,n], color=colors[n], label='w' + str(n));\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Coefficient Value\");\n",
    "\n",
    "plt.subplot(1, 3, 2);\n",
    "sns.lineplot(x=np.arange(itr), y=mse_steps);\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Training MSE\");\n",
    "\n",
    "plt.subplot(1, 3, 3);\n",
    "X1, X2 = np.meshgrid(coefs, coefs);\n",
    "p = plt.contour(X1, X2, mses_coefs, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');\n",
    "sns.lineplot(x=w_steps[:,1], y=w_steps[:,2], color='black', sort=False, alpha=0.5);\n",
    "sns.scatterplot(x=w_steps[:,1], y=w_steps[:,2], hue=np.arange(itr), edgecolor=None);\n",
    "plt.scatter(w_true[1], w_true[2], c='black', marker='*');\n",
    "\n",
    "plt.suptitle(\"Estimate after %d iterations with rate %s: %s\" % \n",
    "          (itr, \"{0:0.4f}\".format(lr), [\"{0:0.4f}\".format(w) for w in w_star]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform stochastic gradient descent on noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = 100\n",
    "lr = 0.1\n",
    "n_batch = 1\n",
    "w_init = [w_true[0], 2, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_steps = np.zeros((itr, len(w_init)))\n",
    "mse_steps = np.zeros(itr)\n",
    "\n",
    "w_star = w_init\n",
    "for i in range(itr):\n",
    "  w_star, mse, grad = sgd_step(w_star, X, y, lr, n_batch) \n",
    "  w_steps[i] = w_star\n",
    "  mse_steps[i] = mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize stochastic gradient descent\n",
    "\n",
    "Note the “noise ball”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "plt.subplot(1,3,1);\n",
    "\n",
    "for n, w in enumerate(w_true):\n",
    "  plt.axhline(y=w, linestyle='--', color=colors[n]);\n",
    "  sns.lineplot(x=np.arange(itr), y=w_steps[:,n], color=colors[n], label='w' + str(n));\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Coefficient Value\");\n",
    "\n",
    "plt.subplot(1, 3, 2);\n",
    "sns.lineplot(x=np.arange(itr), y=mse_steps);\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Training MSE\");\n",
    "\n",
    "plt.subplot(1, 3, 3);\n",
    "X1, X2 = np.meshgrid(coefs, coefs);\n",
    "p = plt.contour(X1, X2, mses_coefs, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');\n",
    "sns.lineplot(x=w_steps[:,1], y=w_steps[:,2], color='black', sort=False, alpha=0.5);\n",
    "sns.scatterplot(x=w_steps[:,1], y=w_steps[:,2], hue=np.arange(itr), edgecolor=None);\n",
    "plt.scatter(w_true[1], w_true[2], c='black', marker='*');\n",
    "\n",
    "plt.suptitle(\"Estimate after %d iterations with rate %s and batch size %d: %s\" % \n",
    "            (itr, \"{0:0.4f}\".format(lr), n_batch, [\"{0:0.4f}\".format(w) for w in w_star]));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interactive\n",
    "-----------\n",
    "\n",
    "You can use this interactive to explore different gradient descent\n",
    "options and see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "w_true = [2, 6, 5]\n",
    "x, y = generate_linear_regression_data(n=n_samples, d=2, coef=w_true[1:], intercept=w_true[0], sigma=0)\n",
    "X = np.hstack((np.ones((n_samples, 1)), x))\n",
    "\n",
    "@interact(itr = widgets.IntSlider(min=10, max=200, step=10, value=100),\n",
    "          lr = widgets.FloatSlider(min=0.05, max=0.95, step=0.05, value=0.1),\n",
    "          n_batch = widgets.IntSlider(min=1, max=100, step=1, value=100),\n",
    "          sigma = widgets.FloatSlider(min=0, max=5, step=0.5, value=1),\n",
    "          X = fixed(X), y = fixed(y))\n",
    "def plot_gd(itr, lr, n_batch, sigma, X, y):\n",
    "\n",
    "  y = y + sigma * np.random.randn(n_samples)\n",
    "\n",
    "  w_init = [w_true[0], 3, 7]\n",
    "  w_steps = np.zeros((itr, len(w_init)))\n",
    "  mse_steps = np.zeros(itr)\n",
    "\n",
    "  w_star = w_init\n",
    "  for i in range(itr):\n",
    "    w_star, mse, grad = sgd_step(w_star, X, y, lr, n_batch)\n",
    "    w_steps[i] = w_star\n",
    "    mse_steps[i] = mse\n",
    "\n",
    "  plt.figure(figsize=(18,5))\n",
    "  plt.subplot(1,3,1);\n",
    "\n",
    "  for n, w in enumerate(w_true):\n",
    "    plt.axhline(y=w, linestyle='--', color=colors[n]);\n",
    "    sns.lineplot(x=np.arange(itr), y=w_steps[:,n], color=colors[n], label='w' + str(n));\n",
    "  plt.xlabel(\"Iteration\");\n",
    "  plt.ylabel(\"Coefficient Value\");\n",
    "\n",
    "  plt.subplot(1, 3, 2);\n",
    "  sns.lineplot(x=np.arange(itr), y=mse_steps);\n",
    "  #plt.yscale(\"log\")\n",
    "  plt.xlabel(\"Iteration\");\n",
    "  plt.ylabel(\"Training MSE\");\n",
    "\n",
    "  plt.subplot(1, 3, 3);\n",
    "  X1, X2 = np.meshgrid(coefs, coefs);\n",
    "  p = plt.contour(X1, X2, mses_coefs, levels=5);\n",
    "  plt.clabel(p, inline=1, fontsize=10);\n",
    "  plt.xlabel('w1');\n",
    "  plt.ylabel('w2');\n",
    "  sns.lineplot(x=w_steps[:,1], y=w_steps[:,2], color='black', sort=False, alpha=0.5);\n",
    "  sns.scatterplot(x=w_steps[:,1], y=w_steps[:,2], hue=np.arange(itr), edgecolor=None);\n",
    "  plt.scatter(w_true[1], w_true[2], c='black', marker='*');\n",
    "\n",
    "  plt.suptitle(\"Estimate after %d iterations with rate %s and batch size %d: %s\" % \n",
    "              (itr, \"{0:0.4f}\".format(lr), n_batch, [\"{0:0.4f}\".format(w) for w in w_star]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A less friendly loss surface\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_true = [2, 5, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "d = 1\n",
    "sigma = 1\n",
    "\n",
    "x1 = np.random.randn(n_samples,d)\n",
    "x2 = x1 + (sigma/5)*np.random.randn(n_samples,1)\n",
    "x = np.column_stack([x1, x2])\n",
    "y = (np.dot(x, w_true[1:]) + w_true[0]).squeeze() + sigma * np.random.randn(n_samples)\n",
    "\n",
    "\n",
    "X = np.column_stack((np.ones((n_samples, 1)), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=x1.squeeze(), y=x2.squeeze());\n",
    "plt.xlabel('x1');\n",
    "plt.ylabel('x2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(3, 7, 0.02)\n",
    "\n",
    "coef_grid = np.array(np.meshgrid(coefs, coefs)).reshape(1, 2, coefs.shape[0], coefs.shape[0])\n",
    "y_hat_c = (w_true[0] + np.sum(coef_grid * x.reshape(x.shape[0], 2, 1, 1), axis=1) )\n",
    "mses_coefs = np.mean((y.reshape(-1, 1, 1)- y_hat_c)**2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contour(X1, X2, mses_coefs, levels=15);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');\n",
    "plt.scatter(w_true[1], w_true[2], c='black', marker='*');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = 100\n",
    "lr = 0.1\n",
    "w_init = [w_true[0], 3, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_steps = np.zeros((itr, len(w_init)))\n",
    "mse_steps = np.zeros(itr)\n",
    "grad_steps = np.zeros((itr, len(w_init)))\n",
    "\n",
    "w_star = w_init\n",
    "for i in range(itr):\n",
    "  w_star, mse, gradient = gd_step(w_star, X, y, lr)\n",
    "  w_steps[i] = w_star\n",
    "  mse_steps[i] = mse\n",
    "  grad_steps[i] = gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "plt.subplot(1,3,1);\n",
    "\n",
    "for n, w in enumerate(w_true):\n",
    "  plt.axhline(y=w, linestyle='--', color=colors[n]);\n",
    "  sns.lineplot(x=np.arange(itr), y=w_steps[:,n], color=colors[n], label='w' + str(n));\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Coefficient Value\");\n",
    "\n",
    "plt.subplot(1, 3, 2);\n",
    "sns.lineplot(x=np.arange(itr), y=mse_steps);\n",
    "#plt.yscale(\"log\")\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Training MSE\");\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 3);\n",
    "X1, X2 = np.meshgrid(coefs, coefs);\n",
    "p = plt.contour(X1, X2, mses_coefs, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');\n",
    "sns.lineplot(x=w_steps[:,1], y=w_steps[:,2], color='black', sort=False, alpha=0.5);\n",
    "sns.scatterplot(x=w_steps[:,1], y=w_steps[:,2], hue=np.arange(itr), edgecolor=None);\n",
    "plt.scatter(w_true[1], w_true[2], c='black', marker='*');\n",
    "\n",
    "plt.suptitle(\"Estimate after %d iterations with rate %s: %s\" % \n",
    "          (itr, \"{0:0.4f}\".format(lr), [\"{0:0.4f}\".format(w) for w in w_star]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other things to try\n",
    "\n",
    "-   What happens if we increase the learning rate (try e.g. 0.9)?\n",
    "-   What happens if we change the initial “guess”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_step_momentum(w, X, y, lr, eta, v):\n",
    "  # use current parameters to get y_hat, error\n",
    "  y_hat = np.dot(X,w)\n",
    "  error = y_hat-y\n",
    "  # compute gradient and velocity\n",
    "  grad = np.matmul(X.T, error)\n",
    "  v_new = eta*v - (lr/X.shape[0])*grad\n",
    "  # update weights\n",
    "  w_new = w - (lr/X.shape[0])*grad + eta*v_new\n",
    "\n",
    "  # we don't have to actually compute MSE\n",
    "  # but I want to, for visualization \n",
    "  mse = np.mean(error**2, axis=0)\n",
    "\n",
    "  return (w_new, mse, grad, v_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = 100\n",
    "lr = 0.1\n",
    "eta = 0.9\n",
    "w_init = [w_true[0], 3, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_steps = np.zeros((itr, len(w_init)))\n",
    "mse_steps = np.zeros(itr)\n",
    "grad_steps = np.zeros((itr, len(w_init)))\n",
    "v_steps = np.zeros((itr, len(w_init)))\n",
    "\n",
    "w_star = w_init\n",
    "velocity = np.zeros(len(w_init))\n",
    "for i in range(itr):\n",
    "  w_star, mse, gradient, velocity = gd_step_momentum(w_star, X, y, lr, eta, velocity)\n",
    "  w_steps[i] = w_star\n",
    "  mse_steps[i] = mse\n",
    "  grad_steps[i] = gradient\n",
    "  v_steps[i] = velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "plt.subplot(1,3,1);\n",
    "\n",
    "for n, w in enumerate(w_true):\n",
    "  plt.axhline(y=w, linestyle='--', color=colors[n]);\n",
    "  sns.lineplot(x=np.arange(itr), y=w_steps[:,n], color=colors[n], label='w' + str(n));\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Coefficient Value\");\n",
    "\n",
    "plt.subplot(1, 3, 2);\n",
    "for n, w in enumerate(w_true):\n",
    "  sns.lineplot(x=np.arange(itr), y=v_steps[:,n], color=colors[n]);\n",
    "plt.xlabel(\"Iteration\");\n",
    "plt.ylabel(\"Velocity\");\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 3);\n",
    "X1, X2 = np.meshgrid(coefs, coefs);\n",
    "p = plt.contour(X1, X2, mses_coefs, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('w2');\n",
    "sns.lineplot(x=w_steps[:,1], y=w_steps[:,2], color='black', sort=False, alpha=0.5);\n",
    "sns.scatterplot(x=w_steps[:,1], y=w_steps[:,2], hue=np.arange(itr), edgecolor=None);\n",
    "plt.scatter(w_true[1], w_true[2], c='black', marker='*');\n",
    "\n",
    "plt.suptitle(\"Estimate after %d iterations with rate %s: %s\" % \n",
    "          (itr, \"{0:0.4f}\".format(lr), [\"{0:0.4f}\".format(w) for w in w_star]));\n"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
