{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization\n",
    "\n",
    "*Fraida Fund*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import scipy\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils.fixes import loguniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For models with a single hyperparameter controlling bias-variance (for example: $k$ in $k$ nearest neighbors), we used sklearns's `KFoldCV` or `validation_curve` to test a range of values for the hyperparameter, and to select the best one.\n",
    "\n",
    "When we have *multiple* hyperparameters to tune, we can use `GridSearchCV` to select the best *combination* of them.\n",
    "\n",
    "For example, we saw three ways to tune the bias-variance of an SVM classifier:\n",
    "\n",
    "-   Changing the kernel\n",
    "-   Changing $C$\n",
    "-   For an RBF kernel, changing $\\gamma$\n",
    "\n",
    "To get the best performance from an SVM classifier, we need to find the best *combination* of these hyperparameters. This notebook shows how to use `GridSearchCV` to tune an SVM classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a subset of the MNIST handwritten digits data. First, we will get the data, and assign a small subset of samples to training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=1000, test_size=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try this initial parameter “grid”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [0.1, 1000], 'kernel': ['linear']},\n",
    "  {'C': [0.1, 1000], 'gamma': [0.01, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll set up the grid search. We can use `fit` on it, just like any other `sklearn` model.\n",
    "\n",
    "I added `return_train_score=True` to my `GridSearchSV` so that it will show me training scores as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=100, n_jobs=-1, return_train_score=True)\n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inform our search, we will use our understanding of how SVMs work, and especially how the $C$ and $\\gamma$ parameters control the bias and variance of the SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s tackle the linear SVM first, since it’s faster to fit. We didn’t see any change in the accuracy when we vary $C$. So, we should extend the range of $C$ over which we search.\n",
    "\n",
    "I’ll try higher and lower values of $C$, to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1e-6, 1e-4, 1e-2, 1e2, 1e4, 1e6], 'kernel': ['linear']},\n",
    " ]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=100, n_jobs=-1, return_train_score=True)\n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x='param_C', y='mean_train_score', label=\"Training score\");\n",
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x='param_C', y='mean_test_score', label=\"Validation score\");\n",
    "plt.xscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we get a slightly better validation score near the smaller values for $C$! What does this mean?\n",
    "\n",
    "Let’s try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': np.linspace(1e-5, 1e-7, num=10), 'kernel': ['linear']},\n",
    " ]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=100, n_jobs=-1, return_train_score=True)\n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x='param_C', y='mean_train_score', label=\"Training score\");\n",
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x='param_C', y='mean_test_score', label=\"Validation score\");\n",
    "plt.xscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can be satisfied that we have found a good hyperparameter only when we see the high bias AND high variance side of the validation curve!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s look at the RBF kernel.\n",
    "\n",
    "In our first search, the accuracy of the RBF kernel is very poor. We may have high bias, high variance, (or both)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $C=0.1$ in our first search, both training and validation scores were low. This suggests high bias.\n",
    "\n",
    "When $C=1000$ in our first search, training scores were high and validation scores were low. This suggests high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What next? We know from our discussion of bias and variance of SVMs that to combat overfitting, we can decrease $\\gamma$ and/or decrease $C$.\n",
    "\n",
    "For now, let’s keep the higher value of $C$, and try to reduce the overfitting by decreasing $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1000], 'gamma': [1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11], 'kernel': ['rbf']},\n",
    " ]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(SVC(), param_grid, cv=2, refit=True, verbose=100, n_jobs=-1, return_train_score=True)\n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x='param_gamma', y='mean_train_score', label=\"Training score\")\n",
    "sns.lineplot(data=pd.DataFrame(clf.cv_results_), x='param_gamma', y='mean_test_score', label=\"Validation score\")\n",
    "plt.xscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that (at least for $C=1000$), values of $\\gamma$ greater than `1e-5` seem to overfit, while decreasing $\\gamma$ lower than `1e-10` may underfit.\n",
    "\n",
    "But we know that changing $C$ also affects the bias variance tradeoff! For different values of $C$, the best value of $\\gamma$ will be different, and there may be a better *combination* of $C$ and $\\gamma$ than any we have seen so far. We can try to increase and decrease $C$ to see if that improves the validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a better idea of where to search, we can set up our “final” search grid.\n",
    "\n",
    "We know that to find the best validation accuracy for the linear kernel, we should make sure our search space includes `1e-6` and `1e-7`. I chose to vary $C$ from `1e-8` to `1e-4`. (I want to make sure the best value is not at the edge of the search space, so that we can be sure there isn’t a better value if we go lower/higher.)\n",
    "\n",
    "We know that to find the best validation accuracy for the RBF kernel, we should make sure our search space includes $\\gamma$ values around `1e-6` and `1e-7` when $C=1000$. For larger values of $C$, we expect that we’ll get better results with smaller values of $\\gamma$. For smaller values of $C$, we expect that we’ll get better results with larger values of $\\gamma$. I chose to vary $C$ from `1` to `1e6` and $\\gamma$ from `1e-4` to `1e-11`.\n",
    "\n",
    "That’s a big search grid, so this takes a long time to fit! (Try this at home with a larger training set to get an idea...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1e-8, 1e-7, 1e-6, 1e-5, 1e-4], 'kernel': ['linear']},\n",
    "  {'C': [1, 1e2, 1e3, 1e4, 1e5, 1e6], 'gamma': [1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11], 'kernel': ['rbf']},\n",
    " ]\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=100, n_jobs=-1, return_train_score=True)\n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linear kernel, here's what we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv   = pd.DataFrame(clf.cv_results_)\n",
    "df_cv = df_cv[df_cv['param_kernel']=='linear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df_cv, x='param_C', y='mean_train_score', label=\"Training score\")\n",
    "sns.lineplot(data=df_cv, x='param_C', y='mean_test_score', label=\"Validation score\")\n",
    "plt.xscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the RBF kernel, here's what we found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv   = pd.DataFrame(clf.cv_results_)\n",
    "df_cv = df_cv[df_cv['param_kernel']=='rbf']\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "ax1=plt.subplot(1,2,1)\n",
    "pvt = pd.pivot_table(df_cv, values='mean_test_score', index='param_C', columns='param_gamma')\n",
    "sns.heatmap(pvt, annot=True, cbar=False, vmin=0, vmax=1, cmap='PiYG');\n",
    "plt.title(\"Validation scores\");\n",
    "\n",
    "ax2=plt.subplot(1,2,2, sharey=ax1)\n",
    "plt.setp(ax2.get_yticklabels(), visible=False)\n",
    "pvt = pd.pivot_table(df_cv, values='mean_train_score', index='param_C', columns='param_gamma')\n",
    "sns.heatmap(pvt, annot=True, cbar=False, vmin=0, vmax=1, cmap='PiYG');\n",
    "plt.title(\"Training scores\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that $\\gamma$ and $C$ control the bias-variance tradeoff of the SVM model as follows.\n",
    "\n",
    "-   In the top left region, $C$ is small (the margin is wider) and $\\gamma$ is small (the kernel bandwidth is large). In this region, the model has more bias (is prone to underfit). The validation scores and training scores are both low.\n",
    "-   On the right side (and we'd expect to see this on the bottom right if we extend the range of $C$ even higher), $C$ is large (the margin is narrower) and $\\gamma$ is large (the kernel bandwidth is small. In this region, the model has more variance (is likely to overfit). The validation scores are low, but the training scores are high.\n",
    "\n",
    "In the middle, we have a region of good combinations of $C$ and $\\gamma$.\n",
    "\n",
    "Since the parameter grid above shows us the validation accuracy decreasing both as we increase each parameter\\* and also as we decrease each parameter, we can be a bit more confident that we captured the point in the bias-variance surface where the error is smallest.\n",
    "\n",
    "\\* $C$ is different because increasing $C$ even more may not actually change the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the “best” parameters, with which the model was re-fitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can evaluate the re-fitted model on the test set. (Note that the `GridSearchCV` only used the training set; we have not used the test set at all for model fitting.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our grid search found a pretty good set of hyperparameters, but it took a long time - about 100 seconds.\n",
    "\n",
    "With a random search, we may be able to find hyperparameters that are still pretty good, in much less time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will search a similar range of parameters, although focusing only on the RBF kernel. But instead of specifying points on a grid like\n",
    "\n",
    "    param_grid = [\n",
    "      {'C': [1, 1e2, 1e3, 1e4, 1e5, 1e6], 'gamma': [1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11], 'kernel': ['rbf']},\n",
    "     ]\n",
    "\n",
    "we will specify distributions from which to sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': loguniform(1, 1e6), 'gamma': loguniform(1e-11, 1e-4), 'kernel': ['rbf']},\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we will specify the total number of points to sample - 10, in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomizedSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=100, n_jobs=-1, return_train_score=True,  n_iter = 10)\n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our random search can find a good solution, in only about ~20 seconds. However, depending on the random samples it chooses, it may be a better solution or a worse solution than the one we found via grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Search (Bayes Search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we’ll consider one other type of hyperparameter optimization: we will look at an adaptive search that uses information about the models it has seen so far in order to decide which part of the hyperparameter space to sample from next.\n",
    "\n",
    "We will install the `scikit-optimize` package, which provides `BayesSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from skopt.plots import plot_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the search space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': (1, 1e6, 'log-uniform'), 'gamma': (1e-11, 1e-4, 'log-uniform'), 'kernel': ['rbf']},\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will specify the total number of points to sample - 5, in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BayesSearchCV(SVC(), param_grid, cv=3, refit=True, verbose=100, n_jobs=-1, return_train_score=True,  n_iter = 5)\n",
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how this works, we will re-run the Bayes search with more iterations than we really need, just so that we can visualize how it searches the hyperparameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BayesSearchCV(SVC(), param_grid, cv=3, refit=False, verbose=100, n_jobs=-1, n_iter = 50)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluations(clf.optimizer_results_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a grid of plots as follows:\n",
    "\n",
    "-   the diagonal plots are histograms, that show the distribution of samples for each hyperparameter.\n",
    "-   the scatter plot shows the samples in the hyperparameter space that were “visited”, and the order in which they were “visited” is encoded i the point’s color. A red star shows the best hyperparameters that we found."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 }
}
