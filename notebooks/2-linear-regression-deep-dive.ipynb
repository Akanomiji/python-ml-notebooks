{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression: deep dive\n",
    "============================\n",
    "\n",
    "*Fraida Fund*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# for 3d interactive plots\n",
    "from ipywidgets import interact, fixed\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generated by a linear function\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose each sample of data is generated as\n",
    "\n",
    "$$y_i = w_0 + w_1 x_{i,1} + \\ldots + w_d x_{i,d} + \\epsilon_i $$\n",
    "\n",
    "where $\\epsilon_i \\sim N(0, \\sigma^2)$.\n",
    "\n",
    "(If we use a linear regression, the assumed hypothesis class is a good\n",
    "match for the problem.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a function to generate this kind of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linear_regression_data(n=100, d=1, coef=[5], intercept=1, sigma=0):\n",
    "  x = np.random.randn(n,d)\n",
    "  y = (np.dot(x, coef) + intercept).squeeze() + sigma * np.random.randn(n)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and some default values we’ll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "coef = [5]\n",
    "intercept = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear regression (univariate)\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_regression_data(n=n_samples, d=1, coef=coef, intercept=intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x_train.squeeze(), y_train, s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_simple = LinearRegression().fit(x_train, y_train)\n",
    "print(\"Intercept: \" , reg_simple.intercept_)\n",
    "print(\"Coefficient list: \", reg_simple.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_train), np.max(x_train)]\n",
    "y_line = x_line*reg_simple.coef_ + reg_simple.intercept_\n",
    "\n",
    "sns.scatterplot(x_train.squeeze(), y_train, s=50);\n",
    "sns.lineplot(x_line, y_line, color='red');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: other ways to do the same thing...\n",
    "x_tilde = np.hstack((np.ones((n_samples, 1)), x_train))\n",
    "\n",
    "# using matrix operations to find (X^T X)^{-1} X^T y\n",
    "print( (np.linalg.inv((x_tilde.T.dot(x_tilde))).dot(x_tilde.T)).dot(y_train) )\n",
    "# using the lstsq solver, which solves ax = b \n",
    "# a may be under-, well-, or over-determined\n",
    "print( np.linalg.lstsq(x_tilde,y_train,rcond=0)[0] ) \n",
    "# using solve: only works on matrix that is square and of full-rank\n",
    "print( np.linalg.solve(x_tilde.T.dot(x_tilde), x_tilde.T.dot(y_train)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mean-removed equivalent\n",
    "\n",
    "Quick digression - what if we don’t want to bother with intercept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_mr = x_train - np.mean(x_train)\n",
    "y_train_mr = y_train - np.mean(y_train)\n",
    "sns.scatterplot(x_train_mr.squeeze(), y_train_mr, s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that now the data is mean removed - zero mean in every dimension.\n",
    "\n",
    "This time, the fitted linear regression has 0 intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_mr = LinearRegression().fit(x_train_mr, y_train_mr)\n",
    "print(\"Intercept: \" , reg_mr.intercept_)\n",
    "print(\"Coefficient list: \", reg_mr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict some new points\n",
    "\n",
    "OK, now we can predict some new points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = generate_linear_regression_data(n=50)\n",
    "y_test_hat = reg_simple.intercept_ + np.dot(x_test,reg_simple.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_simple.coef_ + reg_simple.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x_line, y_line, color='red');\n",
    "sns.scatterplot(x_test.squeeze(), y_test_hat, s=50, color='purple');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we will compute the MSE on the test data (not the\n",
    "data used to find the parameters).\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - (w_0 + w_1 x_i)) ^2$$\n",
    "\n",
    "Use $\\hat{y}_i = w_0 + w_1 x_i$, then\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i) ^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = reg_simple.intercept_ + np.dot(x_test,reg_simple.coef_)\n",
    "mse_simple = 1.0/(len(y_test)) * np.sum((y_test - y_test_hat)**2)\n",
    "mse_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to do the same thing using sklearn\n",
    "y_test_hat = reg_simple.predict(x_test)\n",
    "metrics.mean_squared_error(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize MSE for different coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.scatterplot(x_test.squeeze(), y_test_hat, s=50);\n",
    "p = plt.xlabel('x')\n",
    "p = plt.ylabel('y')\n",
    "\n",
    "coefs = np.arange(2, 8, 0.5)\n",
    "mses = np.zeros(len(coefs))\n",
    "for idx, c in enumerate(coefs):\n",
    "  y_test_coef = (reg_simple.intercept_ + np.dot(x_test,c)).squeeze()\n",
    "  mses[idx] =  1.0/(len(y_test_coef)) * np.sum((y_test - y_test_coef)**2)\n",
    "  x_line = [np.min(x_train), np.max(x_train)]\n",
    "  y_line = [x_line[0]*c + reg_simple.intercept_, x_line[1]*c + intercept]\n",
    "  p = sns.lineplot(x_line, y_line, color='red', alpha=0.2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=coefs, y=mses);\n",
    "sns.scatterplot(x=coefs, y=mses, s=50);\n",
    "sns.scatterplot(x=reg_simple.coef_, y=mse_simple, color='red', s=100);\n",
    "p = plt.xlabel('w1');\n",
    "p = plt.ylabel('Test MSE');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance, explained variance, R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick reminder:\n",
    "\n",
    "Mean of $x$ and $y$:\n",
    "\n",
    "$$\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i, \\quad \\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i$$\n",
    "\n",
    "Sample variance of $x$ and $y$:\n",
    "\n",
    "$$\\sigma_x^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x}) ^2, \\quad \\sigma_y^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y}) ^2$$\n",
    "\n",
    "Sample covariance of $x$ and $y$:\n",
    "\n",
    "$$\\sigma_{xy} = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_y = 1.0/len(y_test) * np.sum((y_test - np.mean(y_test))**2)\n",
    "var_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_y = np.mean(y_test)\n",
    "mean_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of $y$ is the mean sum of the squares of the distances from\n",
    "each $y_i$ to $\\bar{y}$. These distances are illustrated here:\n",
    "\n",
    "-   the horizontal line shows $\\bar{y}$\n",
    "-   each vertical line is a distance from a $y_i$ to $\\bar{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hlines(y=mean_y, xmin=np.min(x_test), xmax=np.max(x_test));\n",
    "plt.vlines(x_test, ymin=mean_y, ymax=y_test, color='magenta');\n",
    "sns.scatterplot(x_test.squeeze(), y_test, color='purple', s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s look at a similar kind of plot, but with distances to the\n",
    "regression line instead of the to mean line:\n",
    "\n",
    "-   In the previous plot, each vertical line was a $y_i - \\bar{y}$\n",
    "-   In the following plot, each vertical line is a $y_i - \\hat{y}_i$\n",
    "\n",
    "(where $\\hat{y}_i$ is the prediction of the linear regression for a\n",
    "given sample $i$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, y_test_hat);\n",
    "plt.vlines(x_test, ymin=y_test, ymax=y_test_hat, color='magenta', alpha=0.5);\n",
    "sns.scatterplot(x_test.squeeze(), y_test, color='purple', s=50);\n",
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_simple.coef_ + reg_simple.intercept_\n",
    "sns.lineplot(x_line, y_line, color='red');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two plots together show how well the variance of $y$ is\n",
    "“explained” by the linear regression model:\n",
    "\n",
    "-   The total variance of $y$ is shown in the first plot, where each\n",
    "    vertical line is $y_i - \\bar{y}$\n",
    "-   The *unexplained* variance of $y$ is shown in the second plot, where\n",
    "    each vertical line is the error of the model, $y_i - \\hat{y}_i$\n",
    "\n",
    "In this example, *all* of the variance of $y$ is “explained” by the\n",
    "linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE for this example is 0, R2 is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear regression with noise\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_regression_data(n=n_samples, d=1, coef=coef, intercept=intercept, sigma=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x_train.squeeze(), y_train, s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_noisy = LinearRegression().fit(x_train, y_train)\n",
    "print(\"Coefficient list: \", reg_noisy.coef_)\n",
    "print(\"Intercept: \" , reg_noisy.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_train), np.max(x_train)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\n",
    "\n",
    "sns.scatterplot(x_train.squeeze(), y_train, s=50);\n",
    "sns.lineplot(x_line, y_line, color='red');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict some new points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = generate_linear_regression_data(n=50, d=1, coef=coef, intercept=intercept, sigma=2)\n",
    "y_test_hat = reg_noisy.intercept_ + np.dot(x_test,reg_noisy.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.scatterplot(x_train.squeeze(), y_train);\n",
    "sns.lineplot(x_line, y_line, color='red');\n",
    "sns.scatterplot(x_test.squeeze(), y_test_hat, color='red', s=50);\n",
    "sns.scatterplot(x_test.squeeze(), y_test, color='purple', s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = reg_noisy.intercept_ + np.dot(x_test,reg_noisy.coef_)\n",
    "mse_noisy = 1.0/(len(y_test)) * np.sum((y_test - y_test_hat)**2)\n",
    "mse_noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE is higher than before! Does this mean our estimate of $w_0$ and\n",
    "$w_1$ is not optimal?\n",
    "\n",
    "Since we generated the data, we know the “true” coefficient value and we\n",
    "can see how much the MSE would be with the true coefficient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_perfect_coef = intercept + np.dot(x_test,coef)\n",
    "\n",
    "mse_perfect_coef = 1.0/(len(y_test_perfect_coef)) * np.sum((y_test_perfect_coef - y_test)**2)\n",
    "mse_perfect_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s still a higher MSE than we had before, even with a “perfect”\n",
    "estimate of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: I thought we selected the coefficients that minimize MSE! But\n",
    "sometimes our linear regression doesn’t select the “right” coefficients,\n",
    "even if they give us lower MSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = reg_noisy.intercept_ + np.dot(x_train,reg_noisy.coef_)\n",
    "mse_train_est = 1.0/(len(y_train)) * np.sum((y_train - y_train_hat)**2)\n",
    "mse_train_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_perfect_coef = intercept + np.dot(x_train,coef)\n",
    "mse_train_perfect = 1.0/(len(y_train_perfect_coef)) * np.sum((y_train_perfect_coef - y_train)**2)\n",
    "mse_train_perfect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The “correct” coefficients actually had slightly higher MSE on the\n",
    "training set. We fit parameters so that they are optimal on the\n",
    "*training* set, then we use the test set to understand how the model\n",
    "will generalize to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw some error due to noise in the data, and some due to error in the\n",
    "parameter estimates.\n",
    "\n",
    "In a couple of weeks - we will formalize this discussion of different\n",
    "sources of error:\n",
    "\n",
    "-   Error in parameter estimates\n",
    "-   “Noise” - any variation in data that is not a function of the $X$\n",
    "    that we use as input to the model\n",
    "-   Other error - wrong hypothesis class, for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize MSE for different coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(4.5, 5.5, 0.1)\n",
    "mses_test = np.zeros(len(coefs))\n",
    "mses_train = np.zeros(len(coefs))\n",
    "\n",
    "for idx, c in enumerate(coefs):\n",
    "  y_test_coef = (reg_noisy.intercept_ + np.dot(x_test,c)).squeeze()\n",
    "  mses_test[idx] =  1.0/(len(y_test_coef)) * np.sum((y_test - y_test_coef)**2)\n",
    "  y_train_coef = (reg_noisy.intercept_ + np.dot(x_train,c)).squeeze()\n",
    "  mses_train[idx] =  1.0/(len(y_train_coef)) * np.sum((y_train - y_train_coef)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.lineplot(x=coefs, y=mses_train)\n",
    "sns.scatterplot(x=coefs, y=mses_train, s=50);\n",
    "sns.scatterplot(x=reg_noisy.coef_, y=mse_train_est, color='red', s=100);\n",
    "plt.title(\"Training MSE vs. coefficient\");\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('MSE');\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.lineplot(x=coefs, y=mses_test)\n",
    "sns.scatterplot(x=coefs, y=mses_test, s=50);\n",
    "sns.scatterplot(x=reg_noisy.coef_, y=mse_noisy, color='red', s=100);\n",
    "plt.title(\"Testing MSE vs. coefficient\");\n",
    "plt.xlabel('w1');\n",
    "plt.ylabel('MSE');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot on the left (for training MSE), the red dot (our coefficient\n",
    "estimate) should always have minimum MSE, because we select parameters\n",
    "to minimize MSE on the training set.\n",
    "\n",
    "In the plot on the right (for test MSE), the red dot might not have the\n",
    "minimum MSE, because there is variance in the data. The best coefficient\n",
    "on the training set might not be the best coefficient on the test set.\n",
    "This gives us some idea of how our model will generalize to new, unseen\n",
    "data. We may suspect that if the coefficient estimate is not perfect for\n",
    "*this* test data, it might have some error on other new, unseen data,\n",
    "too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance, explained variance, R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_y = 1.0/len(y_test) * np.sum((y_test - np.mean(y_test))**2)\n",
    "var_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_y = np.mean(y_test)\n",
    "mean_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\n",
    "plt.hlines(y=mean_y, xmin=np.min(x_test), xmax=np.max(x_test));\n",
    "plt.vlines(x_test, ymin=mean_y, ymax=y_test, color='magenta');\n",
    "sns.scatterplot(x_test.squeeze(), y_test, color='purple', s=50);\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.vlines(x_test, ymin=y_test, ymax=y_test_hat, color='red');\n",
    "sns.scatterplot(x_test.squeeze(), y_test, color='purple', s=50);\n",
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\n",
    "sns.lineplot(x_line, y_line, color='red');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember:\n",
    "\n",
    "-   The total variance of $y$ is shown in the first plot, where each\n",
    "    vertical line is $y_i - \\bar{y}$\n",
    "-   The *unexplained* variance of $y$ is shown in the second plot, where\n",
    "    each vertical line is the error of the model, $y_i - \\hat{y}_i$\n",
    "\n",
    "In the next plot, we’ll combine them to get some intuition regarding the\n",
    "*fraction of unexplained variance*. The dark maroon part of each\n",
    "vertical bar is the *unexplained* part, while the red part is\n",
    "*explained* by the linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_line = [np.min(x_test), np.max(x_test)]\n",
    "y_line = x_line*reg_noisy.coef_ + reg_noisy.intercept_\n",
    "\n",
    "plt.hlines(y=mean_y, xmin=np.min(x_test), xmax=np.max(x_test));\n",
    "plt.vlines(x_test, ymin=mean_y, ymax=y_test, color='red');\n",
    "plt.vlines(x_test, ymin=y_test, ymax=y_test_hat, color='maroon');\n",
    "sns.scatterplot(x_test.squeeze(), y_test, color='purple', s=50);\n",
    "sns.lineplot(x_line, y_line, color='red');\n",
    "plt.xlabel('x');\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fraction of variance unexplained** is the ratio of the sum of squared\n",
    "distances from data to the regression line (sum of squared vertical\n",
    "distances in second plot), to the sum of squared distanced from data to\n",
    "the mean (sum of squared vertical distances in first plot):\n",
    "\n",
    "$$\\frac{MSE}{Var(y)} = \\frac{Var(y-\\hat{y})}{Var(y)} = \\frac{\\sum_{i=1}^n(y_i-\\hat y_i)^2}{\\sum_{i=1}^n(y_i - \\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative interpretation: imagine we would develop a very simple ML\n",
    "model, in which we always predict $\\hat{y}_i = \\bar{y}_i$. Then, we use\n",
    "this model as a basis for comparison for other, more sophisticated\n",
    "models. The ratio above is the ratio of error of the regression model,\n",
    "to the error of a “prediction by mean” model.\n",
    "\n",
    "-   If this quantity is less than 1, our model is better than\n",
    "    “prediction by mean”\n",
    "-   If this quantity is greater than 1, our model is worse than\n",
    "    “prediction by mean”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fvu = mse_noisy/var_y\n",
    "fvu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = 1 - fvu\n",
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to do the same thing...\n",
    "metrics.r2_score(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a negative R2 mean, in terms of a comparison to “prediction by\n",
    "mean”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s not just noise\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"anscombe\")\n",
    "df.groupby('dataset').agg({'x': ['count','mean', 'std'], 'y': ['count','mean', 'std']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_i   = df[df['dataset'].eq('I')]\n",
    "data_ii  = df[df['dataset'].eq('II')]\n",
    "data_iii = df[df['dataset'].eq('III')]\n",
    "data_iv  = df[df['dataset'].eq('IV')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_i   = LinearRegression().fit(data_i[['x']],   data_i['y'])\n",
    "reg_ii  = LinearRegression().fit(data_ii[['x']],  data_ii['y'])\n",
    "reg_iii = LinearRegression().fit(data_iii[['x']], data_iii['y'])\n",
    "reg_iv  = LinearRegression().fit(data_iv[['x']],  data_iv['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset I:   \",   reg_i.coef_,   reg_i.intercept_)\n",
    "print(\"Dataset II:  \",  reg_ii.coef_,  reg_ii.intercept_)\n",
    "print(\"Dataset III: \", reg_iii.coef_, reg_iii.intercept_)\n",
    "print(\"Dataset IV:  \",  reg_iv.coef_,  reg_iv.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset I:   \", metrics.r2_score(data_i['y'],  reg_i.predict(data_i[['x']])))\n",
    "print(\"Dataset II:  \", metrics.r2_score(data_ii['y'], reg_ii.predict(data_ii[['x']])))\n",
    "print(\"Dataset III: \", metrics.r2_score(data_iii['y'],reg_iii.predict(data_iii[['x']])))\n",
    "print(\"Dataset IV:  \", metrics.r2_score(data_iv['y'], reg_iv.predict(data_iv[['x']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\", \n",
    "           data=df, col_wrap=2, ci=None, palette=\"muted\", height=4, \n",
    "           scatter_kws={\"s\": 50, \"alpha\": 1});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy to identify problems in 1D - what about in higher D?\n",
    "\n",
    "-   Plot $y$ against $\\hat{y}$\n",
    "-   Plot residuals against $y$\n",
    "-   Plot residuals against each $x$\n",
    "-   Plot residuals against time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple linear regression\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_regression_data(n=n_samples, d=2, coef=[5,5], intercept=intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.scatter(x_train[:,0],  y_train);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.subplot(1,2,2);\n",
    "plt.scatter(x_train[:,1],  y_train);\n",
    "plt.xlabel(\"x2\");\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_multi = LinearRegression().fit(x_train, y_train)\n",
    "print(\"Coefficient list: \", reg_multi.coef_)\n",
    "print(\"Intercept: \" , reg_multi.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev=20, azim=-20, X=x_train, y=y_train):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "\n",
    "\n",
    "    X1 = np.arange(-4, 4, 0.2)\n",
    "    X2 = np.arange(-4, 4, 0.2)\n",
    "    X1, X2 = np.meshgrid(X1, X2)\n",
    "    Z = X1*reg_multi.coef_[0] + X2*reg_multi.coef_[1]\n",
    "\n",
    "    # Plot the surface.\n",
    "    ax.plot_surface(X1, X2, Z, alpha=0.1, color='gray',\n",
    "                          linewidth=0, antialiased=False)\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], y, s=50)\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('y')\n",
    "\n",
    "interact(plot_3D, elev=np.arange(-90,90,10), azim=np.arange(-90,90,10),\n",
    "         X=fixed(x_train), y=fixed(y_train));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(4.5, 5.5, 0.05)\n",
    "mses_train = np.zeros((len(coefs), len(coefs)))\n",
    "\n",
    "for idx_1, c_1 in enumerate(coefs):\n",
    "  for idx_2, c_2 in enumerate(coefs):\n",
    "    y_train_coef = (reg_multi.intercept_ + np.dot(x_train,[c_1, c_2])).squeeze()\n",
    "    mses_train[idx_1,idx_2] =  1.0/(len(y_train_coef)) * np.sum((y_train - y_train_coef)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contour(X1, X2, mses_train, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple linear regression with noise\n",
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_linear_regression_data(n=n_samples, d=2, coef=[5,5], intercept=intercept, sigma=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "plt.subplot(1,2,1);\n",
    "plt.scatter(x_train[:,0],  y_train);\n",
    "plt.xlabel(\"x1\");\n",
    "plt.ylabel(\"y\");\n",
    "plt.subplot(1,2,2);\n",
    "plt.scatter(x_train[:,1],  y_train);\n",
    "plt.xlabel(\"x2\");\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_multi_noisy = LinearRegression().fit(x_train, y_train)\n",
    "print(\"Coefficient list: \", reg_multi_noisy.coef_)\n",
    "print(\"Intercept: \" , reg_multi_noisy.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_3D(elev=20, azim=-20, X=x_train, y=y_train):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(projection='3d')\n",
    "\n",
    "\n",
    "    X1 = np.arange(-4, 4, 0.2)\n",
    "    X2 = np.arange(-4, 4, 0.2)\n",
    "    X1, X2 = np.meshgrid(X1, X2)\n",
    "    Z = X1*reg_multi_noisy.coef_[0] + X2*reg_multi_noisy.coef_[1]\n",
    "\n",
    "    # Plot the surface.\n",
    "    ax.plot_surface(X1, X2, Z, alpha=0.1, color='gray',\n",
    "                          linewidth=0, antialiased=False)\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], y, s=50)\n",
    "\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_zlabel('y')\n",
    "\n",
    "interact(plot_3D, elev=np.arange(-90,90,10), azim=np.arange(-90,90,10),\n",
    "         X=fixed(x_train), y=fixed(y_train));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.arange(3, 7, 0.05)\n",
    "mses_train = np.zeros((len(coefs), len(coefs)))\n",
    "\n",
    "for idx_1, c_1 in enumerate(coefs):\n",
    "  for idx_2, c_2 in enumerate(coefs):\n",
    "    y_train_coef = (reg_multi_noisy.intercept_ + np.dot(x_train,[c_1, c_2])).squeeze()\n",
    "    mses_train[idx_1,idx_2] =  1.0/(len(y_train_coef)) * np.sum((y_train - y_train_coef)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5));\n",
    "X1, X2 = np.meshgrid(coefs, coefs)\n",
    "p = plt.contour(X1, X2, mses_train, levels=5);\n",
    "plt.clabel(p, inline=1, fontsize=10);\n",
    "plt.xlabel('w2');\n",
    "plt.ylabel('w1');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with semi-realistic data\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate principles of linear regression, we are going to use some\n",
    "data from the textbook \"An Introduction to Statistical Learning with\n",
    "Applications in R\" (Gareth James, Daniela Witten, Trevor Hastie, Robert\n",
    "Tibshirani) ([PDF\n",
    "link](https://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)).\n",
    "\n",
    "The dataset is described as follows:\n",
    "\n",
    "> Suppose that we are statistical consultants hired by a client to\n",
    "> provide advice on how to improve sales of a particular product. The\n",
    "> `Advertising` data set consists of the sales of that product in 200\n",
    "> different markets, along with advertising budgets for the product in\n",
    "> each of those markets for three different media: TV, radio, and\n",
    "> newspaper. ... It is not possible for our client to directly increase\n",
    "> sales of the product. On the other hand, they can control the\n",
    "> advertising expenditure in each of the three media. Therefore, if we\n",
    "> determine that there is an association between advertising and sales,\n",
    "> then we can instruct our client to adjust advertising budgets, thereby\n",
    "> indirectly increasing sales. In other words, our goal is to develop an\n",
    "> accurate model that can be used to predict sales on the basis of the\n",
    "> three media budgets.\n",
    "\n",
    "Sales are reported in thousands of units, and TV, radio, and newspaper\n",
    "budgets, are reported in thousands of dollars.\n",
    "\n",
    "The data is available online at the [author's\n",
    "website](http://faculty.marshall.usc.edu/gareth-james/ISL/data.html). We\n",
    "can get the URL for the actual data file by right-clicking on the\n",
    "`Advertising.csv` link and choosing \"Copy link address\" in Google Chrome\n",
    "(or equivalent in other browsers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://faculty.marshall.usc.edu/gareth-james/ISL/Advertising.csv'\n",
    "df = pd.read_csv(url, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Note that in this dataset, the first column in the data file is the row\n",
    "label; that's why we use `index_col=0` in the `read_csv` command. If we\n",
    "would omit that argument, then we would have an additional (unnamed)\n",
    "column in the dataset, containing the row number.\n",
    "\n",
    "You can try removing the `index_col` argument and re-running the cell\n",
    "above, to see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important panels here are on the bottom row, where `sales` is\n",
    "on the vertical axis and the advertising budgets are on the horizontal\n",
    "axes.\n",
    "\n",
    "It appears, at least from a quick inspection, that each type of\n",
    "advertising - TV, radio, and newspaper - potentially has a positive\n",
    "effect on sales, although the strength of the effect differs by\n",
    "advertising medium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split up data\n",
    "\n",
    "Next, we will split up data into \"training\" and \"testing\" sets; we are\n",
    "interested in evaluating model performance by its predictions on new\n",
    "data, not by how well it fits the data used to estimate the model\n",
    "parameters.\n",
    "\n",
    "(In a later lecture, we will discuss better ways to divide the data, but\n",
    "for now, this will suffice.)\n",
    "\n",
    "We will use 70% of the data for training and the remaining 30% to test\n",
    "the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_tv = LinearRegression().fit(train[['TV']], train['sales'])\n",
    "reg_radio = LinearRegression().fit(train[['radio']], train['sales'])\n",
    "reg_news = LinearRegression().fit(train[['newspaper']], train['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the regression coefficients\n",
    "\n",
    "One of the benefits of linear regression is its interpretability - from\n",
    "the regression coefficient, we can get a sense of how the target\n",
    "variable varies with changes in the feature variable.\n",
    "\n",
    "For example, if $w_1 = 0.0475$ for the TV regression, we can say that an\n",
    "additional \\$1,000 spend on TV advertising is, on average, associated\n",
    "with selling approximately 47.5 units of the product. Note that:\n",
    "\n",
    "-   we can show a correlation, but can't say that the relationship is\n",
    "    causative.\n",
    "-   the value for $w_1$ is only an *estimate* of the true relationship\n",
    "    between TV ad dollars and sales. We know that the estimate may have\n",
    "    some error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TV: \", reg_tv.coef_[0], reg_tv.intercept_)\n",
    "print(\"Radio: \", reg_radio.coef_[0], reg_radio.intercept_)\n",
    "print(\"Newspaper: \", reg_news.coef_[0], reg_news.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, radio appears to be most effective at driving sales of\n",
    "the product.\n",
    "\n",
    "In general, we have to be careful about directly comparing regression\n",
    "coefficients - if columns have different scales, we can't compare the\n",
    "magnitude of the coefficients directly. However, we can infer something\n",
    "about the relationship between the feature and target variable from the\n",
    "sign of the coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data and regression line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,3))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.scatterplot(data=train, x=\"TV\", y=\"sales\");\n",
    "sns.lineplot(data=train, x=\"TV\", y=reg_tv.predict(train[['TV']]), color='red');\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.scatterplot(data=train, x=\"radio\", y=\"sales\");\n",
    "sns.lineplot(data=train, x=\"radio\", y=reg_radio.predict(train[['radio']]), color='red');\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.scatterplot(data=train, x=\"newspaper\", y=\"sales\");\n",
    "sns.lineplot(data=train, x=\"newspaper\", y=reg_news.predict(train[['newspaper']]), color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute R2 for simple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tv = reg_tv.predict(test[['TV']])\n",
    "y_pred_radio = reg_radio.predict(test[['radio']])\n",
    "y_pred_news = reg_news.predict(test[['newspaper']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_tv = 1-np.mean( (y_pred_tv - test['sales'])**2 / np.std(test['sales'])**2 )\n",
    "r2_radio = 1-np.mean( (y_pred_radio - test['sales'])**2 / np.std(test['sales'])**2 )\n",
    "r2_news = 1-np.mean( (y_pred_news - test['sales'])**2 / np.std(test['sales'])**2 )\n",
    "print(\"TV: \", r2_tv)\n",
    "print(\"Radio: \", r2_radio)\n",
    "print(\"Newspaper: \", r2_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although radio ads have a larger effect on sales than TV ads, the effect\n",
    "of TV ads is better *explained* by our model than the effect of radio\n",
    "ads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_multi_ad = LinearRegression().fit(train[['TV', 'radio', 'newspaper']], train['sales'])\n",
    "print(\"Coefficients (TV, radio, newspaper):\", reg_multi_ad.coef_)\n",
    "print(\"Intercept: \", reg_multi_ad.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "We notice that the coefficients for TV, radio, and newspaper ads are\n",
    "different than they had been in the single regression case. In\n",
    "particular, we previously estimated that newspaper ads had a positive\n",
    "effect on sales, similar in magnitude to TV ads. Now, the newspaper ads\n",
    "are estimated as having an effect much closer to zero.\n",
    "\n",
    "This is because:\n",
    "\n",
    "-   In the simple regression case, the coefficent for newspaper ads\n",
    "    represents the effect of an increase in newspaper advertising.\n",
    "-   In the multiple regression case, the coefficient for newspaper ads\n",
    "    represents the effect of an increase in newspaper advertising *while\n",
    "    holding TV and radio advertising* constant.\n",
    "\n",
    "In the simple regression case, the observed \"association\" between\n",
    "newspaper advertising and sales was actually due to a relationship\n",
    "between newspaper spending and spending on other kinds of advertising.\n",
    "There is a correlation between newspaper spending and radio spending (as\n",
    "can be observed in the pairplot); in markets where we spent more on\n",
    "newspaper advertising, we also spent more on radio advertising. In the\n",
    "simple linear regression, newspaper ads got \"credit\" for the effect of\n",
    "radio advertising on sales, even though the newspaper advertising itself\n",
    "did not improve sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute R2 for multiple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_multi_ad = reg_multi_ad.predict(test[['TV', 'radio', 'newspaper']])\n",
    "\n",
    "r2_multi_ad = 1-np.mean( (y_pred_multi_ad - test['sales'])**2 / np.std(test['sales'])**2 )\n",
    "print(\"Multiple regression: \", r2_multi_ad)"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
